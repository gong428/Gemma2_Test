{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 8.298755186721992,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008298755186721992,
      "grad_norm": 9.834770202636719,
      "learning_rate": 4e-05,
      "loss": 3.1989,
      "step": 1
    },
    {
      "epoch": 0.016597510373443983,
      "grad_norm": 8.876495361328125,
      "learning_rate": 8e-05,
      "loss": 2.7387,
      "step": 2
    },
    {
      "epoch": 0.024896265560165973,
      "grad_norm": 7.920222759246826,
      "learning_rate": 0.00012,
      "loss": 2.5127,
      "step": 3
    },
    {
      "epoch": 0.03319502074688797,
      "grad_norm": 8.231279373168945,
      "learning_rate": 0.00016,
      "loss": 2.8868,
      "step": 4
    },
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 5.689220428466797,
      "learning_rate": 0.0002,
      "loss": 1.9865,
      "step": 5
    },
    {
      "epoch": 0.04979253112033195,
      "grad_norm": 5.014395713806152,
      "learning_rate": 0.00019979899497487438,
      "loss": 1.914,
      "step": 6
    },
    {
      "epoch": 0.058091286307053944,
      "grad_norm": 3.4225754737854004,
      "learning_rate": 0.00019959798994974876,
      "loss": 1.6518,
      "step": 7
    },
    {
      "epoch": 0.06639004149377593,
      "grad_norm": 2.520620584487915,
      "learning_rate": 0.00019939698492462313,
      "loss": 1.5366,
      "step": 8
    },
    {
      "epoch": 0.07468879668049792,
      "grad_norm": 1.9455488920211792,
      "learning_rate": 0.0001991959798994975,
      "loss": 1.495,
      "step": 9
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 1.434770107269287,
      "learning_rate": 0.00019899497487437187,
      "loss": 1.3669,
      "step": 10
    },
    {
      "epoch": 0.0912863070539419,
      "grad_norm": 1.3720701932907104,
      "learning_rate": 0.00019879396984924622,
      "loss": 1.4437,
      "step": 11
    },
    {
      "epoch": 0.0995850622406639,
      "grad_norm": 1.2090380191802979,
      "learning_rate": 0.00019859296482412062,
      "loss": 1.2371,
      "step": 12
    },
    {
      "epoch": 0.1078838174273859,
      "grad_norm": 1.174675703048706,
      "learning_rate": 0.000198391959798995,
      "loss": 1.3927,
      "step": 13
    },
    {
      "epoch": 0.11618257261410789,
      "grad_norm": 1.1215078830718994,
      "learning_rate": 0.00019819095477386937,
      "loss": 1.3365,
      "step": 14
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 1.0751546621322632,
      "learning_rate": 0.0001979899497487437,
      "loss": 1.3854,
      "step": 15
    },
    {
      "epoch": 0.13278008298755187,
      "grad_norm": 1.140397548675537,
      "learning_rate": 0.0001977889447236181,
      "loss": 1.3481,
      "step": 16
    },
    {
      "epoch": 0.14107883817427386,
      "grad_norm": 1.0643315315246582,
      "learning_rate": 0.00019758793969849249,
      "loss": 1.2802,
      "step": 17
    },
    {
      "epoch": 0.14937759336099585,
      "grad_norm": 1.1096618175506592,
      "learning_rate": 0.00019738693467336683,
      "loss": 1.3606,
      "step": 18
    },
    {
      "epoch": 0.15767634854771784,
      "grad_norm": 1.0394700765609741,
      "learning_rate": 0.0001971859296482412,
      "loss": 1.1443,
      "step": 19
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 1.058250904083252,
      "learning_rate": 0.0001969849246231156,
      "loss": 1.2893,
      "step": 20
    },
    {
      "epoch": 0.17427385892116182,
      "grad_norm": 1.0969181060791016,
      "learning_rate": 0.00019678391959798995,
      "loss": 1.3521,
      "step": 21
    },
    {
      "epoch": 0.1825726141078838,
      "grad_norm": 1.095960021018982,
      "learning_rate": 0.00019658291457286432,
      "loss": 1.3592,
      "step": 22
    },
    {
      "epoch": 0.1908713692946058,
      "grad_norm": 1.1666111946105957,
      "learning_rate": 0.0001963819095477387,
      "loss": 1.34,
      "step": 23
    },
    {
      "epoch": 0.1991701244813278,
      "grad_norm": 1.1203254461288452,
      "learning_rate": 0.0001961809045226131,
      "loss": 1.3133,
      "step": 24
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 1.1151355504989624,
      "learning_rate": 0.00019597989949748744,
      "loss": 1.2779,
      "step": 25
    },
    {
      "epoch": 0.2157676348547718,
      "grad_norm": 1.046364426612854,
      "learning_rate": 0.00019577889447236181,
      "loss": 1.2051,
      "step": 26
    },
    {
      "epoch": 0.22406639004149378,
      "grad_norm": 0.9693098068237305,
      "learning_rate": 0.0001955778894472362,
      "loss": 0.9997,
      "step": 27
    },
    {
      "epoch": 0.23236514522821577,
      "grad_norm": 1.026024341583252,
      "learning_rate": 0.00019537688442211056,
      "loss": 1.2023,
      "step": 28
    },
    {
      "epoch": 0.24066390041493776,
      "grad_norm": 1.1691055297851562,
      "learning_rate": 0.00019517587939698493,
      "loss": 1.4548,
      "step": 29
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 1.0435459613800049,
      "learning_rate": 0.0001949748743718593,
      "loss": 1.1723,
      "step": 30
    },
    {
      "epoch": 0.2572614107883817,
      "grad_norm": 0.906079888343811,
      "learning_rate": 0.00019477386934673368,
      "loss": 1.0041,
      "step": 31
    },
    {
      "epoch": 0.26556016597510373,
      "grad_norm": 0.9865516424179077,
      "learning_rate": 0.00019457286432160805,
      "loss": 1.1942,
      "step": 32
    },
    {
      "epoch": 0.27385892116182575,
      "grad_norm": 1.021187424659729,
      "learning_rate": 0.00019437185929648243,
      "loss": 1.2537,
      "step": 33
    },
    {
      "epoch": 0.2821576763485477,
      "grad_norm": 0.9775299429893494,
      "learning_rate": 0.0001941708542713568,
      "loss": 1.1173,
      "step": 34
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 0.9696130156517029,
      "learning_rate": 0.00019396984924623117,
      "loss": 1.2971,
      "step": 35
    },
    {
      "epoch": 0.2987551867219917,
      "grad_norm": 0.9941537976264954,
      "learning_rate": 0.00019376884422110552,
      "loss": 1.1919,
      "step": 36
    },
    {
      "epoch": 0.3070539419087137,
      "grad_norm": 0.999888002872467,
      "learning_rate": 0.00019356783919597992,
      "loss": 1.1837,
      "step": 37
    },
    {
      "epoch": 0.3153526970954357,
      "grad_norm": 0.9754721522331238,
      "learning_rate": 0.0001933668341708543,
      "loss": 1.1716,
      "step": 38
    },
    {
      "epoch": 0.3236514522821577,
      "grad_norm": 1.0635788440704346,
      "learning_rate": 0.00019316582914572864,
      "loss": 1.1674,
      "step": 39
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 0.9573739767074585,
      "learning_rate": 0.000192964824120603,
      "loss": 1.2759,
      "step": 40
    },
    {
      "epoch": 0.34024896265560167,
      "grad_norm": 0.9173368811607361,
      "learning_rate": 0.0001927638190954774,
      "loss": 1.0878,
      "step": 41
    },
    {
      "epoch": 0.34854771784232363,
      "grad_norm": 1.0573523044586182,
      "learning_rate": 0.00019256281407035178,
      "loss": 1.2958,
      "step": 42
    },
    {
      "epoch": 0.35684647302904565,
      "grad_norm": 0.9285069704055786,
      "learning_rate": 0.00019236180904522613,
      "loss": 0.9416,
      "step": 43
    },
    {
      "epoch": 0.3651452282157676,
      "grad_norm": 1.0217926502227783,
      "learning_rate": 0.0001921608040201005,
      "loss": 1.2428,
      "step": 44
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 0.9715312123298645,
      "learning_rate": 0.0001919597989949749,
      "loss": 1.0571,
      "step": 45
    },
    {
      "epoch": 0.3817427385892116,
      "grad_norm": 0.9465537071228027,
      "learning_rate": 0.00019175879396984925,
      "loss": 1.2466,
      "step": 46
    },
    {
      "epoch": 0.3900414937759336,
      "grad_norm": 0.9062249660491943,
      "learning_rate": 0.00019155778894472362,
      "loss": 1.0952,
      "step": 47
    },
    {
      "epoch": 0.3983402489626556,
      "grad_norm": 0.8801530599594116,
      "learning_rate": 0.000191356783919598,
      "loss": 1.1594,
      "step": 48
    },
    {
      "epoch": 0.4066390041493776,
      "grad_norm": 0.8837567567825317,
      "learning_rate": 0.0001911557788944724,
      "loss": 1.2214,
      "step": 49
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 0.9074248671531677,
      "learning_rate": 0.00019095477386934674,
      "loss": 1.1842,
      "step": 50
    },
    {
      "epoch": 0.42323651452282157,
      "grad_norm": 0.9016375541687012,
      "learning_rate": 0.0001907537688442211,
      "loss": 1.0276,
      "step": 51
    },
    {
      "epoch": 0.4315352697095436,
      "grad_norm": 0.9039750099182129,
      "learning_rate": 0.00019055276381909548,
      "loss": 1.143,
      "step": 52
    },
    {
      "epoch": 0.43983402489626555,
      "grad_norm": 0.9559658169746399,
      "learning_rate": 0.00019035175879396986,
      "loss": 1.2638,
      "step": 53
    },
    {
      "epoch": 0.44813278008298757,
      "grad_norm": 0.9471536874771118,
      "learning_rate": 0.00019015075376884423,
      "loss": 1.4242,
      "step": 54
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 0.9232811331748962,
      "learning_rate": 0.0001899497487437186,
      "loss": 1.1486,
      "step": 55
    },
    {
      "epoch": 0.46473029045643155,
      "grad_norm": 0.8779085874557495,
      "learning_rate": 0.00018974874371859298,
      "loss": 1.0212,
      "step": 56
    },
    {
      "epoch": 0.4730290456431535,
      "grad_norm": 0.8986270427703857,
      "learning_rate": 0.00018954773869346732,
      "loss": 1.0882,
      "step": 57
    },
    {
      "epoch": 0.48132780082987553,
      "grad_norm": 0.9575268030166626,
      "learning_rate": 0.00018934673366834172,
      "loss": 1.0609,
      "step": 58
    },
    {
      "epoch": 0.4896265560165975,
      "grad_norm": 0.8881059885025024,
      "learning_rate": 0.0001891457286432161,
      "loss": 0.984,
      "step": 59
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 0.9802026152610779,
      "learning_rate": 0.00018894472361809047,
      "loss": 1.0065,
      "step": 60
    },
    {
      "epoch": 0.5062240663900415,
      "grad_norm": 0.8956257104873657,
      "learning_rate": 0.00018874371859296481,
      "loss": 1.0255,
      "step": 61
    },
    {
      "epoch": 0.5145228215767634,
      "grad_norm": 0.9210503697395325,
      "learning_rate": 0.00018854271356783921,
      "loss": 1.0526,
      "step": 62
    },
    {
      "epoch": 0.5228215767634855,
      "grad_norm": 1.0349193811416626,
      "learning_rate": 0.0001883417085427136,
      "loss": 1.2933,
      "step": 63
    },
    {
      "epoch": 0.5311203319502075,
      "grad_norm": 0.9830848574638367,
      "learning_rate": 0.00018814070351758793,
      "loss": 1.2576,
      "step": 64
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 1.0153281688690186,
      "learning_rate": 0.0001879396984924623,
      "loss": 1.2889,
      "step": 65
    },
    {
      "epoch": 0.5477178423236515,
      "grad_norm": 0.9067654013633728,
      "learning_rate": 0.0001877386934673367,
      "loss": 1.004,
      "step": 66
    },
    {
      "epoch": 0.5560165975103735,
      "grad_norm": 0.9022826552391052,
      "learning_rate": 0.00018753768844221108,
      "loss": 1.0246,
      "step": 67
    },
    {
      "epoch": 0.5643153526970954,
      "grad_norm": 0.8329315781593323,
      "learning_rate": 0.00018733668341708543,
      "loss": 1.0887,
      "step": 68
    },
    {
      "epoch": 0.5726141078838174,
      "grad_norm": 0.9449093341827393,
      "learning_rate": 0.0001871356783919598,
      "loss": 0.9657,
      "step": 69
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 0.8289721608161926,
      "learning_rate": 0.0001869346733668342,
      "loss": 1.0229,
      "step": 70
    },
    {
      "epoch": 0.5892116182572614,
      "grad_norm": 0.8489035367965698,
      "learning_rate": 0.00018673366834170854,
      "loss": 1.1857,
      "step": 71
    },
    {
      "epoch": 0.5975103734439834,
      "grad_norm": 0.9677807688713074,
      "learning_rate": 0.00018653266331658292,
      "loss": 1.1606,
      "step": 72
    },
    {
      "epoch": 0.6058091286307054,
      "grad_norm": 0.8593841791152954,
      "learning_rate": 0.0001863316582914573,
      "loss": 1.1037,
      "step": 73
    },
    {
      "epoch": 0.6141078838174274,
      "grad_norm": 1.0242351293563843,
      "learning_rate": 0.0001861306532663317,
      "loss": 1.3459,
      "step": 74
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 1.0419251918792725,
      "learning_rate": 0.00018592964824120604,
      "loss": 1.1912,
      "step": 75
    },
    {
      "epoch": 0.6307053941908713,
      "grad_norm": 0.8729200959205627,
      "learning_rate": 0.0001857286432160804,
      "loss": 1.0862,
      "step": 76
    },
    {
      "epoch": 0.6390041493775933,
      "grad_norm": 0.7877036333084106,
      "learning_rate": 0.00018552763819095478,
      "loss": 0.8849,
      "step": 77
    },
    {
      "epoch": 0.6473029045643154,
      "grad_norm": 0.9751052260398865,
      "learning_rate": 0.00018532663316582915,
      "loss": 1.303,
      "step": 78
    },
    {
      "epoch": 0.6556016597510373,
      "grad_norm": 0.9295453429222107,
      "learning_rate": 0.00018512562814070353,
      "loss": 1.0281,
      "step": 79
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 0.928661584854126,
      "learning_rate": 0.0001849246231155779,
      "loss": 1.2213,
      "step": 80
    },
    {
      "epoch": 0.6721991701244814,
      "grad_norm": 1.009596824645996,
      "learning_rate": 0.00018472361809045227,
      "loss": 1.0756,
      "step": 81
    },
    {
      "epoch": 0.6804979253112033,
      "grad_norm": 0.907459557056427,
      "learning_rate": 0.00018452261306532662,
      "loss": 1.094,
      "step": 82
    },
    {
      "epoch": 0.6887966804979253,
      "grad_norm": 0.8837058544158936,
      "learning_rate": 0.00018432160804020102,
      "loss": 1.1637,
      "step": 83
    },
    {
      "epoch": 0.6970954356846473,
      "grad_norm": 0.917782723903656,
      "learning_rate": 0.0001841206030150754,
      "loss": 1.1039,
      "step": 84
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 0.8537139892578125,
      "learning_rate": 0.00018391959798994977,
      "loss": 1.1867,
      "step": 85
    },
    {
      "epoch": 0.7136929460580913,
      "grad_norm": 0.8984200358390808,
      "learning_rate": 0.0001837185929648241,
      "loss": 1.0321,
      "step": 86
    },
    {
      "epoch": 0.7219917012448133,
      "grad_norm": 0.8227758407592773,
      "learning_rate": 0.0001835175879396985,
      "loss": 1.1252,
      "step": 87
    },
    {
      "epoch": 0.7302904564315352,
      "grad_norm": 0.8680870532989502,
      "learning_rate": 0.00018331658291457288,
      "loss": 1.1791,
      "step": 88
    },
    {
      "epoch": 0.7385892116182573,
      "grad_norm": 0.8905494213104248,
      "learning_rate": 0.00018311557788944723,
      "loss": 1.1316,
      "step": 89
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 0.8255711793899536,
      "learning_rate": 0.0001829145728643216,
      "loss": 0.8688,
      "step": 90
    },
    {
      "epoch": 0.7551867219917012,
      "grad_norm": 0.8046554327011108,
      "learning_rate": 0.000182713567839196,
      "loss": 1.0993,
      "step": 91
    },
    {
      "epoch": 0.7634854771784232,
      "grad_norm": 1.0828288793563843,
      "learning_rate": 0.00018251256281407038,
      "loss": 1.0765,
      "step": 92
    },
    {
      "epoch": 0.7717842323651453,
      "grad_norm": 0.7832619547843933,
      "learning_rate": 0.00018231155778894472,
      "loss": 1.0401,
      "step": 93
    },
    {
      "epoch": 0.7800829875518672,
      "grad_norm": 0.9080089330673218,
      "learning_rate": 0.0001821105527638191,
      "loss": 1.0679,
      "step": 94
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 0.8765209913253784,
      "learning_rate": 0.0001819095477386935,
      "loss": 0.9614,
      "step": 95
    },
    {
      "epoch": 0.7966804979253111,
      "grad_norm": 0.927598774433136,
      "learning_rate": 0.00018170854271356784,
      "loss": 1.021,
      "step": 96
    },
    {
      "epoch": 0.8049792531120332,
      "grad_norm": 0.8881963491439819,
      "learning_rate": 0.00018150753768844221,
      "loss": 1.0176,
      "step": 97
    },
    {
      "epoch": 0.8132780082987552,
      "grad_norm": 0.9378779530525208,
      "learning_rate": 0.0001813065326633166,
      "loss": 1.1495,
      "step": 98
    },
    {
      "epoch": 0.8215767634854771,
      "grad_norm": 1.0039005279541016,
      "learning_rate": 0.00018110552763819096,
      "loss": 1.1909,
      "step": 99
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 0.8454744815826416,
      "learning_rate": 0.00018090452261306533,
      "loss": 0.8871,
      "step": 100
    },
    {
      "epoch": 0.8381742738589212,
      "grad_norm": 1.0230399370193481,
      "learning_rate": 0.0001807035175879397,
      "loss": 1.2223,
      "step": 101
    },
    {
      "epoch": 0.8464730290456431,
      "grad_norm": 1.0655370950698853,
      "learning_rate": 0.00018050251256281408,
      "loss": 1.1749,
      "step": 102
    },
    {
      "epoch": 0.8547717842323651,
      "grad_norm": 0.8916755318641663,
      "learning_rate": 0.00018030150753768845,
      "loss": 1.0435,
      "step": 103
    },
    {
      "epoch": 0.8630705394190872,
      "grad_norm": 0.935066282749176,
      "learning_rate": 0.00018010050251256282,
      "loss": 1.2371,
      "step": 104
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 0.8592977523803711,
      "learning_rate": 0.0001798994974874372,
      "loss": 1.0313,
      "step": 105
    },
    {
      "epoch": 0.8796680497925311,
      "grad_norm": 0.8850722908973694,
      "learning_rate": 0.00017969849246231157,
      "loss": 1.2017,
      "step": 106
    },
    {
      "epoch": 0.8879668049792531,
      "grad_norm": 0.8350237011909485,
      "learning_rate": 0.00017949748743718592,
      "loss": 0.9661,
      "step": 107
    },
    {
      "epoch": 0.8962655601659751,
      "grad_norm": 0.8727333545684814,
      "learning_rate": 0.00017929648241206032,
      "loss": 1.1939,
      "step": 108
    },
    {
      "epoch": 0.9045643153526971,
      "grad_norm": 0.8180919289588928,
      "learning_rate": 0.0001790954773869347,
      "loss": 0.9684,
      "step": 109
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 0.96207195520401,
      "learning_rate": 0.00017889447236180906,
      "loss": 1.033,
      "step": 110
    },
    {
      "epoch": 0.921161825726141,
      "grad_norm": 0.8653010129928589,
      "learning_rate": 0.0001786934673366834,
      "loss": 1.1068,
      "step": 111
    },
    {
      "epoch": 0.9294605809128631,
      "grad_norm": 0.8511499166488647,
      "learning_rate": 0.0001784924623115578,
      "loss": 0.9369,
      "step": 112
    },
    {
      "epoch": 0.9377593360995851,
      "grad_norm": 0.9456340670585632,
      "learning_rate": 0.00017829145728643218,
      "loss": 1.1872,
      "step": 113
    },
    {
      "epoch": 0.946058091286307,
      "grad_norm": 0.8849169015884399,
      "learning_rate": 0.00017809045226130653,
      "loss": 0.9248,
      "step": 114
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 0.8799336552619934,
      "learning_rate": 0.0001778894472361809,
      "loss": 1.0893,
      "step": 115
    },
    {
      "epoch": 0.9626556016597511,
      "grad_norm": 0.8502377867698669,
      "learning_rate": 0.0001776884422110553,
      "loss": 1.0415,
      "step": 116
    },
    {
      "epoch": 0.970954356846473,
      "grad_norm": 0.9667127728462219,
      "learning_rate": 0.00017748743718592967,
      "loss": 1.162,
      "step": 117
    },
    {
      "epoch": 0.979253112033195,
      "grad_norm": 0.8258129954338074,
      "learning_rate": 0.00017728643216080402,
      "loss": 1.1026,
      "step": 118
    },
    {
      "epoch": 0.9875518672199171,
      "grad_norm": 0.866804301738739,
      "learning_rate": 0.0001770854271356784,
      "loss": 0.934,
      "step": 119
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 0.9576835632324219,
      "learning_rate": 0.0001768844221105528,
      "loss": 1.1231,
      "step": 120
    },
    {
      "epoch": 1.004149377593361,
      "grad_norm": 0.9966908693313599,
      "learning_rate": 0.00017668341708542714,
      "loss": 0.8512,
      "step": 121
    },
    {
      "epoch": 1.012448132780083,
      "grad_norm": 0.766650915145874,
      "learning_rate": 0.0001764824120603015,
      "loss": 0.8274,
      "step": 122
    },
    {
      "epoch": 1.020746887966805,
      "grad_norm": 0.8157761096954346,
      "learning_rate": 0.00017628140703517588,
      "loss": 0.8751,
      "step": 123
    },
    {
      "epoch": 1.0290456431535269,
      "grad_norm": 0.8358221054077148,
      "learning_rate": 0.00017608040201005026,
      "loss": 0.7612,
      "step": 124
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 0.8332679271697998,
      "learning_rate": 0.00017587939698492463,
      "loss": 0.9263,
      "step": 125
    },
    {
      "epoch": 1.045643153526971,
      "grad_norm": 0.8390955924987793,
      "learning_rate": 0.000175678391959799,
      "loss": 0.8524,
      "step": 126
    },
    {
      "epoch": 1.053941908713693,
      "grad_norm": 0.8618723750114441,
      "learning_rate": 0.00017547738693467338,
      "loss": 0.841,
      "step": 127
    },
    {
      "epoch": 1.062240663900415,
      "grad_norm": 0.8663930892944336,
      "learning_rate": 0.00017527638190954775,
      "loss": 0.7894,
      "step": 128
    },
    {
      "epoch": 1.070539419087137,
      "grad_norm": 0.9730732440948486,
      "learning_rate": 0.00017507537688442212,
      "loss": 0.8326,
      "step": 129
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 0.9107650518417358,
      "learning_rate": 0.0001748743718592965,
      "loss": 0.8185,
      "step": 130
    },
    {
      "epoch": 1.0871369294605808,
      "grad_norm": 0.9191886782646179,
      "learning_rate": 0.00017467336683417087,
      "loss": 0.7931,
      "step": 131
    },
    {
      "epoch": 1.095435684647303,
      "grad_norm": 0.9611661434173584,
      "learning_rate": 0.00017447236180904521,
      "loss": 0.7425,
      "step": 132
    },
    {
      "epoch": 1.103734439834025,
      "grad_norm": 1.0332282781600952,
      "learning_rate": 0.00017427135678391961,
      "loss": 0.902,
      "step": 133
    },
    {
      "epoch": 1.112033195020747,
      "grad_norm": 1.001977562904358,
      "learning_rate": 0.000174070351758794,
      "loss": 0.7224,
      "step": 134
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 1.2880430221557617,
      "learning_rate": 0.00017386934673366836,
      "loss": 0.8492,
      "step": 135
    },
    {
      "epoch": 1.1286307053941909,
      "grad_norm": 1.0940419435501099,
      "learning_rate": 0.0001736683417085427,
      "loss": 0.8821,
      "step": 136
    },
    {
      "epoch": 1.1369294605809128,
      "grad_norm": 1.0259041786193848,
      "learning_rate": 0.0001734673366834171,
      "loss": 0.773,
      "step": 137
    },
    {
      "epoch": 1.1452282157676348,
      "grad_norm": 1.1189830303192139,
      "learning_rate": 0.00017326633165829148,
      "loss": 0.9387,
      "step": 138
    },
    {
      "epoch": 1.1535269709543567,
      "grad_norm": 0.9735915064811707,
      "learning_rate": 0.00017306532663316582,
      "loss": 0.7305,
      "step": 139
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 1.1672183275222778,
      "learning_rate": 0.0001728643216080402,
      "loss": 0.93,
      "step": 140
    },
    {
      "epoch": 1.170124481327801,
      "grad_norm": 0.991202712059021,
      "learning_rate": 0.0001726633165829146,
      "loss": 0.7077,
      "step": 141
    },
    {
      "epoch": 1.1784232365145229,
      "grad_norm": 1.1231645345687866,
      "learning_rate": 0.00017246231155778897,
      "loss": 0.8509,
      "step": 142
    },
    {
      "epoch": 1.1867219917012448,
      "grad_norm": 1.166212558746338,
      "learning_rate": 0.00017226130653266332,
      "loss": 0.9317,
      "step": 143
    },
    {
      "epoch": 1.1950207468879668,
      "grad_norm": 0.9466010332107544,
      "learning_rate": 0.0001720603015075377,
      "loss": 0.6634,
      "step": 144
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 1.1704787015914917,
      "learning_rate": 0.00017185929648241206,
      "loss": 0.8615,
      "step": 145
    },
    {
      "epoch": 1.2116182572614107,
      "grad_norm": 1.0647733211517334,
      "learning_rate": 0.00017165829145728644,
      "loss": 0.7379,
      "step": 146
    },
    {
      "epoch": 1.2199170124481329,
      "grad_norm": 1.0223045349121094,
      "learning_rate": 0.0001714572864321608,
      "loss": 0.7575,
      "step": 147
    },
    {
      "epoch": 1.2282157676348548,
      "grad_norm": 1.0791923999786377,
      "learning_rate": 0.00017125628140703518,
      "loss": 0.8638,
      "step": 148
    },
    {
      "epoch": 1.2365145228215768,
      "grad_norm": 1.1411551237106323,
      "learning_rate": 0.00017105527638190955,
      "loss": 0.8712,
      "step": 149
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 1.2744802236557007,
      "learning_rate": 0.00017085427135678393,
      "loss": 1.077,
      "step": 150
    },
    {
      "epoch": 1.2531120331950207,
      "grad_norm": 1.0772117376327515,
      "learning_rate": 0.0001706532663316583,
      "loss": 0.8626,
      "step": 151
    },
    {
      "epoch": 1.2614107883817427,
      "grad_norm": 1.1927402019500732,
      "learning_rate": 0.00017045226130653267,
      "loss": 0.9746,
      "step": 152
    },
    {
      "epoch": 1.2697095435684647,
      "grad_norm": 1.0795263051986694,
      "learning_rate": 0.00017025125628140705,
      "loss": 0.7622,
      "step": 153
    },
    {
      "epoch": 1.2780082987551866,
      "grad_norm": 1.108771562576294,
      "learning_rate": 0.00017005025125628142,
      "loss": 0.8431,
      "step": 154
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 0.962293267250061,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.744,
      "step": 155
    },
    {
      "epoch": 1.2946058091286308,
      "grad_norm": 1.1309700012207031,
      "learning_rate": 0.00016964824120603016,
      "loss": 0.7943,
      "step": 156
    },
    {
      "epoch": 1.3029045643153527,
      "grad_norm": 0.9431834816932678,
      "learning_rate": 0.0001694472361809045,
      "loss": 0.7365,
      "step": 157
    },
    {
      "epoch": 1.3112033195020747,
      "grad_norm": 1.0529766082763672,
      "learning_rate": 0.0001692462311557789,
      "loss": 0.7294,
      "step": 158
    },
    {
      "epoch": 1.3195020746887967,
      "grad_norm": 1.1238313913345337,
      "learning_rate": 0.00016904522613065328,
      "loss": 0.8127,
      "step": 159
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 1.1808702945709229,
      "learning_rate": 0.00016884422110552766,
      "loss": 0.7708,
      "step": 160
    },
    {
      "epoch": 1.3360995850622408,
      "grad_norm": 1.0923155546188354,
      "learning_rate": 0.000168643216080402,
      "loss": 0.6723,
      "step": 161
    },
    {
      "epoch": 1.3443983402489628,
      "grad_norm": 1.1982401609420776,
      "learning_rate": 0.0001684422110552764,
      "loss": 0.7671,
      "step": 162
    },
    {
      "epoch": 1.3526970954356847,
      "grad_norm": 1.076532006263733,
      "learning_rate": 0.00016824120603015078,
      "loss": 0.7768,
      "step": 163
    },
    {
      "epoch": 1.3609958506224067,
      "grad_norm": 1.0442031621932983,
      "learning_rate": 0.00016804020100502512,
      "loss": 0.6618,
      "step": 164
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 1.1625006198883057,
      "learning_rate": 0.0001678391959798995,
      "loss": 0.9103,
      "step": 165
    },
    {
      "epoch": 1.3775933609958506,
      "grad_norm": 1.1231801509857178,
      "learning_rate": 0.0001676381909547739,
      "loss": 0.8194,
      "step": 166
    },
    {
      "epoch": 1.3858921161825726,
      "grad_norm": 1.0368504524230957,
      "learning_rate": 0.00016743718592964827,
      "loss": 0.6221,
      "step": 167
    },
    {
      "epoch": 1.3941908713692945,
      "grad_norm": 1.316107153892517,
      "learning_rate": 0.0001672361809045226,
      "loss": 0.8604,
      "step": 168
    },
    {
      "epoch": 1.4024896265560165,
      "grad_norm": 1.205311894416809,
      "learning_rate": 0.00016703517587939699,
      "loss": 0.821,
      "step": 169
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 1.173353910446167,
      "learning_rate": 0.00016683417085427136,
      "loss": 0.8628,
      "step": 170
    },
    {
      "epoch": 1.4190871369294606,
      "grad_norm": 1.1120721101760864,
      "learning_rate": 0.00016663316582914573,
      "loss": 0.6534,
      "step": 171
    },
    {
      "epoch": 1.4273858921161826,
      "grad_norm": 1.1444933414459229,
      "learning_rate": 0.0001664321608040201,
      "loss": 0.8155,
      "step": 172
    },
    {
      "epoch": 1.4356846473029046,
      "grad_norm": 1.2159368991851807,
      "learning_rate": 0.00016623115577889448,
      "loss": 0.9404,
      "step": 173
    },
    {
      "epoch": 1.4439834024896265,
      "grad_norm": 1.1341862678527832,
      "learning_rate": 0.00016603015075376885,
      "loss": 0.7995,
      "step": 174
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 1.097231149673462,
      "learning_rate": 0.00016582914572864322,
      "loss": 0.8662,
      "step": 175
    },
    {
      "epoch": 1.4605809128630705,
      "grad_norm": 1.0552012920379639,
      "learning_rate": 0.0001656281407035176,
      "loss": 0.736,
      "step": 176
    },
    {
      "epoch": 1.4688796680497926,
      "grad_norm": 1.0139731168746948,
      "learning_rate": 0.00016542713567839197,
      "loss": 0.7086,
      "step": 177
    },
    {
      "epoch": 1.4771784232365146,
      "grad_norm": 1.1557316780090332,
      "learning_rate": 0.00016522613065326634,
      "loss": 0.7486,
      "step": 178
    },
    {
      "epoch": 1.4854771784232366,
      "grad_norm": 1.1118431091308594,
      "learning_rate": 0.00016502512562814072,
      "loss": 0.897,
      "step": 179
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 1.0895923376083374,
      "learning_rate": 0.0001648241206030151,
      "loss": 0.7315,
      "step": 180
    },
    {
      "epoch": 1.5020746887966805,
      "grad_norm": 1.0663847923278809,
      "learning_rate": 0.00016462311557788946,
      "loss": 0.7519,
      "step": 181
    },
    {
      "epoch": 1.5103734439834025,
      "grad_norm": 1.1324940919876099,
      "learning_rate": 0.0001644221105527638,
      "loss": 0.7563,
      "step": 182
    },
    {
      "epoch": 1.5186721991701244,
      "grad_norm": 1.2150053977966309,
      "learning_rate": 0.0001642211055276382,
      "loss": 0.8168,
      "step": 183
    },
    {
      "epoch": 1.5269709543568464,
      "grad_norm": 1.1770274639129639,
      "learning_rate": 0.00016402010050251258,
      "loss": 0.7233,
      "step": 184
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 1.0841350555419922,
      "learning_rate": 0.00016381909547738695,
      "loss": 0.68,
      "step": 185
    },
    {
      "epoch": 1.5435684647302903,
      "grad_norm": 1.1049998998641968,
      "learning_rate": 0.0001636180904522613,
      "loss": 0.7211,
      "step": 186
    },
    {
      "epoch": 1.5518672199170125,
      "grad_norm": 1.205085039138794,
      "learning_rate": 0.0001634170854271357,
      "loss": 0.8381,
      "step": 187
    },
    {
      "epoch": 1.5601659751037344,
      "grad_norm": 1.0672179460525513,
      "learning_rate": 0.00016321608040201007,
      "loss": 0.6602,
      "step": 188
    },
    {
      "epoch": 1.5684647302904564,
      "grad_norm": 1.139390468597412,
      "learning_rate": 0.00016301507537688442,
      "loss": 0.754,
      "step": 189
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 1.1813709735870361,
      "learning_rate": 0.0001628140703517588,
      "loss": 0.7663,
      "step": 190
    },
    {
      "epoch": 1.5850622406639006,
      "grad_norm": 1.23631751537323,
      "learning_rate": 0.00016261306532663316,
      "loss": 0.8166,
      "step": 191
    },
    {
      "epoch": 1.5933609958506225,
      "grad_norm": 1.1930744647979736,
      "learning_rate": 0.00016241206030150756,
      "loss": 0.9419,
      "step": 192
    },
    {
      "epoch": 1.6016597510373445,
      "grad_norm": 1.3094475269317627,
      "learning_rate": 0.0001622110552763819,
      "loss": 0.9255,
      "step": 193
    },
    {
      "epoch": 1.6099585062240664,
      "grad_norm": 1.2743759155273438,
      "learning_rate": 0.00016201005025125628,
      "loss": 1.0056,
      "step": 194
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 1.1439329385757446,
      "learning_rate": 0.00016180904522613066,
      "loss": 0.8781,
      "step": 195
    },
    {
      "epoch": 1.6265560165975104,
      "grad_norm": 1.110429286956787,
      "learning_rate": 0.00016160804020100503,
      "loss": 0.7872,
      "step": 196
    },
    {
      "epoch": 1.6348547717842323,
      "grad_norm": 1.1604655981063843,
      "learning_rate": 0.0001614070351758794,
      "loss": 0.8802,
      "step": 197
    },
    {
      "epoch": 1.6431535269709543,
      "grad_norm": 1.1259818077087402,
      "learning_rate": 0.00016120603015075378,
      "loss": 0.8,
      "step": 198
    },
    {
      "epoch": 1.6514522821576763,
      "grad_norm": 1.1615406274795532,
      "learning_rate": 0.00016100502512562815,
      "loss": 0.789,
      "step": 199
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 1.2093924283981323,
      "learning_rate": 0.00016080402010050252,
      "loss": 1.0213,
      "step": 200
    },
    {
      "epoch": 1.6680497925311202,
      "grad_norm": 1.0446913242340088,
      "learning_rate": 0.0001606030150753769,
      "loss": 0.7272,
      "step": 201
    },
    {
      "epoch": 1.6763485477178424,
      "grad_norm": 1.1881910562515259,
      "learning_rate": 0.00016040201005025127,
      "loss": 0.8217,
      "step": 202
    },
    {
      "epoch": 1.6846473029045643,
      "grad_norm": 1.139897346496582,
      "learning_rate": 0.00016020100502512564,
      "loss": 0.8207,
      "step": 203
    },
    {
      "epoch": 1.6929460580912863,
      "grad_norm": 1.0955438613891602,
      "learning_rate": 0.00016,
      "loss": 0.8115,
      "step": 204
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 1.1490460634231567,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.9039,
      "step": 205
    },
    {
      "epoch": 1.7095435684647304,
      "grad_norm": 1.13961660861969,
      "learning_rate": 0.00015959798994974876,
      "loss": 0.7681,
      "step": 206
    },
    {
      "epoch": 1.7178423236514524,
      "grad_norm": 1.0851726531982422,
      "learning_rate": 0.0001593969849246231,
      "loss": 0.7862,
      "step": 207
    },
    {
      "epoch": 1.7261410788381744,
      "grad_norm": 1.1023656129837036,
      "learning_rate": 0.0001591959798994975,
      "loss": 0.6669,
      "step": 208
    },
    {
      "epoch": 1.7344398340248963,
      "grad_norm": 1.1414350271224976,
      "learning_rate": 0.00015899497487437188,
      "loss": 0.796,
      "step": 209
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 1.0385098457336426,
      "learning_rate": 0.00015879396984924625,
      "loss": 0.7034,
      "step": 210
    },
    {
      "epoch": 1.7510373443983402,
      "grad_norm": 1.0851752758026123,
      "learning_rate": 0.0001585929648241206,
      "loss": 0.7084,
      "step": 211
    },
    {
      "epoch": 1.7593360995850622,
      "grad_norm": 1.2279151678085327,
      "learning_rate": 0.000158391959798995,
      "loss": 0.8162,
      "step": 212
    },
    {
      "epoch": 1.7676348547717842,
      "grad_norm": 1.0834919214248657,
      "learning_rate": 0.00015819095477386937,
      "loss": 0.8092,
      "step": 213
    },
    {
      "epoch": 1.7759336099585061,
      "grad_norm": 1.202647089958191,
      "learning_rate": 0.00015798994974874372,
      "loss": 0.7616,
      "step": 214
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 1.0954307317733765,
      "learning_rate": 0.0001577889447236181,
      "loss": 0.731,
      "step": 215
    },
    {
      "epoch": 1.79253112033195,
      "grad_norm": 1.210024356842041,
      "learning_rate": 0.00015758793969849246,
      "loss": 0.8165,
      "step": 216
    },
    {
      "epoch": 1.8008298755186722,
      "grad_norm": 1.2587181329727173,
      "learning_rate": 0.00015738693467336686,
      "loss": 0.7842,
      "step": 217
    },
    {
      "epoch": 1.8091286307053942,
      "grad_norm": 1.2736486196517944,
      "learning_rate": 0.0001571859296482412,
      "loss": 0.7744,
      "step": 218
    },
    {
      "epoch": 1.8174273858921162,
      "grad_norm": 1.0740529298782349,
      "learning_rate": 0.00015698492462311558,
      "loss": 0.7231,
      "step": 219
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 1.270075798034668,
      "learning_rate": 0.00015678391959798995,
      "loss": 0.8698,
      "step": 220
    },
    {
      "epoch": 1.8340248962655603,
      "grad_norm": 1.0416918992996216,
      "learning_rate": 0.00015658291457286433,
      "loss": 0.7478,
      "step": 221
    },
    {
      "epoch": 1.8423236514522823,
      "grad_norm": 1.2136261463165283,
      "learning_rate": 0.0001563819095477387,
      "loss": 0.8747,
      "step": 222
    },
    {
      "epoch": 1.8506224066390042,
      "grad_norm": 1.352434754371643,
      "learning_rate": 0.00015618090452261307,
      "loss": 0.8396,
      "step": 223
    },
    {
      "epoch": 1.8589211618257262,
      "grad_norm": 1.0699255466461182,
      "learning_rate": 0.00015597989949748745,
      "loss": 0.807,
      "step": 224
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 1.294288158416748,
      "learning_rate": 0.00015577889447236182,
      "loss": 0.8846,
      "step": 225
    },
    {
      "epoch": 1.8755186721991701,
      "grad_norm": 1.235807180404663,
      "learning_rate": 0.0001555778894472362,
      "loss": 0.7925,
      "step": 226
    },
    {
      "epoch": 1.883817427385892,
      "grad_norm": 1.2441710233688354,
      "learning_rate": 0.00015537688442211056,
      "loss": 0.8438,
      "step": 227
    },
    {
      "epoch": 1.892116182572614,
      "grad_norm": 1.0660206079483032,
      "learning_rate": 0.00015517587939698494,
      "loss": 0.642,
      "step": 228
    },
    {
      "epoch": 1.900414937759336,
      "grad_norm": 1.0658175945281982,
      "learning_rate": 0.0001549748743718593,
      "loss": 0.6239,
      "step": 229
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 1.2978029251098633,
      "learning_rate": 0.00015477386934673368,
      "loss": 0.8851,
      "step": 230
    },
    {
      "epoch": 1.91701244813278,
      "grad_norm": 1.2388139963150024,
      "learning_rate": 0.00015457286432160806,
      "loss": 0.7939,
      "step": 231
    },
    {
      "epoch": 1.9253112033195021,
      "grad_norm": 1.0918916463851929,
      "learning_rate": 0.0001543718592964824,
      "loss": 0.7042,
      "step": 232
    },
    {
      "epoch": 1.933609958506224,
      "grad_norm": 1.1465984582901,
      "learning_rate": 0.0001541708542713568,
      "loss": 0.8131,
      "step": 233
    },
    {
      "epoch": 1.941908713692946,
      "grad_norm": 1.1936181783676147,
      "learning_rate": 0.00015396984924623117,
      "loss": 0.9302,
      "step": 234
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 1.1323003768920898,
      "learning_rate": 0.00015376884422110555,
      "loss": 0.7874,
      "step": 235
    },
    {
      "epoch": 1.9585062240663902,
      "grad_norm": 1.1818000078201294,
      "learning_rate": 0.0001535678391959799,
      "loss": 0.8504,
      "step": 236
    },
    {
      "epoch": 1.9668049792531122,
      "grad_norm": 1.0579715967178345,
      "learning_rate": 0.00015336683417085427,
      "loss": 0.6439,
      "step": 237
    },
    {
      "epoch": 1.9751037344398341,
      "grad_norm": 1.0311403274536133,
      "learning_rate": 0.00015316582914572867,
      "loss": 0.7426,
      "step": 238
    },
    {
      "epoch": 1.983402489626556,
      "grad_norm": 1.125788927078247,
      "learning_rate": 0.000152964824120603,
      "loss": 0.88,
      "step": 239
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 1.28829026222229,
      "learning_rate": 0.00015276381909547739,
      "loss": 0.771,
      "step": 240
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3153051137924194,
      "learning_rate": 0.00015256281407035176,
      "loss": 0.8033,
      "step": 241
    },
    {
      "epoch": 2.008298755186722,
      "grad_norm": 1.0306272506713867,
      "learning_rate": 0.00015236180904522613,
      "loss": 0.5551,
      "step": 242
    },
    {
      "epoch": 2.016597510373444,
      "grad_norm": 0.9033105969429016,
      "learning_rate": 0.0001521608040201005,
      "loss": 0.4459,
      "step": 243
    },
    {
      "epoch": 2.024896265560166,
      "grad_norm": 1.0479035377502441,
      "learning_rate": 0.00015195979899497488,
      "loss": 0.5417,
      "step": 244
    },
    {
      "epoch": 2.033195020746888,
      "grad_norm": 1.165921926498413,
      "learning_rate": 0.00015175879396984925,
      "loss": 0.6249,
      "step": 245
    },
    {
      "epoch": 2.04149377593361,
      "grad_norm": 1.1623784303665161,
      "learning_rate": 0.00015155778894472362,
      "loss": 0.6443,
      "step": 246
    },
    {
      "epoch": 2.0497925311203318,
      "grad_norm": 1.3475767374038696,
      "learning_rate": 0.000151356783919598,
      "loss": 0.6067,
      "step": 247
    },
    {
      "epoch": 2.0580912863070537,
      "grad_norm": 1.3195610046386719,
      "learning_rate": 0.00015115577889447237,
      "loss": 0.6229,
      "step": 248
    },
    {
      "epoch": 2.066390041493776,
      "grad_norm": 1.4343822002410889,
      "learning_rate": 0.00015095477386934674,
      "loss": 0.676,
      "step": 249
    },
    {
      "epoch": 2.074688796680498,
      "grad_norm": 1.41526460647583,
      "learning_rate": 0.00015075376884422112,
      "loss": 0.5436,
      "step": 250
    },
    {
      "epoch": 2.08298755186722,
      "grad_norm": 1.4628443717956543,
      "learning_rate": 0.0001505527638190955,
      "loss": 0.4251,
      "step": 251
    },
    {
      "epoch": 2.091286307053942,
      "grad_norm": 1.6013458967208862,
      "learning_rate": 0.00015035175879396986,
      "loss": 0.4609,
      "step": 252
    },
    {
      "epoch": 2.099585062240664,
      "grad_norm": 1.4982620477676392,
      "learning_rate": 0.00015015075376884423,
      "loss": 0.4682,
      "step": 253
    },
    {
      "epoch": 2.107883817427386,
      "grad_norm": 1.5559749603271484,
      "learning_rate": 0.0001499497487437186,
      "loss": 0.5458,
      "step": 254
    },
    {
      "epoch": 2.116182572614108,
      "grad_norm": 1.370057225227356,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.3259,
      "step": 255
    },
    {
      "epoch": 2.12448132780083,
      "grad_norm": 1.626512885093689,
      "learning_rate": 0.00014954773869346735,
      "loss": 0.5596,
      "step": 256
    },
    {
      "epoch": 2.132780082987552,
      "grad_norm": 1.5761382579803467,
      "learning_rate": 0.0001493467336683417,
      "loss": 0.4569,
      "step": 257
    },
    {
      "epoch": 2.141078838174274,
      "grad_norm": 1.3972676992416382,
      "learning_rate": 0.0001491457286432161,
      "loss": 0.3561,
      "step": 258
    },
    {
      "epoch": 2.1493775933609958,
      "grad_norm": 1.452401041984558,
      "learning_rate": 0.00014894472361809047,
      "loss": 0.5342,
      "step": 259
    },
    {
      "epoch": 2.1576763485477177,
      "grad_norm": 1.2507885694503784,
      "learning_rate": 0.00014874371859296482,
      "loss": 0.4624,
      "step": 260
    },
    {
      "epoch": 2.1659751037344397,
      "grad_norm": 1.254610300064087,
      "learning_rate": 0.0001485427135678392,
      "loss": 0.3703,
      "step": 261
    },
    {
      "epoch": 2.1742738589211617,
      "grad_norm": 1.3569316864013672,
      "learning_rate": 0.00014834170854271356,
      "loss": 0.409,
      "step": 262
    },
    {
      "epoch": 2.1825726141078836,
      "grad_norm": 1.4537683725357056,
      "learning_rate": 0.00014814070351758796,
      "loss": 0.562,
      "step": 263
    },
    {
      "epoch": 2.190871369294606,
      "grad_norm": 1.4368224143981934,
      "learning_rate": 0.0001479396984924623,
      "loss": 0.558,
      "step": 264
    },
    {
      "epoch": 2.199170124481328,
      "grad_norm": 1.3906646966934204,
      "learning_rate": 0.00014773869346733668,
      "loss": 0.61,
      "step": 265
    },
    {
      "epoch": 2.20746887966805,
      "grad_norm": 1.345909833908081,
      "learning_rate": 0.00014753768844221106,
      "loss": 0.476,
      "step": 266
    },
    {
      "epoch": 2.215767634854772,
      "grad_norm": 1.6240447759628296,
      "learning_rate": 0.00014733668341708543,
      "loss": 0.5487,
      "step": 267
    },
    {
      "epoch": 2.224066390041494,
      "grad_norm": 1.5580388307571411,
      "learning_rate": 0.0001471356783919598,
      "loss": 0.4983,
      "step": 268
    },
    {
      "epoch": 2.232365145228216,
      "grad_norm": 1.4720885753631592,
      "learning_rate": 0.00014693467336683417,
      "loss": 0.5069,
      "step": 269
    },
    {
      "epoch": 2.240663900414938,
      "grad_norm": 1.503630518913269,
      "learning_rate": 0.00014673366834170855,
      "loss": 0.4464,
      "step": 270
    },
    {
      "epoch": 2.2489626556016598,
      "grad_norm": 1.5194319486618042,
      "learning_rate": 0.00014653266331658292,
      "loss": 0.533,
      "step": 271
    },
    {
      "epoch": 2.2572614107883817,
      "grad_norm": 1.4581201076507568,
      "learning_rate": 0.0001463316582914573,
      "loss": 0.476,
      "step": 272
    },
    {
      "epoch": 2.2655601659751037,
      "grad_norm": 1.3086541891098022,
      "learning_rate": 0.00014613065326633167,
      "loss": 0.5336,
      "step": 273
    },
    {
      "epoch": 2.2738589211618256,
      "grad_norm": 1.6226232051849365,
      "learning_rate": 0.00014592964824120604,
      "loss": 0.5103,
      "step": 274
    },
    {
      "epoch": 2.2821576763485476,
      "grad_norm": 1.4995474815368652,
      "learning_rate": 0.0001457286432160804,
      "loss": 0.5015,
      "step": 275
    },
    {
      "epoch": 2.2904564315352696,
      "grad_norm": 1.633615255355835,
      "learning_rate": 0.00014552763819095479,
      "loss": 0.5829,
      "step": 276
    },
    {
      "epoch": 2.2987551867219915,
      "grad_norm": 1.5225706100463867,
      "learning_rate": 0.00014532663316582916,
      "loss": 0.4864,
      "step": 277
    },
    {
      "epoch": 2.3070539419087135,
      "grad_norm": 1.5390501022338867,
      "learning_rate": 0.00014512562814070353,
      "loss": 0.5627,
      "step": 278
    },
    {
      "epoch": 2.3153526970954355,
      "grad_norm": 1.5574965476989746,
      "learning_rate": 0.0001449246231155779,
      "loss": 0.5721,
      "step": 279
    },
    {
      "epoch": 2.323651452282158,
      "grad_norm": 1.4507496356964111,
      "learning_rate": 0.00014472361809045228,
      "loss": 0.417,
      "step": 280
    },
    {
      "epoch": 2.33195020746888,
      "grad_norm": 1.6556905508041382,
      "learning_rate": 0.00014452261306532665,
      "loss": 0.6319,
      "step": 281
    },
    {
      "epoch": 2.340248962655602,
      "grad_norm": 1.269890546798706,
      "learning_rate": 0.000144321608040201,
      "loss": 0.5073,
      "step": 282
    },
    {
      "epoch": 2.3485477178423237,
      "grad_norm": 1.3045636415481567,
      "learning_rate": 0.00014412060301507537,
      "loss": 0.4349,
      "step": 283
    },
    {
      "epoch": 2.3568464730290457,
      "grad_norm": 1.5845695734024048,
      "learning_rate": 0.00014391959798994977,
      "loss": 0.4871,
      "step": 284
    },
    {
      "epoch": 2.3651452282157677,
      "grad_norm": 1.4645051956176758,
      "learning_rate": 0.00014371859296482411,
      "loss": 0.496,
      "step": 285
    },
    {
      "epoch": 2.3734439834024896,
      "grad_norm": 1.293988823890686,
      "learning_rate": 0.0001435175879396985,
      "loss": 0.4848,
      "step": 286
    },
    {
      "epoch": 2.3817427385892116,
      "grad_norm": 1.4349843263626099,
      "learning_rate": 0.00014331658291457286,
      "loss": 0.515,
      "step": 287
    },
    {
      "epoch": 2.3900414937759336,
      "grad_norm": 1.411733627319336,
      "learning_rate": 0.00014311557788944726,
      "loss": 0.5961,
      "step": 288
    },
    {
      "epoch": 2.3983402489626555,
      "grad_norm": 1.5085761547088623,
      "learning_rate": 0.0001429145728643216,
      "loss": 0.6686,
      "step": 289
    },
    {
      "epoch": 2.4066390041493775,
      "grad_norm": 1.5481432676315308,
      "learning_rate": 0.00014271356783919598,
      "loss": 0.5847,
      "step": 290
    },
    {
      "epoch": 2.4149377593360994,
      "grad_norm": 1.488020896911621,
      "learning_rate": 0.00014251256281407035,
      "loss": 0.559,
      "step": 291
    },
    {
      "epoch": 2.4232365145228214,
      "grad_norm": 1.1918635368347168,
      "learning_rate": 0.00014231155778894473,
      "loss": 0.4551,
      "step": 292
    },
    {
      "epoch": 2.431535269709544,
      "grad_norm": 1.418962836265564,
      "learning_rate": 0.0001421105527638191,
      "loss": 0.6567,
      "step": 293
    },
    {
      "epoch": 2.4398340248962658,
      "grad_norm": 1.4779499769210815,
      "learning_rate": 0.00014190954773869347,
      "loss": 0.5272,
      "step": 294
    },
    {
      "epoch": 2.4481327800829877,
      "grad_norm": 1.2152917385101318,
      "learning_rate": 0.00014170854271356784,
      "loss": 0.4129,
      "step": 295
    },
    {
      "epoch": 2.4564315352697097,
      "grad_norm": 1.3103164434432983,
      "learning_rate": 0.00014150753768844222,
      "loss": 0.434,
      "step": 296
    },
    {
      "epoch": 2.4647302904564317,
      "grad_norm": 1.5716071128845215,
      "learning_rate": 0.0001413065326633166,
      "loss": 0.6958,
      "step": 297
    },
    {
      "epoch": 2.4730290456431536,
      "grad_norm": 1.5472100973129272,
      "learning_rate": 0.00014110552763819096,
      "loss": 0.5077,
      "step": 298
    },
    {
      "epoch": 2.4813278008298756,
      "grad_norm": 1.6116987466812134,
      "learning_rate": 0.00014090452261306534,
      "loss": 0.561,
      "step": 299
    },
    {
      "epoch": 2.4896265560165975,
      "grad_norm": 1.7108062505722046,
      "learning_rate": 0.0001407035175879397,
      "loss": 0.5897,
      "step": 300
    },
    {
      "epoch": 2.4979253112033195,
      "grad_norm": 1.3344438076019287,
      "learning_rate": 0.00014050251256281408,
      "loss": 0.3637,
      "step": 301
    },
    {
      "epoch": 2.5062240663900415,
      "grad_norm": 1.6170378923416138,
      "learning_rate": 0.00014030150753768846,
      "loss": 0.6306,
      "step": 302
    },
    {
      "epoch": 2.5145228215767634,
      "grad_norm": 1.3905774354934692,
      "learning_rate": 0.0001401005025125628,
      "loss": 0.3971,
      "step": 303
    },
    {
      "epoch": 2.5228215767634854,
      "grad_norm": 1.2503491640090942,
      "learning_rate": 0.0001398994974874372,
      "loss": 0.3916,
      "step": 304
    },
    {
      "epoch": 2.5311203319502074,
      "grad_norm": 1.4203201532363892,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.5421,
      "step": 305
    },
    {
      "epoch": 2.5394190871369293,
      "grad_norm": 1.5038259029388428,
      "learning_rate": 0.00013949748743718595,
      "loss": 0.4543,
      "step": 306
    },
    {
      "epoch": 2.5477178423236513,
      "grad_norm": 1.6125001907348633,
      "learning_rate": 0.0001392964824120603,
      "loss": 0.6194,
      "step": 307
    },
    {
      "epoch": 2.5560165975103732,
      "grad_norm": 1.4636293649673462,
      "learning_rate": 0.00013909547738693467,
      "loss": 0.4445,
      "step": 308
    },
    {
      "epoch": 2.564315352697095,
      "grad_norm": 1.5559983253479004,
      "learning_rate": 0.00013889447236180907,
      "loss": 0.5097,
      "step": 309
    },
    {
      "epoch": 2.572614107883817,
      "grad_norm": 1.5436317920684814,
      "learning_rate": 0.0001386934673366834,
      "loss": 0.4196,
      "step": 310
    },
    {
      "epoch": 2.5809128630705396,
      "grad_norm": 1.5327664613723755,
      "learning_rate": 0.00013849246231155778,
      "loss": 0.4551,
      "step": 311
    },
    {
      "epoch": 2.5892116182572615,
      "grad_norm": 1.5487794876098633,
      "learning_rate": 0.00013829145728643216,
      "loss": 0.4719,
      "step": 312
    },
    {
      "epoch": 2.5975103734439835,
      "grad_norm": 1.386175513267517,
      "learning_rate": 0.00013809045226130656,
      "loss": 0.5179,
      "step": 313
    },
    {
      "epoch": 2.6058091286307055,
      "grad_norm": 1.5780686140060425,
      "learning_rate": 0.0001378894472361809,
      "loss": 0.5392,
      "step": 314
    },
    {
      "epoch": 2.6141078838174274,
      "grad_norm": 1.3493915796279907,
      "learning_rate": 0.00013768844221105528,
      "loss": 0.4789,
      "step": 315
    },
    {
      "epoch": 2.6224066390041494,
      "grad_norm": 1.3132704496383667,
      "learning_rate": 0.00013748743718592965,
      "loss": 0.475,
      "step": 316
    },
    {
      "epoch": 2.6307053941908713,
      "grad_norm": 1.408057451248169,
      "learning_rate": 0.00013728643216080402,
      "loss": 0.5086,
      "step": 317
    },
    {
      "epoch": 2.6390041493775933,
      "grad_norm": 1.4688434600830078,
      "learning_rate": 0.0001370854271356784,
      "loss": 0.5581,
      "step": 318
    },
    {
      "epoch": 2.6473029045643153,
      "grad_norm": 1.629462480545044,
      "learning_rate": 0.00013688442211055277,
      "loss": 0.5956,
      "step": 319
    },
    {
      "epoch": 2.6556016597510372,
      "grad_norm": 1.3723214864730835,
      "learning_rate": 0.00013668341708542714,
      "loss": 0.4978,
      "step": 320
    },
    {
      "epoch": 2.663900414937759,
      "grad_norm": 1.6624574661254883,
      "learning_rate": 0.00013648241206030151,
      "loss": 0.6127,
      "step": 321
    },
    {
      "epoch": 2.6721991701244816,
      "grad_norm": 1.589137077331543,
      "learning_rate": 0.0001362814070351759,
      "loss": 0.5504,
      "step": 322
    },
    {
      "epoch": 2.6804979253112036,
      "grad_norm": 1.6160728931427002,
      "learning_rate": 0.00013608040201005026,
      "loss": 0.4938,
      "step": 323
    },
    {
      "epoch": 2.6887966804979255,
      "grad_norm": 1.4274375438690186,
      "learning_rate": 0.00013587939698492463,
      "loss": 0.5306,
      "step": 324
    },
    {
      "epoch": 2.6970954356846475,
      "grad_norm": 1.4162631034851074,
      "learning_rate": 0.000135678391959799,
      "loss": 0.4896,
      "step": 325
    },
    {
      "epoch": 2.7053941908713695,
      "grad_norm": 1.3767859935760498,
      "learning_rate": 0.00013547738693467338,
      "loss": 0.5601,
      "step": 326
    },
    {
      "epoch": 2.7136929460580914,
      "grad_norm": 1.6843544244766235,
      "learning_rate": 0.00013527638190954775,
      "loss": 0.6061,
      "step": 327
    },
    {
      "epoch": 2.7219917012448134,
      "grad_norm": 1.3982195854187012,
      "learning_rate": 0.0001350753768844221,
      "loss": 0.5964,
      "step": 328
    },
    {
      "epoch": 2.7302904564315353,
      "grad_norm": 1.6013236045837402,
      "learning_rate": 0.00013487437185929647,
      "loss": 0.573,
      "step": 329
    },
    {
      "epoch": 2.7385892116182573,
      "grad_norm": 1.512844204902649,
      "learning_rate": 0.00013467336683417087,
      "loss": 0.6266,
      "step": 330
    },
    {
      "epoch": 2.7468879668049793,
      "grad_norm": 1.2383170127868652,
      "learning_rate": 0.00013447236180904524,
      "loss": 0.421,
      "step": 331
    },
    {
      "epoch": 2.7551867219917012,
      "grad_norm": 1.1637558937072754,
      "learning_rate": 0.0001342713567839196,
      "loss": 0.3855,
      "step": 332
    },
    {
      "epoch": 2.763485477178423,
      "grad_norm": 1.364800214767456,
      "learning_rate": 0.00013407035175879396,
      "loss": 0.5157,
      "step": 333
    },
    {
      "epoch": 2.771784232365145,
      "grad_norm": 1.525434970855713,
      "learning_rate": 0.00013386934673366836,
      "loss": 0.6112,
      "step": 334
    },
    {
      "epoch": 2.780082987551867,
      "grad_norm": 1.3496448993682861,
      "learning_rate": 0.0001336683417085427,
      "loss": 0.5108,
      "step": 335
    },
    {
      "epoch": 2.788381742738589,
      "grad_norm": 1.296197772026062,
      "learning_rate": 0.00013346733668341708,
      "loss": 0.5022,
      "step": 336
    },
    {
      "epoch": 2.796680497925311,
      "grad_norm": 1.3150502443313599,
      "learning_rate": 0.00013326633165829146,
      "loss": 0.4493,
      "step": 337
    },
    {
      "epoch": 2.804979253112033,
      "grad_norm": 1.3933988809585571,
      "learning_rate": 0.00013306532663316586,
      "loss": 0.5702,
      "step": 338
    },
    {
      "epoch": 2.813278008298755,
      "grad_norm": 1.4046320915222168,
      "learning_rate": 0.0001328643216080402,
      "loss": 0.5101,
      "step": 339
    },
    {
      "epoch": 2.821576763485477,
      "grad_norm": 1.4432249069213867,
      "learning_rate": 0.00013266331658291457,
      "loss": 0.6391,
      "step": 340
    },
    {
      "epoch": 2.8298755186721993,
      "grad_norm": 1.4560797214508057,
      "learning_rate": 0.00013246231155778895,
      "loss": 0.5309,
      "step": 341
    },
    {
      "epoch": 2.8381742738589213,
      "grad_norm": 1.3300548791885376,
      "learning_rate": 0.00013226130653266332,
      "loss": 0.466,
      "step": 342
    },
    {
      "epoch": 2.8464730290456433,
      "grad_norm": 1.3274461030960083,
      "learning_rate": 0.0001320603015075377,
      "loss": 0.5081,
      "step": 343
    },
    {
      "epoch": 2.854771784232365,
      "grad_norm": 1.2527449131011963,
      "learning_rate": 0.00013185929648241207,
      "loss": 0.4719,
      "step": 344
    },
    {
      "epoch": 2.863070539419087,
      "grad_norm": 1.378798484802246,
      "learning_rate": 0.00013165829145728644,
      "loss": 0.4598,
      "step": 345
    },
    {
      "epoch": 2.871369294605809,
      "grad_norm": 1.4690757989883423,
      "learning_rate": 0.0001314572864321608,
      "loss": 0.5713,
      "step": 346
    },
    {
      "epoch": 2.879668049792531,
      "grad_norm": 1.4666279554367065,
      "learning_rate": 0.00013125628140703518,
      "loss": 0.5453,
      "step": 347
    },
    {
      "epoch": 2.887966804979253,
      "grad_norm": 1.6002241373062134,
      "learning_rate": 0.00013105527638190956,
      "loss": 0.5778,
      "step": 348
    },
    {
      "epoch": 2.896265560165975,
      "grad_norm": 1.3132392168045044,
      "learning_rate": 0.00013085427135678393,
      "loss": 0.3643,
      "step": 349
    },
    {
      "epoch": 2.904564315352697,
      "grad_norm": 1.668445348739624,
      "learning_rate": 0.0001306532663316583,
      "loss": 0.6205,
      "step": 350
    },
    {
      "epoch": 2.912863070539419,
      "grad_norm": 1.3509478569030762,
      "learning_rate": 0.00013045226130653268,
      "loss": 0.4044,
      "step": 351
    },
    {
      "epoch": 2.921161825726141,
      "grad_norm": 1.5193318128585815,
      "learning_rate": 0.00013025125628140705,
      "loss": 0.4695,
      "step": 352
    },
    {
      "epoch": 2.9294605809128633,
      "grad_norm": 1.5286747217178345,
      "learning_rate": 0.0001300502512562814,
      "loss": 0.5294,
      "step": 353
    },
    {
      "epoch": 2.9377593360995853,
      "grad_norm": 1.4287731647491455,
      "learning_rate": 0.00012984924623115577,
      "loss": 0.4803,
      "step": 354
    },
    {
      "epoch": 2.9460580912863072,
      "grad_norm": 1.3258037567138672,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.438,
      "step": 355
    },
    {
      "epoch": 2.954356846473029,
      "grad_norm": 1.4105286598205566,
      "learning_rate": 0.00012944723618090454,
      "loss": 0.4822,
      "step": 356
    },
    {
      "epoch": 2.962655601659751,
      "grad_norm": 1.3815860748291016,
      "learning_rate": 0.0001292462311557789,
      "loss": 0.5421,
      "step": 357
    },
    {
      "epoch": 2.970954356846473,
      "grad_norm": 1.7275184392929077,
      "learning_rate": 0.00012904522613065326,
      "loss": 0.5488,
      "step": 358
    },
    {
      "epoch": 2.979253112033195,
      "grad_norm": 1.4605810642242432,
      "learning_rate": 0.00012884422110552766,
      "loss": 0.4857,
      "step": 359
    },
    {
      "epoch": 2.987551867219917,
      "grad_norm": 1.6668367385864258,
      "learning_rate": 0.000128643216080402,
      "loss": 0.6744,
      "step": 360
    },
    {
      "epoch": 2.995850622406639,
      "grad_norm": 1.6439107656478882,
      "learning_rate": 0.00012844221105527638,
      "loss": 0.6162,
      "step": 361
    },
    {
      "epoch": 3.004149377593361,
      "grad_norm": 1.2980343103408813,
      "learning_rate": 0.00012824120603015075,
      "loss": 0.3288,
      "step": 362
    },
    {
      "epoch": 3.012448132780083,
      "grad_norm": 0.9949775338172913,
      "learning_rate": 0.00012804020100502515,
      "loss": 0.2816,
      "step": 363
    },
    {
      "epoch": 3.020746887966805,
      "grad_norm": 1.3273799419403076,
      "learning_rate": 0.0001278391959798995,
      "loss": 0.3171,
      "step": 364
    },
    {
      "epoch": 3.029045643153527,
      "grad_norm": 1.3138936758041382,
      "learning_rate": 0.00012763819095477387,
      "loss": 0.343,
      "step": 365
    },
    {
      "epoch": 3.037344398340249,
      "grad_norm": 1.2748262882232666,
      "learning_rate": 0.00012743718592964824,
      "loss": 0.3094,
      "step": 366
    },
    {
      "epoch": 3.045643153526971,
      "grad_norm": 1.1729071140289307,
      "learning_rate": 0.00012723618090452262,
      "loss": 0.25,
      "step": 367
    },
    {
      "epoch": 3.0539419087136928,
      "grad_norm": 1.5705856084823608,
      "learning_rate": 0.000127035175879397,
      "loss": 0.3983,
      "step": 368
    },
    {
      "epoch": 3.0622406639004147,
      "grad_norm": 1.2656395435333252,
      "learning_rate": 0.00012683417085427136,
      "loss": 0.2342,
      "step": 369
    },
    {
      "epoch": 3.070539419087137,
      "grad_norm": 1.5403857231140137,
      "learning_rate": 0.00012663316582914574,
      "loss": 0.2225,
      "step": 370
    },
    {
      "epoch": 3.078838174273859,
      "grad_norm": 1.498780608177185,
      "learning_rate": 0.0001264321608040201,
      "loss": 0.2395,
      "step": 371
    },
    {
      "epoch": 3.087136929460581,
      "grad_norm": 2.152519941329956,
      "learning_rate": 0.00012623115577889448,
      "loss": 0.2919,
      "step": 372
    },
    {
      "epoch": 3.095435684647303,
      "grad_norm": 2.0457377433776855,
      "learning_rate": 0.00012603015075376885,
      "loss": 0.3276,
      "step": 373
    },
    {
      "epoch": 3.103734439834025,
      "grad_norm": 1.6457867622375488,
      "learning_rate": 0.00012582914572864323,
      "loss": 0.2551,
      "step": 374
    },
    {
      "epoch": 3.112033195020747,
      "grad_norm": 1.990527629852295,
      "learning_rate": 0.0001256281407035176,
      "loss": 0.3764,
      "step": 375
    },
    {
      "epoch": 3.120331950207469,
      "grad_norm": 2.1637842655181885,
      "learning_rate": 0.00012542713567839197,
      "loss": 0.2699,
      "step": 376
    },
    {
      "epoch": 3.128630705394191,
      "grad_norm": 1.9699734449386597,
      "learning_rate": 0.00012522613065326635,
      "loss": 0.2693,
      "step": 377
    },
    {
      "epoch": 3.136929460580913,
      "grad_norm": 2.0203089714050293,
      "learning_rate": 0.0001250251256281407,
      "loss": 0.3985,
      "step": 378
    },
    {
      "epoch": 3.145228215767635,
      "grad_norm": 1.7054259777069092,
      "learning_rate": 0.00012482412060301507,
      "loss": 0.3108,
      "step": 379
    },
    {
      "epoch": 3.1535269709543567,
      "grad_norm": 1.7709321975708008,
      "learning_rate": 0.00012462311557788947,
      "loss": 0.3441,
      "step": 380
    },
    {
      "epoch": 3.1618257261410787,
      "grad_norm": 1.743093490600586,
      "learning_rate": 0.00012442211055276384,
      "loss": 0.3682,
      "step": 381
    },
    {
      "epoch": 3.1701244813278007,
      "grad_norm": 1.4276049137115479,
      "learning_rate": 0.00012422110552763818,
      "loss": 0.261,
      "step": 382
    },
    {
      "epoch": 3.1784232365145226,
      "grad_norm": 1.4463350772857666,
      "learning_rate": 0.00012402010050251256,
      "loss": 0.2906,
      "step": 383
    },
    {
      "epoch": 3.186721991701245,
      "grad_norm": 1.373047113418579,
      "learning_rate": 0.00012381909547738696,
      "loss": 0.2677,
      "step": 384
    },
    {
      "epoch": 3.195020746887967,
      "grad_norm": 1.5100605487823486,
      "learning_rate": 0.0001236180904522613,
      "loss": 0.3099,
      "step": 385
    },
    {
      "epoch": 3.203319502074689,
      "grad_norm": 1.569165825843811,
      "learning_rate": 0.00012341708542713568,
      "loss": 0.3448,
      "step": 386
    },
    {
      "epoch": 3.211618257261411,
      "grad_norm": 1.6049097776412964,
      "learning_rate": 0.00012321608040201005,
      "loss": 0.2713,
      "step": 387
    },
    {
      "epoch": 3.219917012448133,
      "grad_norm": 1.851670265197754,
      "learning_rate": 0.00012301507537688445,
      "loss": 0.3652,
      "step": 388
    },
    {
      "epoch": 3.228215767634855,
      "grad_norm": 1.4777473211288452,
      "learning_rate": 0.0001228140703517588,
      "loss": 0.333,
      "step": 389
    },
    {
      "epoch": 3.236514522821577,
      "grad_norm": 1.3974393606185913,
      "learning_rate": 0.00012261306532663317,
      "loss": 0.2542,
      "step": 390
    },
    {
      "epoch": 3.2448132780082988,
      "grad_norm": 1.483158826828003,
      "learning_rate": 0.00012241206030150754,
      "loss": 0.2806,
      "step": 391
    },
    {
      "epoch": 3.2531120331950207,
      "grad_norm": 1.5087217092514038,
      "learning_rate": 0.00012221105527638191,
      "loss": 0.2946,
      "step": 392
    },
    {
      "epoch": 3.2614107883817427,
      "grad_norm": 1.6400272846221924,
      "learning_rate": 0.00012201005025125629,
      "loss": 0.3357,
      "step": 393
    },
    {
      "epoch": 3.2697095435684647,
      "grad_norm": 1.8245055675506592,
      "learning_rate": 0.00012180904522613066,
      "loss": 0.3752,
      "step": 394
    },
    {
      "epoch": 3.2780082987551866,
      "grad_norm": 1.3522634506225586,
      "learning_rate": 0.00012160804020100502,
      "loss": 0.2813,
      "step": 395
    },
    {
      "epoch": 3.2863070539419086,
      "grad_norm": 2.292672634124756,
      "learning_rate": 0.00012140703517587942,
      "loss": 0.3503,
      "step": 396
    },
    {
      "epoch": 3.2946058091286305,
      "grad_norm": 1.5787632465362549,
      "learning_rate": 0.00012120603015075378,
      "loss": 0.3053,
      "step": 397
    },
    {
      "epoch": 3.3029045643153525,
      "grad_norm": 1.5995595455169678,
      "learning_rate": 0.00012100502512562815,
      "loss": 0.3786,
      "step": 398
    },
    {
      "epoch": 3.3112033195020745,
      "grad_norm": 1.5775854587554932,
      "learning_rate": 0.00012080402010050251,
      "loss": 0.2661,
      "step": 399
    },
    {
      "epoch": 3.3195020746887964,
      "grad_norm": 1.4307591915130615,
      "learning_rate": 0.00012060301507537688,
      "loss": 0.2648,
      "step": 400
    },
    {
      "epoch": 3.327800829875519,
      "grad_norm": 1.75480318069458,
      "learning_rate": 0.00012040201005025127,
      "loss": 0.2852,
      "step": 401
    },
    {
      "epoch": 3.336099585062241,
      "grad_norm": 1.7646872997283936,
      "learning_rate": 0.00012020100502512563,
      "loss": 0.3003,
      "step": 402
    },
    {
      "epoch": 3.3443983402489628,
      "grad_norm": 1.785440444946289,
      "learning_rate": 0.00012,
      "loss": 0.2849,
      "step": 403
    },
    {
      "epoch": 3.3526970954356847,
      "grad_norm": 1.388603687286377,
      "learning_rate": 0.00011979899497487436,
      "loss": 0.2307,
      "step": 404
    },
    {
      "epoch": 3.3609958506224067,
      "grad_norm": 1.5026975870132446,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.2813,
      "step": 405
    },
    {
      "epoch": 3.3692946058091287,
      "grad_norm": 1.7790018320083618,
      "learning_rate": 0.00011939698492462312,
      "loss": 0.3112,
      "step": 406
    },
    {
      "epoch": 3.3775933609958506,
      "grad_norm": 1.5257751941680908,
      "learning_rate": 0.0001191959798994975,
      "loss": 0.2586,
      "step": 407
    },
    {
      "epoch": 3.3858921161825726,
      "grad_norm": 1.251250982284546,
      "learning_rate": 0.00011899497487437185,
      "loss": 0.2715,
      "step": 408
    },
    {
      "epoch": 3.3941908713692945,
      "grad_norm": 1.684721827507019,
      "learning_rate": 0.00011879396984924624,
      "loss": 0.365,
      "step": 409
    },
    {
      "epoch": 3.4024896265560165,
      "grad_norm": 1.561068058013916,
      "learning_rate": 0.00011859296482412061,
      "loss": 0.2675,
      "step": 410
    },
    {
      "epoch": 3.4107883817427385,
      "grad_norm": 1.8195722103118896,
      "learning_rate": 0.00011839195979899497,
      "loss": 0.3503,
      "step": 411
    },
    {
      "epoch": 3.4190871369294604,
      "grad_norm": 1.6030350923538208,
      "learning_rate": 0.00011819095477386935,
      "loss": 0.3374,
      "step": 412
    },
    {
      "epoch": 3.4273858921161824,
      "grad_norm": 1.6927751302719116,
      "learning_rate": 0.00011798994974874373,
      "loss": 0.3239,
      "step": 413
    },
    {
      "epoch": 3.435684647302905,
      "grad_norm": 1.713119626045227,
      "learning_rate": 0.0001177889447236181,
      "loss": 0.3539,
      "step": 414
    },
    {
      "epoch": 3.4439834024896268,
      "grad_norm": 1.6695854663848877,
      "learning_rate": 0.00011758793969849247,
      "loss": 0.2646,
      "step": 415
    },
    {
      "epoch": 3.4522821576763487,
      "grad_norm": 1.5293790102005005,
      "learning_rate": 0.00011738693467336684,
      "loss": 0.2884,
      "step": 416
    },
    {
      "epoch": 3.4605809128630707,
      "grad_norm": 1.5337064266204834,
      "learning_rate": 0.00011718592964824122,
      "loss": 0.2703,
      "step": 417
    },
    {
      "epoch": 3.4688796680497926,
      "grad_norm": 1.7782806158065796,
      "learning_rate": 0.00011698492462311558,
      "loss": 0.3427,
      "step": 418
    },
    {
      "epoch": 3.4771784232365146,
      "grad_norm": 1.6614301204681396,
      "learning_rate": 0.00011678391959798996,
      "loss": 0.3049,
      "step": 419
    },
    {
      "epoch": 3.4854771784232366,
      "grad_norm": 1.931740164756775,
      "learning_rate": 0.00011658291457286432,
      "loss": 0.3682,
      "step": 420
    },
    {
      "epoch": 3.4937759336099585,
      "grad_norm": 1.9377493858337402,
      "learning_rate": 0.00011638190954773872,
      "loss": 0.3339,
      "step": 421
    },
    {
      "epoch": 3.5020746887966805,
      "grad_norm": 1.7052581310272217,
      "learning_rate": 0.00011618090452261308,
      "loss": 0.2986,
      "step": 422
    },
    {
      "epoch": 3.5103734439834025,
      "grad_norm": 1.4318687915802002,
      "learning_rate": 0.00011597989949748745,
      "loss": 0.2872,
      "step": 423
    },
    {
      "epoch": 3.5186721991701244,
      "grad_norm": 1.9974919557571411,
      "learning_rate": 0.00011577889447236181,
      "loss": 0.4528,
      "step": 424
    },
    {
      "epoch": 3.5269709543568464,
      "grad_norm": 1.4653384685516357,
      "learning_rate": 0.00011557788944723618,
      "loss": 0.2848,
      "step": 425
    },
    {
      "epoch": 3.5352697095435683,
      "grad_norm": 1.6809428930282593,
      "learning_rate": 0.00011537688442211057,
      "loss": 0.3389,
      "step": 426
    },
    {
      "epoch": 3.5435684647302903,
      "grad_norm": 1.6793209314346313,
      "learning_rate": 0.00011517587939698493,
      "loss": 0.3269,
      "step": 427
    },
    {
      "epoch": 3.5518672199170123,
      "grad_norm": 1.2731343507766724,
      "learning_rate": 0.0001149748743718593,
      "loss": 0.2155,
      "step": 428
    },
    {
      "epoch": 3.5601659751037342,
      "grad_norm": 1.6625101566314697,
      "learning_rate": 0.00011477386934673366,
      "loss": 0.374,
      "step": 429
    },
    {
      "epoch": 3.568464730290456,
      "grad_norm": 1.384021520614624,
      "learning_rate": 0.00011457286432160806,
      "loss": 0.2368,
      "step": 430
    },
    {
      "epoch": 3.576763485477178,
      "grad_norm": 1.7394168376922607,
      "learning_rate": 0.00011437185929648242,
      "loss": 0.3383,
      "step": 431
    },
    {
      "epoch": 3.5850622406639006,
      "grad_norm": 1.7575079202651978,
      "learning_rate": 0.00011417085427135679,
      "loss": 0.323,
      "step": 432
    },
    {
      "epoch": 3.5933609958506225,
      "grad_norm": 1.8756413459777832,
      "learning_rate": 0.00011396984924623115,
      "loss": 0.3938,
      "step": 433
    },
    {
      "epoch": 3.6016597510373445,
      "grad_norm": 1.6502516269683838,
      "learning_rate": 0.00011376884422110554,
      "loss": 0.3438,
      "step": 434
    },
    {
      "epoch": 3.6099585062240664,
      "grad_norm": 1.3170372247695923,
      "learning_rate": 0.00011356783919597991,
      "loss": 0.2741,
      "step": 435
    },
    {
      "epoch": 3.6182572614107884,
      "grad_norm": 1.383571743965149,
      "learning_rate": 0.00011336683417085427,
      "loss": 0.236,
      "step": 436
    },
    {
      "epoch": 3.6265560165975104,
      "grad_norm": 1.7583584785461426,
      "learning_rate": 0.00011316582914572864,
      "loss": 0.2911,
      "step": 437
    },
    {
      "epoch": 3.6348547717842323,
      "grad_norm": 1.4136300086975098,
      "learning_rate": 0.00011296482412060303,
      "loss": 0.2718,
      "step": 438
    },
    {
      "epoch": 3.6431535269709543,
      "grad_norm": 1.435177206993103,
      "learning_rate": 0.0001127638190954774,
      "loss": 0.2737,
      "step": 439
    },
    {
      "epoch": 3.6514522821576763,
      "grad_norm": 1.9127167463302612,
      "learning_rate": 0.00011256281407035176,
      "loss": 0.3877,
      "step": 440
    },
    {
      "epoch": 3.659751037344398,
      "grad_norm": 1.628758430480957,
      "learning_rate": 0.00011236180904522614,
      "loss": 0.2323,
      "step": 441
    },
    {
      "epoch": 3.66804979253112,
      "grad_norm": 1.8547006845474243,
      "learning_rate": 0.00011216080402010052,
      "loss": 0.3143,
      "step": 442
    },
    {
      "epoch": 3.6763485477178426,
      "grad_norm": 1.7076060771942139,
      "learning_rate": 0.00011195979899497488,
      "loss": 0.2936,
      "step": 443
    },
    {
      "epoch": 3.6846473029045645,
      "grad_norm": 1.410319209098816,
      "learning_rate": 0.00011175879396984925,
      "loss": 0.2542,
      "step": 444
    },
    {
      "epoch": 3.6929460580912865,
      "grad_norm": 1.997183918952942,
      "learning_rate": 0.00011155778894472361,
      "loss": 0.3589,
      "step": 445
    },
    {
      "epoch": 3.7012448132780085,
      "grad_norm": 1.7927271127700806,
      "learning_rate": 0.00011135678391959799,
      "loss": 0.3609,
      "step": 446
    },
    {
      "epoch": 3.7095435684647304,
      "grad_norm": 1.8161580562591553,
      "learning_rate": 0.00011115577889447237,
      "loss": 0.3863,
      "step": 447
    },
    {
      "epoch": 3.7178423236514524,
      "grad_norm": 1.2800289392471313,
      "learning_rate": 0.00011095477386934675,
      "loss": 0.2233,
      "step": 448
    },
    {
      "epoch": 3.7261410788381744,
      "grad_norm": 1.5680533647537231,
      "learning_rate": 0.0001107537688442211,
      "loss": 0.2681,
      "step": 449
    },
    {
      "epoch": 3.7344398340248963,
      "grad_norm": 1.7572847604751587,
      "learning_rate": 0.00011055276381909548,
      "loss": 0.2943,
      "step": 450
    },
    {
      "epoch": 3.7427385892116183,
      "grad_norm": 1.3184082508087158,
      "learning_rate": 0.00011035175879396986,
      "loss": 0.2227,
      "step": 451
    },
    {
      "epoch": 3.7510373443983402,
      "grad_norm": 1.3905905485153198,
      "learning_rate": 0.00011015075376884422,
      "loss": 0.2267,
      "step": 452
    },
    {
      "epoch": 3.759336099585062,
      "grad_norm": 1.8163409233093262,
      "learning_rate": 0.0001099497487437186,
      "loss": 0.2645,
      "step": 453
    },
    {
      "epoch": 3.767634854771784,
      "grad_norm": 1.5182335376739502,
      "learning_rate": 0.00010974874371859296,
      "loss": 0.3135,
      "step": 454
    },
    {
      "epoch": 3.775933609958506,
      "grad_norm": 1.8685775995254517,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.2897,
      "step": 455
    },
    {
      "epoch": 3.784232365145228,
      "grad_norm": 1.936037302017212,
      "learning_rate": 0.00010934673366834172,
      "loss": 0.3515,
      "step": 456
    },
    {
      "epoch": 3.79253112033195,
      "grad_norm": 1.59627366065979,
      "learning_rate": 0.00010914572864321609,
      "loss": 0.3386,
      "step": 457
    },
    {
      "epoch": 3.800829875518672,
      "grad_norm": 1.7852853536605835,
      "learning_rate": 0.00010894472361809045,
      "loss": 0.3541,
      "step": 458
    },
    {
      "epoch": 3.809128630705394,
      "grad_norm": 1.6673699617385864,
      "learning_rate": 0.00010874371859296483,
      "loss": 0.3551,
      "step": 459
    },
    {
      "epoch": 3.817427385892116,
      "grad_norm": 1.6056805849075317,
      "learning_rate": 0.00010854271356783921,
      "loss": 0.3121,
      "step": 460
    },
    {
      "epoch": 3.825726141078838,
      "grad_norm": 1.9683942794799805,
      "learning_rate": 0.00010834170854271357,
      "loss": 0.357,
      "step": 461
    },
    {
      "epoch": 3.8340248962655603,
      "grad_norm": 1.584997534751892,
      "learning_rate": 0.00010814070351758794,
      "loss": 0.245,
      "step": 462
    },
    {
      "epoch": 3.8423236514522823,
      "grad_norm": 1.7992727756500244,
      "learning_rate": 0.00010793969849246233,
      "loss": 0.3382,
      "step": 463
    },
    {
      "epoch": 3.8506224066390042,
      "grad_norm": 1.4429272413253784,
      "learning_rate": 0.0001077386934673367,
      "loss": 0.2822,
      "step": 464
    },
    {
      "epoch": 3.858921161825726,
      "grad_norm": 1.1504849195480347,
      "learning_rate": 0.00010753768844221106,
      "loss": 0.2219,
      "step": 465
    },
    {
      "epoch": 3.867219917012448,
      "grad_norm": 1.7789454460144043,
      "learning_rate": 0.00010733668341708543,
      "loss": 0.4031,
      "step": 466
    },
    {
      "epoch": 3.87551867219917,
      "grad_norm": 1.60355544090271,
      "learning_rate": 0.00010713567839195982,
      "loss": 0.308,
      "step": 467
    },
    {
      "epoch": 3.883817427385892,
      "grad_norm": 1.4862003326416016,
      "learning_rate": 0.00010693467336683418,
      "loss": 0.2755,
      "step": 468
    },
    {
      "epoch": 3.892116182572614,
      "grad_norm": 1.7626913785934448,
      "learning_rate": 0.00010673366834170855,
      "loss": 0.3377,
      "step": 469
    },
    {
      "epoch": 3.900414937759336,
      "grad_norm": 2.076101541519165,
      "learning_rate": 0.00010653266331658291,
      "loss": 0.3786,
      "step": 470
    },
    {
      "epoch": 3.908713692946058,
      "grad_norm": 1.790191650390625,
      "learning_rate": 0.00010633165829145728,
      "loss": 0.3904,
      "step": 471
    },
    {
      "epoch": 3.91701244813278,
      "grad_norm": 1.5966328382492065,
      "learning_rate": 0.00010613065326633167,
      "loss": 0.3486,
      "step": 472
    },
    {
      "epoch": 3.9253112033195023,
      "grad_norm": 1.5402107238769531,
      "learning_rate": 0.00010592964824120604,
      "loss": 0.2909,
      "step": 473
    },
    {
      "epoch": 3.9336099585062243,
      "grad_norm": 1.6316250562667847,
      "learning_rate": 0.0001057286432160804,
      "loss": 0.3231,
      "step": 474
    },
    {
      "epoch": 3.9419087136929463,
      "grad_norm": 1.528947353363037,
      "learning_rate": 0.00010552763819095478,
      "loss": 0.2669,
      "step": 475
    },
    {
      "epoch": 3.9502074688796682,
      "grad_norm": 1.5234590768814087,
      "learning_rate": 0.00010532663316582916,
      "loss": 0.3236,
      "step": 476
    },
    {
      "epoch": 3.95850622406639,
      "grad_norm": 1.5661605596542358,
      "learning_rate": 0.00010512562814070352,
      "loss": 0.3433,
      "step": 477
    },
    {
      "epoch": 3.966804979253112,
      "grad_norm": 1.7142928838729858,
      "learning_rate": 0.0001049246231155779,
      "loss": 0.3548,
      "step": 478
    },
    {
      "epoch": 3.975103734439834,
      "grad_norm": 2.0498223304748535,
      "learning_rate": 0.00010472361809045225,
      "loss": 0.3014,
      "step": 479
    },
    {
      "epoch": 3.983402489626556,
      "grad_norm": 1.7480872869491577,
      "learning_rate": 0.00010452261306532664,
      "loss": 0.3366,
      "step": 480
    },
    {
      "epoch": 3.991701244813278,
      "grad_norm": 1.642552137374878,
      "learning_rate": 0.00010432160804020101,
      "loss": 0.34,
      "step": 481
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.0438427925109863,
      "learning_rate": 0.00010412060301507539,
      "loss": 0.3159,
      "step": 482
    },
    {
      "epoch": 4.008298755186722,
      "grad_norm": 0.9733163714408875,
      "learning_rate": 0.00010391959798994975,
      "loss": 0.1746,
      "step": 483
    },
    {
      "epoch": 4.016597510373444,
      "grad_norm": 1.1325403451919556,
      "learning_rate": 0.00010371859296482413,
      "loss": 0.2169,
      "step": 484
    },
    {
      "epoch": 4.024896265560166,
      "grad_norm": 0.912590742111206,
      "learning_rate": 0.0001035175879396985,
      "loss": 0.1461,
      "step": 485
    },
    {
      "epoch": 4.033195020746888,
      "grad_norm": 0.9720569252967834,
      "learning_rate": 0.00010331658291457286,
      "loss": 0.1631,
      "step": 486
    },
    {
      "epoch": 4.04149377593361,
      "grad_norm": 0.9019319415092468,
      "learning_rate": 0.00010311557788944724,
      "loss": 0.1161,
      "step": 487
    },
    {
      "epoch": 4.049792531120332,
      "grad_norm": 1.3256398439407349,
      "learning_rate": 0.00010291457286432162,
      "loss": 0.1509,
      "step": 488
    },
    {
      "epoch": 4.058091286307054,
      "grad_norm": 1.3138483762741089,
      "learning_rate": 0.00010271356783919598,
      "loss": 0.1614,
      "step": 489
    },
    {
      "epoch": 4.066390041493776,
      "grad_norm": 1.6167417764663696,
      "learning_rate": 0.00010251256281407036,
      "loss": 0.1747,
      "step": 490
    },
    {
      "epoch": 4.074688796680498,
      "grad_norm": 1.7400906085968018,
      "learning_rate": 0.00010231155778894473,
      "loss": 0.1579,
      "step": 491
    },
    {
      "epoch": 4.08298755186722,
      "grad_norm": 1.4375563859939575,
      "learning_rate": 0.00010211055276381909,
      "loss": 0.1499,
      "step": 492
    },
    {
      "epoch": 4.091286307053942,
      "grad_norm": 1.3594424724578857,
      "learning_rate": 0.00010190954773869348,
      "loss": 0.1823,
      "step": 493
    },
    {
      "epoch": 4.0995850622406635,
      "grad_norm": 1.796951174736023,
      "learning_rate": 0.00010170854271356785,
      "loss": 0.1899,
      "step": 494
    },
    {
      "epoch": 4.1078838174273855,
      "grad_norm": 2.16772723197937,
      "learning_rate": 0.00010150753768844221,
      "loss": 0.2319,
      "step": 495
    },
    {
      "epoch": 4.1161825726141075,
      "grad_norm": 1.6479599475860596,
      "learning_rate": 0.00010130653266331658,
      "loss": 0.1914,
      "step": 496
    },
    {
      "epoch": 4.124481327800829,
      "grad_norm": 1.6715614795684814,
      "learning_rate": 0.00010110552763819097,
      "loss": 0.207,
      "step": 497
    },
    {
      "epoch": 4.132780082987552,
      "grad_norm": 2.0872504711151123,
      "learning_rate": 0.00010090452261306533,
      "loss": 0.2126,
      "step": 498
    },
    {
      "epoch": 4.141078838174274,
      "grad_norm": 1.2104239463806152,
      "learning_rate": 0.0001007035175879397,
      "loss": 0.1714,
      "step": 499
    },
    {
      "epoch": 4.149377593360996,
      "grad_norm": 1.6436537504196167,
      "learning_rate": 0.00010050251256281407,
      "loss": 0.2109,
      "step": 500
    },
    {
      "epoch": 4.157676348547718,
      "grad_norm": 1.3645967245101929,
      "learning_rate": 0.00010030150753768846,
      "loss": 0.1387,
      "step": 501
    },
    {
      "epoch": 4.16597510373444,
      "grad_norm": 1.4104175567626953,
      "learning_rate": 0.00010010050251256282,
      "loss": 0.1851,
      "step": 502
    },
    {
      "epoch": 4.174273858921162,
      "grad_norm": 1.2109479904174805,
      "learning_rate": 9.989949748743719e-05,
      "loss": 0.1417,
      "step": 503
    },
    {
      "epoch": 4.182572614107884,
      "grad_norm": 1.3064295053482056,
      "learning_rate": 9.969849246231156e-05,
      "loss": 0.1773,
      "step": 504
    },
    {
      "epoch": 4.190871369294606,
      "grad_norm": 1.794522762298584,
      "learning_rate": 9.949748743718594e-05,
      "loss": 0.1844,
      "step": 505
    },
    {
      "epoch": 4.199170124481328,
      "grad_norm": 1.3411674499511719,
      "learning_rate": 9.929648241206031e-05,
      "loss": 0.168,
      "step": 506
    },
    {
      "epoch": 4.20746887966805,
      "grad_norm": 1.5340691804885864,
      "learning_rate": 9.909547738693468e-05,
      "loss": 0.237,
      "step": 507
    },
    {
      "epoch": 4.215767634854772,
      "grad_norm": 1.4786258935928345,
      "learning_rate": 9.889447236180906e-05,
      "loss": 0.2147,
      "step": 508
    },
    {
      "epoch": 4.224066390041494,
      "grad_norm": 1.5990290641784668,
      "learning_rate": 9.869346733668342e-05,
      "loss": 0.217,
      "step": 509
    },
    {
      "epoch": 4.232365145228216,
      "grad_norm": 1.170043706893921,
      "learning_rate": 9.84924623115578e-05,
      "loss": 0.1553,
      "step": 510
    },
    {
      "epoch": 4.240663900414938,
      "grad_norm": 1.1200672388076782,
      "learning_rate": 9.829145728643216e-05,
      "loss": 0.1233,
      "step": 511
    },
    {
      "epoch": 4.24896265560166,
      "grad_norm": 1.1443681716918945,
      "learning_rate": 9.809045226130655e-05,
      "loss": 0.1452,
      "step": 512
    },
    {
      "epoch": 4.257261410788382,
      "grad_norm": 1.4839608669281006,
      "learning_rate": 9.788944723618091e-05,
      "loss": 0.2199,
      "step": 513
    },
    {
      "epoch": 4.265560165975104,
      "grad_norm": 1.1730594635009766,
      "learning_rate": 9.768844221105528e-05,
      "loss": 0.1536,
      "step": 514
    },
    {
      "epoch": 4.273858921161826,
      "grad_norm": 1.39699125289917,
      "learning_rate": 9.748743718592965e-05,
      "loss": 0.1898,
      "step": 515
    },
    {
      "epoch": 4.282157676348548,
      "grad_norm": 1.484057903289795,
      "learning_rate": 9.728643216080403e-05,
      "loss": 0.1643,
      "step": 516
    },
    {
      "epoch": 4.29045643153527,
      "grad_norm": 1.6057219505310059,
      "learning_rate": 9.70854271356784e-05,
      "loss": 0.2016,
      "step": 517
    },
    {
      "epoch": 4.2987551867219915,
      "grad_norm": 1.7851160764694214,
      "learning_rate": 9.688442211055276e-05,
      "loss": 0.2624,
      "step": 518
    },
    {
      "epoch": 4.3070539419087135,
      "grad_norm": 1.41278076171875,
      "learning_rate": 9.668341708542715e-05,
      "loss": 0.1769,
      "step": 519
    },
    {
      "epoch": 4.3153526970954355,
      "grad_norm": 1.948487401008606,
      "learning_rate": 9.64824120603015e-05,
      "loss": 0.1639,
      "step": 520
    },
    {
      "epoch": 4.323651452282157,
      "grad_norm": 1.5319994688034058,
      "learning_rate": 9.628140703517589e-05,
      "loss": 0.1988,
      "step": 521
    },
    {
      "epoch": 4.331950207468879,
      "grad_norm": 1.4854130744934082,
      "learning_rate": 9.608040201005025e-05,
      "loss": 0.185,
      "step": 522
    },
    {
      "epoch": 4.340248962655601,
      "grad_norm": 1.397639513015747,
      "learning_rate": 9.587939698492462e-05,
      "loss": 0.1973,
      "step": 523
    },
    {
      "epoch": 4.348547717842323,
      "grad_norm": 1.2757185697555542,
      "learning_rate": 9.5678391959799e-05,
      "loss": 0.1813,
      "step": 524
    },
    {
      "epoch": 4.356846473029045,
      "grad_norm": 1.4807848930358887,
      "learning_rate": 9.547738693467337e-05,
      "loss": 0.1644,
      "step": 525
    },
    {
      "epoch": 4.365145228215767,
      "grad_norm": 1.261371374130249,
      "learning_rate": 9.527638190954774e-05,
      "loss": 0.1875,
      "step": 526
    },
    {
      "epoch": 4.37344398340249,
      "grad_norm": 1.5351641178131104,
      "learning_rate": 9.507537688442212e-05,
      "loss": 0.2032,
      "step": 527
    },
    {
      "epoch": 4.381742738589212,
      "grad_norm": 1.1589634418487549,
      "learning_rate": 9.487437185929649e-05,
      "loss": 0.1813,
      "step": 528
    },
    {
      "epoch": 4.390041493775934,
      "grad_norm": 1.216048002243042,
      "learning_rate": 9.467336683417086e-05,
      "loss": 0.1383,
      "step": 529
    },
    {
      "epoch": 4.398340248962656,
      "grad_norm": 1.8163096904754639,
      "learning_rate": 9.447236180904523e-05,
      "loss": 0.189,
      "step": 530
    },
    {
      "epoch": 4.406639004149378,
      "grad_norm": 1.2811946868896484,
      "learning_rate": 9.427135678391961e-05,
      "loss": 0.1543,
      "step": 531
    },
    {
      "epoch": 4.4149377593361,
      "grad_norm": 1.1907305717468262,
      "learning_rate": 9.407035175879397e-05,
      "loss": 0.1787,
      "step": 532
    },
    {
      "epoch": 4.423236514522822,
      "grad_norm": 1.373051643371582,
      "learning_rate": 9.386934673366835e-05,
      "loss": 0.1845,
      "step": 533
    },
    {
      "epoch": 4.431535269709544,
      "grad_norm": 1.6668635606765747,
      "learning_rate": 9.366834170854271e-05,
      "loss": 0.2134,
      "step": 534
    },
    {
      "epoch": 4.439834024896266,
      "grad_norm": 1.358568787574768,
      "learning_rate": 9.34673366834171e-05,
      "loss": 0.172,
      "step": 535
    },
    {
      "epoch": 4.448132780082988,
      "grad_norm": 1.3424373865127563,
      "learning_rate": 9.326633165829146e-05,
      "loss": 0.1959,
      "step": 536
    },
    {
      "epoch": 4.45643153526971,
      "grad_norm": 1.332130789756775,
      "learning_rate": 9.306532663316585e-05,
      "loss": 0.1819,
      "step": 537
    },
    {
      "epoch": 4.464730290456432,
      "grad_norm": 1.4648545980453491,
      "learning_rate": 9.28643216080402e-05,
      "loss": 0.1781,
      "step": 538
    },
    {
      "epoch": 4.473029045643154,
      "grad_norm": 1.4294289350509644,
      "learning_rate": 9.266331658291458e-05,
      "loss": 0.1728,
      "step": 539
    },
    {
      "epoch": 4.481327800829876,
      "grad_norm": 1.714474081993103,
      "learning_rate": 9.246231155778895e-05,
      "loss": 0.2426,
      "step": 540
    },
    {
      "epoch": 4.4896265560165975,
      "grad_norm": 1.4020370244979858,
      "learning_rate": 9.226130653266331e-05,
      "loss": 0.1916,
      "step": 541
    },
    {
      "epoch": 4.4979253112033195,
      "grad_norm": 1.3006837368011475,
      "learning_rate": 9.20603015075377e-05,
      "loss": 0.2088,
      "step": 542
    },
    {
      "epoch": 4.5062240663900415,
      "grad_norm": 1.4336950778961182,
      "learning_rate": 9.185929648241206e-05,
      "loss": 0.1862,
      "step": 543
    },
    {
      "epoch": 4.514522821576763,
      "grad_norm": 1.3236011266708374,
      "learning_rate": 9.165829145728644e-05,
      "loss": 0.1838,
      "step": 544
    },
    {
      "epoch": 4.522821576763485,
      "grad_norm": 1.8776798248291016,
      "learning_rate": 9.14572864321608e-05,
      "loss": 0.2082,
      "step": 545
    },
    {
      "epoch": 4.531120331950207,
      "grad_norm": 1.5640034675598145,
      "learning_rate": 9.125628140703519e-05,
      "loss": 0.1767,
      "step": 546
    },
    {
      "epoch": 4.539419087136929,
      "grad_norm": 1.3732753992080688,
      "learning_rate": 9.105527638190955e-05,
      "loss": 0.1953,
      "step": 547
    },
    {
      "epoch": 4.547717842323651,
      "grad_norm": 1.4692909717559814,
      "learning_rate": 9.085427135678392e-05,
      "loss": 0.1669,
      "step": 548
    },
    {
      "epoch": 4.556016597510373,
      "grad_norm": 1.677757740020752,
      "learning_rate": 9.06532663316583e-05,
      "loss": 0.181,
      "step": 549
    },
    {
      "epoch": 4.564315352697095,
      "grad_norm": 1.3355292081832886,
      "learning_rate": 9.045226130653267e-05,
      "loss": 0.1654,
      "step": 550
    },
    {
      "epoch": 4.572614107883817,
      "grad_norm": 1.1633497476577759,
      "learning_rate": 9.025125628140704e-05,
      "loss": 0.173,
      "step": 551
    },
    {
      "epoch": 4.580912863070539,
      "grad_norm": 1.5782132148742676,
      "learning_rate": 9.005025125628141e-05,
      "loss": 0.2045,
      "step": 552
    },
    {
      "epoch": 4.589211618257261,
      "grad_norm": 1.1764979362487793,
      "learning_rate": 8.984924623115579e-05,
      "loss": 0.1381,
      "step": 553
    },
    {
      "epoch": 4.597510373443983,
      "grad_norm": 1.9153672456741333,
      "learning_rate": 8.964824120603016e-05,
      "loss": 0.227,
      "step": 554
    },
    {
      "epoch": 4.605809128630705,
      "grad_norm": 1.568001389503479,
      "learning_rate": 8.944723618090453e-05,
      "loss": 0.2052,
      "step": 555
    },
    {
      "epoch": 4.614107883817427,
      "grad_norm": 1.385179877281189,
      "learning_rate": 8.92462311557789e-05,
      "loss": 0.1674,
      "step": 556
    },
    {
      "epoch": 4.622406639004149,
      "grad_norm": 1.6241682767868042,
      "learning_rate": 8.904522613065326e-05,
      "loss": 0.1676,
      "step": 557
    },
    {
      "epoch": 4.630705394190871,
      "grad_norm": 1.868019938468933,
      "learning_rate": 8.884422110552765e-05,
      "loss": 0.2227,
      "step": 558
    },
    {
      "epoch": 4.639004149377593,
      "grad_norm": 1.604238510131836,
      "learning_rate": 8.864321608040201e-05,
      "loss": 0.186,
      "step": 559
    },
    {
      "epoch": 4.647302904564316,
      "grad_norm": 1.3426119089126587,
      "learning_rate": 8.84422110552764e-05,
      "loss": 0.152,
      "step": 560
    },
    {
      "epoch": 4.655601659751038,
      "grad_norm": 1.7998607158660889,
      "learning_rate": 8.824120603015076e-05,
      "loss": 0.2121,
      "step": 561
    },
    {
      "epoch": 4.66390041493776,
      "grad_norm": 1.4764772653579712,
      "learning_rate": 8.804020100502513e-05,
      "loss": 0.1876,
      "step": 562
    },
    {
      "epoch": 4.672199170124482,
      "grad_norm": 1.3907111883163452,
      "learning_rate": 8.78391959798995e-05,
      "loss": 0.1682,
      "step": 563
    },
    {
      "epoch": 4.680497925311204,
      "grad_norm": 1.3407179117202759,
      "learning_rate": 8.763819095477387e-05,
      "loss": 0.1831,
      "step": 564
    },
    {
      "epoch": 4.6887966804979255,
      "grad_norm": 2.0628151893615723,
      "learning_rate": 8.743718592964825e-05,
      "loss": 0.2334,
      "step": 565
    },
    {
      "epoch": 4.6970954356846475,
      "grad_norm": 1.3039873838424683,
      "learning_rate": 8.723618090452261e-05,
      "loss": 0.2002,
      "step": 566
    },
    {
      "epoch": 4.7053941908713695,
      "grad_norm": 1.4252634048461914,
      "learning_rate": 8.7035175879397e-05,
      "loss": 0.1648,
      "step": 567
    },
    {
      "epoch": 4.713692946058091,
      "grad_norm": 1.7381393909454346,
      "learning_rate": 8.683417085427135e-05,
      "loss": 0.2524,
      "step": 568
    },
    {
      "epoch": 4.721991701244813,
      "grad_norm": 1.1458721160888672,
      "learning_rate": 8.663316582914574e-05,
      "loss": 0.1767,
      "step": 569
    },
    {
      "epoch": 4.730290456431535,
      "grad_norm": 1.7173982858657837,
      "learning_rate": 8.64321608040201e-05,
      "loss": 0.2152,
      "step": 570
    },
    {
      "epoch": 4.738589211618257,
      "grad_norm": 1.7253450155258179,
      "learning_rate": 8.623115577889449e-05,
      "loss": 0.2111,
      "step": 571
    },
    {
      "epoch": 4.746887966804979,
      "grad_norm": 1.4491336345672607,
      "learning_rate": 8.603015075376884e-05,
      "loss": 0.181,
      "step": 572
    },
    {
      "epoch": 4.755186721991701,
      "grad_norm": 1.566475749015808,
      "learning_rate": 8.582914572864322e-05,
      "loss": 0.2099,
      "step": 573
    },
    {
      "epoch": 4.763485477178423,
      "grad_norm": 1.4839808940887451,
      "learning_rate": 8.562814070351759e-05,
      "loss": 0.1743,
      "step": 574
    },
    {
      "epoch": 4.771784232365145,
      "grad_norm": 1.4781770706176758,
      "learning_rate": 8.542713567839196e-05,
      "loss": 0.199,
      "step": 575
    },
    {
      "epoch": 4.780082987551867,
      "grad_norm": 1.2090426683425903,
      "learning_rate": 8.522613065326634e-05,
      "loss": 0.1784,
      "step": 576
    },
    {
      "epoch": 4.788381742738589,
      "grad_norm": 1.5723832845687866,
      "learning_rate": 8.502512562814071e-05,
      "loss": 0.2249,
      "step": 577
    },
    {
      "epoch": 4.796680497925311,
      "grad_norm": 1.4134279489517212,
      "learning_rate": 8.482412060301508e-05,
      "loss": 0.2419,
      "step": 578
    },
    {
      "epoch": 4.804979253112033,
      "grad_norm": 1.2515228986740112,
      "learning_rate": 8.462311557788946e-05,
      "loss": 0.1582,
      "step": 579
    },
    {
      "epoch": 4.813278008298755,
      "grad_norm": 1.3754252195358276,
      "learning_rate": 8.442211055276383e-05,
      "loss": 0.1749,
      "step": 580
    },
    {
      "epoch": 4.821576763485477,
      "grad_norm": 1.3990615606307983,
      "learning_rate": 8.42211055276382e-05,
      "loss": 0.1756,
      "step": 581
    },
    {
      "epoch": 4.829875518672199,
      "grad_norm": 1.7137327194213867,
      "learning_rate": 8.402010050251256e-05,
      "loss": 0.2296,
      "step": 582
    },
    {
      "epoch": 4.838174273858921,
      "grad_norm": 1.242525577545166,
      "learning_rate": 8.381909547738695e-05,
      "loss": 0.1901,
      "step": 583
    },
    {
      "epoch": 4.846473029045643,
      "grad_norm": 1.5068769454956055,
      "learning_rate": 8.36180904522613e-05,
      "loss": 0.1519,
      "step": 584
    },
    {
      "epoch": 4.854771784232365,
      "grad_norm": 1.4815396070480347,
      "learning_rate": 8.341708542713568e-05,
      "loss": 0.1824,
      "step": 585
    },
    {
      "epoch": 4.863070539419088,
      "grad_norm": 1.523232102394104,
      "learning_rate": 8.321608040201005e-05,
      "loss": 0.1524,
      "step": 586
    },
    {
      "epoch": 4.87136929460581,
      "grad_norm": 1.2720040082931519,
      "learning_rate": 8.301507537688443e-05,
      "loss": 0.1717,
      "step": 587
    },
    {
      "epoch": 4.8796680497925315,
      "grad_norm": 1.3196454048156738,
      "learning_rate": 8.28140703517588e-05,
      "loss": 0.176,
      "step": 588
    },
    {
      "epoch": 4.8879668049792535,
      "grad_norm": 1.5233769416809082,
      "learning_rate": 8.261306532663317e-05,
      "loss": 0.1905,
      "step": 589
    },
    {
      "epoch": 4.8962655601659755,
      "grad_norm": 1.2973872423171997,
      "learning_rate": 8.241206030150754e-05,
      "loss": 0.1791,
      "step": 590
    },
    {
      "epoch": 4.904564315352697,
      "grad_norm": 1.3678194284439087,
      "learning_rate": 8.22110552763819e-05,
      "loss": 0.1849,
      "step": 591
    },
    {
      "epoch": 4.912863070539419,
      "grad_norm": 1.371701717376709,
      "learning_rate": 8.201005025125629e-05,
      "loss": 0.1725,
      "step": 592
    },
    {
      "epoch": 4.921161825726141,
      "grad_norm": 1.5632320642471313,
      "learning_rate": 8.180904522613065e-05,
      "loss": 0.1761,
      "step": 593
    },
    {
      "epoch": 4.929460580912863,
      "grad_norm": 1.2242908477783203,
      "learning_rate": 8.160804020100504e-05,
      "loss": 0.148,
      "step": 594
    },
    {
      "epoch": 4.937759336099585,
      "grad_norm": 1.4289993047714233,
      "learning_rate": 8.14070351758794e-05,
      "loss": 0.1522,
      "step": 595
    },
    {
      "epoch": 4.946058091286307,
      "grad_norm": 1.4088106155395508,
      "learning_rate": 8.120603015075378e-05,
      "loss": 0.1614,
      "step": 596
    },
    {
      "epoch": 4.954356846473029,
      "grad_norm": 1.499361276626587,
      "learning_rate": 8.100502512562814e-05,
      "loss": 0.1973,
      "step": 597
    },
    {
      "epoch": 4.962655601659751,
      "grad_norm": 1.662398338317871,
      "learning_rate": 8.080402010050251e-05,
      "loss": 0.2057,
      "step": 598
    },
    {
      "epoch": 4.970954356846473,
      "grad_norm": 1.5690703392028809,
      "learning_rate": 8.060301507537689e-05,
      "loss": 0.1613,
      "step": 599
    },
    {
      "epoch": 4.979253112033195,
      "grad_norm": 1.72515070438385,
      "learning_rate": 8.040201005025126e-05,
      "loss": 0.2144,
      "step": 600
    },
    {
      "epoch": 4.987551867219917,
      "grad_norm": 1.6854521036148071,
      "learning_rate": 8.020100502512563e-05,
      "loss": 0.1842,
      "step": 601
    },
    {
      "epoch": 4.995850622406639,
      "grad_norm": 1.367065668106079,
      "learning_rate": 8e-05,
      "loss": 0.1754,
      "step": 602
    },
    {
      "epoch": 5.004149377593361,
      "grad_norm": 1.2799018621444702,
      "learning_rate": 7.979899497487438e-05,
      "loss": 0.1543,
      "step": 603
    },
    {
      "epoch": 5.012448132780083,
      "grad_norm": 0.7513083815574646,
      "learning_rate": 7.959798994974875e-05,
      "loss": 0.0999,
      "step": 604
    },
    {
      "epoch": 5.020746887966805,
      "grad_norm": 1.117409348487854,
      "learning_rate": 7.939698492462313e-05,
      "loss": 0.1327,
      "step": 605
    },
    {
      "epoch": 5.029045643153527,
      "grad_norm": 0.790401816368103,
      "learning_rate": 7.91959798994975e-05,
      "loss": 0.0996,
      "step": 606
    },
    {
      "epoch": 5.037344398340249,
      "grad_norm": 0.708958625793457,
      "learning_rate": 7.899497487437186e-05,
      "loss": 0.1037,
      "step": 607
    },
    {
      "epoch": 5.045643153526971,
      "grad_norm": 0.8822044730186462,
      "learning_rate": 7.879396984924623e-05,
      "loss": 0.1255,
      "step": 608
    },
    {
      "epoch": 5.053941908713693,
      "grad_norm": 0.8684053421020508,
      "learning_rate": 7.85929648241206e-05,
      "loss": 0.1211,
      "step": 609
    },
    {
      "epoch": 5.062240663900415,
      "grad_norm": 1.024769902229309,
      "learning_rate": 7.839195979899498e-05,
      "loss": 0.1433,
      "step": 610
    },
    {
      "epoch": 5.070539419087137,
      "grad_norm": 0.9872031211853027,
      "learning_rate": 7.819095477386935e-05,
      "loss": 0.1039,
      "step": 611
    },
    {
      "epoch": 5.078838174273859,
      "grad_norm": 0.8423542380332947,
      "learning_rate": 7.798994974874372e-05,
      "loss": 0.09,
      "step": 612
    },
    {
      "epoch": 5.087136929460581,
      "grad_norm": 1.0097553730010986,
      "learning_rate": 7.77889447236181e-05,
      "loss": 0.1085,
      "step": 613
    },
    {
      "epoch": 5.095435684647303,
      "grad_norm": 1.0086945295333862,
      "learning_rate": 7.758793969849247e-05,
      "loss": 0.1025,
      "step": 614
    },
    {
      "epoch": 5.1037344398340245,
      "grad_norm": 1.3757327795028687,
      "learning_rate": 7.738693467336684e-05,
      "loss": 0.1133,
      "step": 615
    },
    {
      "epoch": 5.1120331950207465,
      "grad_norm": 0.9782530665397644,
      "learning_rate": 7.71859296482412e-05,
      "loss": 0.1154,
      "step": 616
    },
    {
      "epoch": 5.1203319502074685,
      "grad_norm": 0.9740864634513855,
      "learning_rate": 7.698492462311559e-05,
      "loss": 0.1164,
      "step": 617
    },
    {
      "epoch": 5.12863070539419,
      "grad_norm": 0.973979651927948,
      "learning_rate": 7.678391959798995e-05,
      "loss": 0.1083,
      "step": 618
    },
    {
      "epoch": 5.136929460580913,
      "grad_norm": 1.1792808771133423,
      "learning_rate": 7.658291457286433e-05,
      "loss": 0.1219,
      "step": 619
    },
    {
      "epoch": 5.145228215767635,
      "grad_norm": 1.2527796030044556,
      "learning_rate": 7.638190954773869e-05,
      "loss": 0.1281,
      "step": 620
    },
    {
      "epoch": 5.153526970954357,
      "grad_norm": 1.4495407342910767,
      "learning_rate": 7.618090452261307e-05,
      "loss": 0.1258,
      "step": 621
    },
    {
      "epoch": 5.161825726141079,
      "grad_norm": 0.9848902225494385,
      "learning_rate": 7.597989949748744e-05,
      "loss": 0.1065,
      "step": 622
    },
    {
      "epoch": 5.170124481327801,
      "grad_norm": 1.1695706844329834,
      "learning_rate": 7.577889447236181e-05,
      "loss": 0.1103,
      "step": 623
    },
    {
      "epoch": 5.178423236514523,
      "grad_norm": 1.1220661401748657,
      "learning_rate": 7.557788944723618e-05,
      "loss": 0.111,
      "step": 624
    },
    {
      "epoch": 5.186721991701245,
      "grad_norm": 1.0061224699020386,
      "learning_rate": 7.537688442211056e-05,
      "loss": 0.1235,
      "step": 625
    },
    {
      "epoch": 5.195020746887967,
      "grad_norm": 1.5249722003936768,
      "learning_rate": 7.517587939698493e-05,
      "loss": 0.1356,
      "step": 626
    },
    {
      "epoch": 5.203319502074689,
      "grad_norm": 1.1889243125915527,
      "learning_rate": 7.49748743718593e-05,
      "loss": 0.1034,
      "step": 627
    },
    {
      "epoch": 5.211618257261411,
      "grad_norm": 1.0160413980484009,
      "learning_rate": 7.477386934673368e-05,
      "loss": 0.1088,
      "step": 628
    },
    {
      "epoch": 5.219917012448133,
      "grad_norm": 0.9814373254776001,
      "learning_rate": 7.457286432160805e-05,
      "loss": 0.1095,
      "step": 629
    },
    {
      "epoch": 5.228215767634855,
      "grad_norm": 1.2478083372116089,
      "learning_rate": 7.437185929648241e-05,
      "loss": 0.1583,
      "step": 630
    },
    {
      "epoch": 5.236514522821577,
      "grad_norm": 1.1025968790054321,
      "learning_rate": 7.417085427135678e-05,
      "loss": 0.1205,
      "step": 631
    },
    {
      "epoch": 5.244813278008299,
      "grad_norm": 1.1670022010803223,
      "learning_rate": 7.396984924623115e-05,
      "loss": 0.1056,
      "step": 632
    },
    {
      "epoch": 5.253112033195021,
      "grad_norm": 0.8068422675132751,
      "learning_rate": 7.376884422110553e-05,
      "loss": 0.099,
      "step": 633
    },
    {
      "epoch": 5.261410788381743,
      "grad_norm": 1.1762293577194214,
      "learning_rate": 7.35678391959799e-05,
      "loss": 0.1185,
      "step": 634
    },
    {
      "epoch": 5.269709543568465,
      "grad_norm": 1.3691885471343994,
      "learning_rate": 7.336683417085427e-05,
      "loss": 0.1328,
      "step": 635
    },
    {
      "epoch": 5.278008298755187,
      "grad_norm": 0.9013711810112,
      "learning_rate": 7.316582914572865e-05,
      "loss": 0.103,
      "step": 636
    },
    {
      "epoch": 5.286307053941909,
      "grad_norm": 0.866764485836029,
      "learning_rate": 7.296482412060302e-05,
      "loss": 0.0979,
      "step": 637
    },
    {
      "epoch": 5.2946058091286305,
      "grad_norm": 1.2282432317733765,
      "learning_rate": 7.276381909547739e-05,
      "loss": 0.1324,
      "step": 638
    },
    {
      "epoch": 5.3029045643153525,
      "grad_norm": 0.8546429872512817,
      "learning_rate": 7.256281407035177e-05,
      "loss": 0.1213,
      "step": 639
    },
    {
      "epoch": 5.3112033195020745,
      "grad_norm": 1.060254454612732,
      "learning_rate": 7.236180904522614e-05,
      "loss": 0.1286,
      "step": 640
    },
    {
      "epoch": 5.319502074688796,
      "grad_norm": 1.058289885520935,
      "learning_rate": 7.21608040201005e-05,
      "loss": 0.1295,
      "step": 641
    },
    {
      "epoch": 5.327800829875518,
      "grad_norm": 1.3547574281692505,
      "learning_rate": 7.195979899497488e-05,
      "loss": 0.1215,
      "step": 642
    },
    {
      "epoch": 5.33609958506224,
      "grad_norm": 1.1492220163345337,
      "learning_rate": 7.175879396984924e-05,
      "loss": 0.1321,
      "step": 643
    },
    {
      "epoch": 5.344398340248962,
      "grad_norm": 1.0006784200668335,
      "learning_rate": 7.155778894472363e-05,
      "loss": 0.1044,
      "step": 644
    },
    {
      "epoch": 5.352697095435684,
      "grad_norm": 1.1081030368804932,
      "learning_rate": 7.135678391959799e-05,
      "loss": 0.1217,
      "step": 645
    },
    {
      "epoch": 5.360995850622406,
      "grad_norm": 1.164111614227295,
      "learning_rate": 7.115577889447236e-05,
      "loss": 0.142,
      "step": 646
    },
    {
      "epoch": 5.369294605809129,
      "grad_norm": 1.1877259016036987,
      "learning_rate": 7.095477386934674e-05,
      "loss": 0.1042,
      "step": 647
    },
    {
      "epoch": 5.377593360995851,
      "grad_norm": 1.0509371757507324,
      "learning_rate": 7.075376884422111e-05,
      "loss": 0.1426,
      "step": 648
    },
    {
      "epoch": 5.385892116182573,
      "grad_norm": 0.9046774506568909,
      "learning_rate": 7.055276381909548e-05,
      "loss": 0.107,
      "step": 649
    },
    {
      "epoch": 5.394190871369295,
      "grad_norm": 1.096817135810852,
      "learning_rate": 7.035175879396985e-05,
      "loss": 0.1273,
      "step": 650
    },
    {
      "epoch": 5.402489626556017,
      "grad_norm": 1.0301069021224976,
      "learning_rate": 7.015075376884423e-05,
      "loss": 0.1014,
      "step": 651
    },
    {
      "epoch": 5.410788381742739,
      "grad_norm": 1.1896450519561768,
      "learning_rate": 6.99497487437186e-05,
      "loss": 0.1299,
      "step": 652
    },
    {
      "epoch": 5.419087136929461,
      "grad_norm": 1.2188961505889893,
      "learning_rate": 6.974874371859297e-05,
      "loss": 0.136,
      "step": 653
    },
    {
      "epoch": 5.427385892116183,
      "grad_norm": 1.0867514610290527,
      "learning_rate": 6.954773869346733e-05,
      "loss": 0.1365,
      "step": 654
    },
    {
      "epoch": 5.435684647302905,
      "grad_norm": 1.2425719499588013,
      "learning_rate": 6.93467336683417e-05,
      "loss": 0.1077,
      "step": 655
    },
    {
      "epoch": 5.443983402489627,
      "grad_norm": 0.8042731881141663,
      "learning_rate": 6.914572864321608e-05,
      "loss": 0.1037,
      "step": 656
    },
    {
      "epoch": 5.452282157676349,
      "grad_norm": 0.9282207489013672,
      "learning_rate": 6.894472361809045e-05,
      "loss": 0.0971,
      "step": 657
    },
    {
      "epoch": 5.460580912863071,
      "grad_norm": 1.2599701881408691,
      "learning_rate": 6.874371859296482e-05,
      "loss": 0.1108,
      "step": 658
    },
    {
      "epoch": 5.468879668049793,
      "grad_norm": 1.0323599576950073,
      "learning_rate": 6.85427135678392e-05,
      "loss": 0.1258,
      "step": 659
    },
    {
      "epoch": 5.477178423236515,
      "grad_norm": 1.1885217428207397,
      "learning_rate": 6.834170854271357e-05,
      "loss": 0.1341,
      "step": 660
    },
    {
      "epoch": 5.485477178423237,
      "grad_norm": 1.143471598625183,
      "learning_rate": 6.814070351758794e-05,
      "loss": 0.146,
      "step": 661
    },
    {
      "epoch": 5.4937759336099585,
      "grad_norm": 1.2591724395751953,
      "learning_rate": 6.793969849246232e-05,
      "loss": 0.1391,
      "step": 662
    },
    {
      "epoch": 5.5020746887966805,
      "grad_norm": 1.178755521774292,
      "learning_rate": 6.773869346733669e-05,
      "loss": 0.1173,
      "step": 663
    },
    {
      "epoch": 5.5103734439834025,
      "grad_norm": 1.0432261228561401,
      "learning_rate": 6.753768844221105e-05,
      "loss": 0.1141,
      "step": 664
    },
    {
      "epoch": 5.518672199170124,
      "grad_norm": 1.0488296747207642,
      "learning_rate": 6.733668341708544e-05,
      "loss": 0.126,
      "step": 665
    },
    {
      "epoch": 5.526970954356846,
      "grad_norm": 0.7871274948120117,
      "learning_rate": 6.71356783919598e-05,
      "loss": 0.1023,
      "step": 666
    },
    {
      "epoch": 5.535269709543568,
      "grad_norm": 0.8441354036331177,
      "learning_rate": 6.693467336683418e-05,
      "loss": 0.0987,
      "step": 667
    },
    {
      "epoch": 5.54356846473029,
      "grad_norm": 0.78030925989151,
      "learning_rate": 6.673366834170854e-05,
      "loss": 0.093,
      "step": 668
    },
    {
      "epoch": 5.551867219917012,
      "grad_norm": 1.257749319076538,
      "learning_rate": 6.653266331658293e-05,
      "loss": 0.1345,
      "step": 669
    },
    {
      "epoch": 5.560165975103734,
      "grad_norm": 1.033074975013733,
      "learning_rate": 6.633165829145729e-05,
      "loss": 0.1229,
      "step": 670
    },
    {
      "epoch": 5.568464730290456,
      "grad_norm": 1.3054912090301514,
      "learning_rate": 6.613065326633166e-05,
      "loss": 0.1698,
      "step": 671
    },
    {
      "epoch": 5.576763485477178,
      "grad_norm": 1.182917594909668,
      "learning_rate": 6.592964824120603e-05,
      "loss": 0.1281,
      "step": 672
    },
    {
      "epoch": 5.5850622406639,
      "grad_norm": 1.1956312656402588,
      "learning_rate": 6.57286432160804e-05,
      "loss": 0.1185,
      "step": 673
    },
    {
      "epoch": 5.593360995850622,
      "grad_norm": 1.055136799812317,
      "learning_rate": 6.552763819095478e-05,
      "loss": 0.1213,
      "step": 674
    },
    {
      "epoch": 5.601659751037344,
      "grad_norm": 0.9252353310585022,
      "learning_rate": 6.532663316582915e-05,
      "loss": 0.1059,
      "step": 675
    },
    {
      "epoch": 5.609958506224066,
      "grad_norm": 1.0760375261306763,
      "learning_rate": 6.512562814070352e-05,
      "loss": 0.1173,
      "step": 676
    },
    {
      "epoch": 5.618257261410788,
      "grad_norm": 1.196535587310791,
      "learning_rate": 6.492462311557788e-05,
      "loss": 0.1352,
      "step": 677
    },
    {
      "epoch": 5.62655601659751,
      "grad_norm": 0.8881775736808777,
      "learning_rate": 6.472361809045227e-05,
      "loss": 0.1284,
      "step": 678
    },
    {
      "epoch": 5.634854771784232,
      "grad_norm": 0.9986284971237183,
      "learning_rate": 6.452261306532663e-05,
      "loss": 0.1284,
      "step": 679
    },
    {
      "epoch": 5.643153526970955,
      "grad_norm": 1.205542802810669,
      "learning_rate": 6.4321608040201e-05,
      "loss": 0.1238,
      "step": 680
    },
    {
      "epoch": 5.651452282157677,
      "grad_norm": 1.0844048261642456,
      "learning_rate": 6.412060301507538e-05,
      "loss": 0.1953,
      "step": 681
    },
    {
      "epoch": 5.659751037344399,
      "grad_norm": 0.9609887003898621,
      "learning_rate": 6.391959798994975e-05,
      "loss": 0.1046,
      "step": 682
    },
    {
      "epoch": 5.668049792531121,
      "grad_norm": 0.8919401168823242,
      "learning_rate": 6.371859296482412e-05,
      "loss": 0.1094,
      "step": 683
    },
    {
      "epoch": 5.676348547717843,
      "grad_norm": 1.001847743988037,
      "learning_rate": 6.35175879396985e-05,
      "loss": 0.1047,
      "step": 684
    },
    {
      "epoch": 5.6846473029045645,
      "grad_norm": 1.1736377477645874,
      "learning_rate": 6.331658291457287e-05,
      "loss": 0.1104,
      "step": 685
    },
    {
      "epoch": 5.6929460580912865,
      "grad_norm": 1.1956347227096558,
      "learning_rate": 6.311557788944724e-05,
      "loss": 0.1266,
      "step": 686
    },
    {
      "epoch": 5.7012448132780085,
      "grad_norm": 1.5701714754104614,
      "learning_rate": 6.291457286432161e-05,
      "loss": 0.1338,
      "step": 687
    },
    {
      "epoch": 5.70954356846473,
      "grad_norm": 1.1887215375900269,
      "learning_rate": 6.271356783919599e-05,
      "loss": 0.1532,
      "step": 688
    },
    {
      "epoch": 5.717842323651452,
      "grad_norm": 0.9288482666015625,
      "learning_rate": 6.251256281407035e-05,
      "loss": 0.1183,
      "step": 689
    },
    {
      "epoch": 5.726141078838174,
      "grad_norm": 1.0066978931427002,
      "learning_rate": 6.231155778894473e-05,
      "loss": 0.0911,
      "step": 690
    },
    {
      "epoch": 5.734439834024896,
      "grad_norm": 1.1956787109375,
      "learning_rate": 6.211055276381909e-05,
      "loss": 0.1331,
      "step": 691
    },
    {
      "epoch": 5.742738589211618,
      "grad_norm": 1.1735706329345703,
      "learning_rate": 6.190954773869348e-05,
      "loss": 0.1301,
      "step": 692
    },
    {
      "epoch": 5.75103734439834,
      "grad_norm": 1.0108484029769897,
      "learning_rate": 6.170854271356784e-05,
      "loss": 0.1036,
      "step": 693
    },
    {
      "epoch": 5.759336099585062,
      "grad_norm": 1.4615143537521362,
      "learning_rate": 6.150753768844222e-05,
      "loss": 0.1386,
      "step": 694
    },
    {
      "epoch": 5.767634854771784,
      "grad_norm": 1.2296226024627686,
      "learning_rate": 6.130653266331658e-05,
      "loss": 0.1024,
      "step": 695
    },
    {
      "epoch": 5.775933609958506,
      "grad_norm": 1.140562653541565,
      "learning_rate": 6.110552763819096e-05,
      "loss": 0.1353,
      "step": 696
    },
    {
      "epoch": 5.784232365145228,
      "grad_norm": 1.1192476749420166,
      "learning_rate": 6.090452261306533e-05,
      "loss": 0.1354,
      "step": 697
    },
    {
      "epoch": 5.79253112033195,
      "grad_norm": 1.4206680059432983,
      "learning_rate": 6.070351758793971e-05,
      "loss": 0.1426,
      "step": 698
    },
    {
      "epoch": 5.800829875518672,
      "grad_norm": 0.9125518202781677,
      "learning_rate": 6.0502512562814076e-05,
      "loss": 0.1226,
      "step": 699
    },
    {
      "epoch": 5.809128630705394,
      "grad_norm": 1.0456956624984741,
      "learning_rate": 6.030150753768844e-05,
      "loss": 0.1272,
      "step": 700
    },
    {
      "epoch": 5.817427385892116,
      "grad_norm": 1.031342625617981,
      "learning_rate": 6.0100502512562815e-05,
      "loss": 0.1072,
      "step": 701
    },
    {
      "epoch": 5.825726141078838,
      "grad_norm": 1.1944793462753296,
      "learning_rate": 5.989949748743718e-05,
      "loss": 0.1318,
      "step": 702
    },
    {
      "epoch": 5.83402489626556,
      "grad_norm": 1.58222234249115,
      "learning_rate": 5.969849246231156e-05,
      "loss": 0.1329,
      "step": 703
    },
    {
      "epoch": 5.842323651452282,
      "grad_norm": 1.1209603548049927,
      "learning_rate": 5.949748743718593e-05,
      "loss": 0.1399,
      "step": 704
    },
    {
      "epoch": 5.850622406639004,
      "grad_norm": 1.17646062374115,
      "learning_rate": 5.929648241206031e-05,
      "loss": 0.1365,
      "step": 705
    },
    {
      "epoch": 5.858921161825726,
      "grad_norm": 0.8717462420463562,
      "learning_rate": 5.909547738693467e-05,
      "loss": 0.1051,
      "step": 706
    },
    {
      "epoch": 5.867219917012449,
      "grad_norm": 1.1398049592971802,
      "learning_rate": 5.889447236180905e-05,
      "loss": 0.1212,
      "step": 707
    },
    {
      "epoch": 5.875518672199171,
      "grad_norm": 0.9171994924545288,
      "learning_rate": 5.869346733668342e-05,
      "loss": 0.1112,
      "step": 708
    },
    {
      "epoch": 5.8838174273858925,
      "grad_norm": 1.025341272354126,
      "learning_rate": 5.849246231155779e-05,
      "loss": 0.1092,
      "step": 709
    },
    {
      "epoch": 5.8921161825726145,
      "grad_norm": 1.1963118314743042,
      "learning_rate": 5.829145728643216e-05,
      "loss": 0.1736,
      "step": 710
    },
    {
      "epoch": 5.9004149377593365,
      "grad_norm": 1.1901286840438843,
      "learning_rate": 5.809045226130654e-05,
      "loss": 0.1304,
      "step": 711
    },
    {
      "epoch": 5.908713692946058,
      "grad_norm": 0.7974649667739868,
      "learning_rate": 5.7889447236180904e-05,
      "loss": 0.1169,
      "step": 712
    },
    {
      "epoch": 5.91701244813278,
      "grad_norm": 1.2379108667373657,
      "learning_rate": 5.7688442211055284e-05,
      "loss": 0.139,
      "step": 713
    },
    {
      "epoch": 5.925311203319502,
      "grad_norm": 1.123214602470398,
      "learning_rate": 5.748743718592965e-05,
      "loss": 0.1321,
      "step": 714
    },
    {
      "epoch": 5.933609958506224,
      "grad_norm": 1.0536726713180542,
      "learning_rate": 5.728643216080403e-05,
      "loss": 0.1526,
      "step": 715
    },
    {
      "epoch": 5.941908713692946,
      "grad_norm": 0.9607227444648743,
      "learning_rate": 5.7085427135678396e-05,
      "loss": 0.1331,
      "step": 716
    },
    {
      "epoch": 5.950207468879668,
      "grad_norm": 0.9029622673988342,
      "learning_rate": 5.688442211055277e-05,
      "loss": 0.1103,
      "step": 717
    },
    {
      "epoch": 5.95850622406639,
      "grad_norm": 1.1401755809783936,
      "learning_rate": 5.6683417085427135e-05,
      "loss": 0.1395,
      "step": 718
    },
    {
      "epoch": 5.966804979253112,
      "grad_norm": 0.9943388104438782,
      "learning_rate": 5.6482412060301515e-05,
      "loss": 0.121,
      "step": 719
    },
    {
      "epoch": 5.975103734439834,
      "grad_norm": 1.0430394411087036,
      "learning_rate": 5.628140703517588e-05,
      "loss": 0.1295,
      "step": 720
    },
    {
      "epoch": 5.983402489626556,
      "grad_norm": 0.8715236186981201,
      "learning_rate": 5.608040201005026e-05,
      "loss": 0.1028,
      "step": 721
    },
    {
      "epoch": 5.991701244813278,
      "grad_norm": 1.191081166267395,
      "learning_rate": 5.587939698492463e-05,
      "loss": 0.1465,
      "step": 722
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.3148242235183716,
      "learning_rate": 5.567839195979899e-05,
      "loss": 0.1342,
      "step": 723
    },
    {
      "epoch": 6.008298755186722,
      "grad_norm": 0.6049588918685913,
      "learning_rate": 5.547738693467337e-05,
      "loss": 0.0957,
      "step": 724
    },
    {
      "epoch": 6.016597510373444,
      "grad_norm": 0.7533386945724487,
      "learning_rate": 5.527638190954774e-05,
      "loss": 0.0844,
      "step": 725
    },
    {
      "epoch": 6.024896265560166,
      "grad_norm": 0.8626192212104797,
      "learning_rate": 5.507537688442211e-05,
      "loss": 0.0814,
      "step": 726
    },
    {
      "epoch": 6.033195020746888,
      "grad_norm": 0.6310971975326538,
      "learning_rate": 5.487437185929648e-05,
      "loss": 0.0754,
      "step": 727
    },
    {
      "epoch": 6.04149377593361,
      "grad_norm": 0.71369469165802,
      "learning_rate": 5.467336683417086e-05,
      "loss": 0.0878,
      "step": 728
    },
    {
      "epoch": 6.049792531120332,
      "grad_norm": 0.8278961777687073,
      "learning_rate": 5.4472361809045224e-05,
      "loss": 0.0788,
      "step": 729
    },
    {
      "epoch": 6.058091286307054,
      "grad_norm": 0.5756589770317078,
      "learning_rate": 5.4271356783919604e-05,
      "loss": 0.0722,
      "step": 730
    },
    {
      "epoch": 6.066390041493776,
      "grad_norm": 0.8407052755355835,
      "learning_rate": 5.407035175879397e-05,
      "loss": 0.0982,
      "step": 731
    },
    {
      "epoch": 6.074688796680498,
      "grad_norm": 1.0137767791748047,
      "learning_rate": 5.386934673366835e-05,
      "loss": 0.1102,
      "step": 732
    },
    {
      "epoch": 6.08298755186722,
      "grad_norm": 0.7381811141967773,
      "learning_rate": 5.3668341708542716e-05,
      "loss": 0.0955,
      "step": 733
    },
    {
      "epoch": 6.091286307053942,
      "grad_norm": 0.7276250720024109,
      "learning_rate": 5.346733668341709e-05,
      "loss": 0.0935,
      "step": 734
    },
    {
      "epoch": 6.0995850622406635,
      "grad_norm": 0.7128432393074036,
      "learning_rate": 5.3266331658291455e-05,
      "loss": 0.0958,
      "step": 735
    },
    {
      "epoch": 6.1078838174273855,
      "grad_norm": 0.7628387212753296,
      "learning_rate": 5.3065326633165835e-05,
      "loss": 0.0868,
      "step": 736
    },
    {
      "epoch": 6.1161825726141075,
      "grad_norm": 0.7778459191322327,
      "learning_rate": 5.28643216080402e-05,
      "loss": 0.0986,
      "step": 737
    },
    {
      "epoch": 6.124481327800829,
      "grad_norm": 0.5972809791564941,
      "learning_rate": 5.266331658291458e-05,
      "loss": 0.0787,
      "step": 738
    },
    {
      "epoch": 6.132780082987552,
      "grad_norm": 0.8565507531166077,
      "learning_rate": 5.246231155778895e-05,
      "loss": 0.0937,
      "step": 739
    },
    {
      "epoch": 6.141078838174274,
      "grad_norm": 0.43703320622444153,
      "learning_rate": 5.226130653266332e-05,
      "loss": 0.0664,
      "step": 740
    },
    {
      "epoch": 6.149377593360996,
      "grad_norm": 1.0536231994628906,
      "learning_rate": 5.206030150753769e-05,
      "loss": 0.102,
      "step": 741
    },
    {
      "epoch": 6.157676348547718,
      "grad_norm": 0.7100486159324646,
      "learning_rate": 5.1859296482412066e-05,
      "loss": 0.0906,
      "step": 742
    },
    {
      "epoch": 6.16597510373444,
      "grad_norm": 0.5723758935928345,
      "learning_rate": 5.165829145728643e-05,
      "loss": 0.0823,
      "step": 743
    },
    {
      "epoch": 6.174273858921162,
      "grad_norm": 0.7066110372543335,
      "learning_rate": 5.145728643216081e-05,
      "loss": 0.0884,
      "step": 744
    },
    {
      "epoch": 6.182572614107884,
      "grad_norm": 0.6579684019088745,
      "learning_rate": 5.125628140703518e-05,
      "loss": 0.0831,
      "step": 745
    },
    {
      "epoch": 6.190871369294606,
      "grad_norm": 0.6562890410423279,
      "learning_rate": 5.1055276381909544e-05,
      "loss": 0.0773,
      "step": 746
    },
    {
      "epoch": 6.199170124481328,
      "grad_norm": 0.8459333777427673,
      "learning_rate": 5.0854271356783924e-05,
      "loss": 0.0996,
      "step": 747
    },
    {
      "epoch": 6.20746887966805,
      "grad_norm": 0.8480926156044006,
      "learning_rate": 5.065326633165829e-05,
      "loss": 0.0941,
      "step": 748
    },
    {
      "epoch": 6.215767634854772,
      "grad_norm": 0.6377203464508057,
      "learning_rate": 5.045226130653266e-05,
      "loss": 0.0965,
      "step": 749
    },
    {
      "epoch": 6.224066390041494,
      "grad_norm": 0.8791307806968689,
      "learning_rate": 5.0251256281407036e-05,
      "loss": 0.1126,
      "step": 750
    },
    {
      "epoch": 6.232365145228216,
      "grad_norm": 0.7411600351333618,
      "learning_rate": 5.005025125628141e-05,
      "loss": 0.0873,
      "step": 751
    },
    {
      "epoch": 6.240663900414938,
      "grad_norm": 1.249772310256958,
      "learning_rate": 4.984924623115578e-05,
      "loss": 0.1179,
      "step": 752
    },
    {
      "epoch": 6.24896265560166,
      "grad_norm": 0.6409496665000916,
      "learning_rate": 4.9648241206030155e-05,
      "loss": 0.0762,
      "step": 753
    },
    {
      "epoch": 6.257261410788382,
      "grad_norm": 0.7666827440261841,
      "learning_rate": 4.944723618090453e-05,
      "loss": 0.0852,
      "step": 754
    },
    {
      "epoch": 6.265560165975104,
      "grad_norm": 0.6819324493408203,
      "learning_rate": 4.92462311557789e-05,
      "loss": 0.0655,
      "step": 755
    },
    {
      "epoch": 6.273858921161826,
      "grad_norm": 0.7853745222091675,
      "learning_rate": 4.9045226130653274e-05,
      "loss": 0.0964,
      "step": 756
    },
    {
      "epoch": 6.282157676348548,
      "grad_norm": 0.8457928895950317,
      "learning_rate": 4.884422110552764e-05,
      "loss": 0.0953,
      "step": 757
    },
    {
      "epoch": 6.29045643153527,
      "grad_norm": 1.0980827808380127,
      "learning_rate": 4.864321608040201e-05,
      "loss": 0.1142,
      "step": 758
    },
    {
      "epoch": 6.2987551867219915,
      "grad_norm": 0.7082809805870056,
      "learning_rate": 4.844221105527638e-05,
      "loss": 0.0701,
      "step": 759
    },
    {
      "epoch": 6.3070539419087135,
      "grad_norm": 0.7840237617492676,
      "learning_rate": 4.824120603015075e-05,
      "loss": 0.0967,
      "step": 760
    },
    {
      "epoch": 6.3153526970954355,
      "grad_norm": 0.7170233726501465,
      "learning_rate": 4.8040201005025125e-05,
      "loss": 0.0788,
      "step": 761
    },
    {
      "epoch": 6.323651452282157,
      "grad_norm": 0.7900985479354858,
      "learning_rate": 4.78391959798995e-05,
      "loss": 0.0963,
      "step": 762
    },
    {
      "epoch": 6.331950207468879,
      "grad_norm": 0.8302997350692749,
      "learning_rate": 4.763819095477387e-05,
      "loss": 0.0937,
      "step": 763
    },
    {
      "epoch": 6.340248962655601,
      "grad_norm": 0.9402018189430237,
      "learning_rate": 4.7437185929648244e-05,
      "loss": 0.0765,
      "step": 764
    },
    {
      "epoch": 6.348547717842323,
      "grad_norm": 0.6850914359092712,
      "learning_rate": 4.723618090452262e-05,
      "loss": 0.0782,
      "step": 765
    },
    {
      "epoch": 6.356846473029045,
      "grad_norm": 0.7774332761764526,
      "learning_rate": 4.703517587939698e-05,
      "loss": 0.0929,
      "step": 766
    },
    {
      "epoch": 6.365145228215767,
      "grad_norm": 1.0124616622924805,
      "learning_rate": 4.6834170854271356e-05,
      "loss": 0.0961,
      "step": 767
    },
    {
      "epoch": 6.37344398340249,
      "grad_norm": 1.3467265367507935,
      "learning_rate": 4.663316582914573e-05,
      "loss": 0.0917,
      "step": 768
    },
    {
      "epoch": 6.381742738589212,
      "grad_norm": 0.8122695684432983,
      "learning_rate": 4.64321608040201e-05,
      "loss": 0.0726,
      "step": 769
    },
    {
      "epoch": 6.390041493775934,
      "grad_norm": 0.7407058477401733,
      "learning_rate": 4.6231155778894475e-05,
      "loss": 0.0981,
      "step": 770
    },
    {
      "epoch": 6.398340248962656,
      "grad_norm": 0.7476844191551208,
      "learning_rate": 4.603015075376885e-05,
      "loss": 0.0859,
      "step": 771
    },
    {
      "epoch": 6.406639004149378,
      "grad_norm": 0.5024213790893555,
      "learning_rate": 4.582914572864322e-05,
      "loss": 0.0695,
      "step": 772
    },
    {
      "epoch": 6.4149377593361,
      "grad_norm": 0.5837110877037048,
      "learning_rate": 4.5628140703517594e-05,
      "loss": 0.0717,
      "step": 773
    },
    {
      "epoch": 6.423236514522822,
      "grad_norm": 0.795830249786377,
      "learning_rate": 4.542713567839196e-05,
      "loss": 0.0799,
      "step": 774
    },
    {
      "epoch": 6.431535269709544,
      "grad_norm": 0.8018327355384827,
      "learning_rate": 4.522613065326633e-05,
      "loss": 0.099,
      "step": 775
    },
    {
      "epoch": 6.439834024896266,
      "grad_norm": 0.8355006575584412,
      "learning_rate": 4.5025125628140706e-05,
      "loss": 0.0883,
      "step": 776
    },
    {
      "epoch": 6.448132780082988,
      "grad_norm": 0.7386964559555054,
      "learning_rate": 4.482412060301508e-05,
      "loss": 0.0845,
      "step": 777
    },
    {
      "epoch": 6.45643153526971,
      "grad_norm": 0.8444579839706421,
      "learning_rate": 4.462311557788945e-05,
      "loss": 0.1172,
      "step": 778
    },
    {
      "epoch": 6.464730290456432,
      "grad_norm": 0.8312764763832092,
      "learning_rate": 4.4422110552763825e-05,
      "loss": 0.0834,
      "step": 779
    },
    {
      "epoch": 6.473029045643154,
      "grad_norm": 0.7715323567390442,
      "learning_rate": 4.42211055276382e-05,
      "loss": 0.0718,
      "step": 780
    },
    {
      "epoch": 6.481327800829876,
      "grad_norm": 0.7915472984313965,
      "learning_rate": 4.4020100502512564e-05,
      "loss": 0.0918,
      "step": 781
    },
    {
      "epoch": 6.4896265560165975,
      "grad_norm": 1.1783053874969482,
      "learning_rate": 4.381909547738694e-05,
      "loss": 0.1198,
      "step": 782
    },
    {
      "epoch": 6.4979253112033195,
      "grad_norm": 0.8284636735916138,
      "learning_rate": 4.3618090452261303e-05,
      "loss": 0.1173,
      "step": 783
    },
    {
      "epoch": 6.5062240663900415,
      "grad_norm": 0.7001990675926208,
      "learning_rate": 4.3417085427135676e-05,
      "loss": 0.0894,
      "step": 784
    },
    {
      "epoch": 6.514522821576763,
      "grad_norm": 1.138282299041748,
      "learning_rate": 4.321608040201005e-05,
      "loss": 0.1014,
      "step": 785
    },
    {
      "epoch": 6.522821576763485,
      "grad_norm": 0.6858628392219543,
      "learning_rate": 4.301507537688442e-05,
      "loss": 0.0906,
      "step": 786
    },
    {
      "epoch": 6.531120331950207,
      "grad_norm": 0.7285899519920349,
      "learning_rate": 4.2814070351758795e-05,
      "loss": 0.075,
      "step": 787
    },
    {
      "epoch": 6.539419087136929,
      "grad_norm": 0.8334895372390747,
      "learning_rate": 4.261306532663317e-05,
      "loss": 0.0886,
      "step": 788
    },
    {
      "epoch": 6.547717842323651,
      "grad_norm": 0.7329304814338684,
      "learning_rate": 4.241206030150754e-05,
      "loss": 0.0973,
      "step": 789
    },
    {
      "epoch": 6.556016597510373,
      "grad_norm": 0.9192265868186951,
      "learning_rate": 4.2211055276381914e-05,
      "loss": 0.0971,
      "step": 790
    },
    {
      "epoch": 6.564315352697095,
      "grad_norm": 0.7339341640472412,
      "learning_rate": 4.201005025125628e-05,
      "loss": 0.0758,
      "step": 791
    },
    {
      "epoch": 6.572614107883817,
      "grad_norm": 0.5334381461143494,
      "learning_rate": 4.180904522613065e-05,
      "loss": 0.0726,
      "step": 792
    },
    {
      "epoch": 6.580912863070539,
      "grad_norm": 0.634914219379425,
      "learning_rate": 4.1608040201005026e-05,
      "loss": 0.0834,
      "step": 793
    },
    {
      "epoch": 6.589211618257261,
      "grad_norm": 1.0288970470428467,
      "learning_rate": 4.14070351758794e-05,
      "loss": 0.1001,
      "step": 794
    },
    {
      "epoch": 6.597510373443983,
      "grad_norm": 0.7575836777687073,
      "learning_rate": 4.120603015075377e-05,
      "loss": 0.1,
      "step": 795
    },
    {
      "epoch": 6.605809128630705,
      "grad_norm": 0.628746509552002,
      "learning_rate": 4.1005025125628145e-05,
      "loss": 0.0746,
      "step": 796
    },
    {
      "epoch": 6.614107883817427,
      "grad_norm": 0.7375231981277466,
      "learning_rate": 4.080402010050252e-05,
      "loss": 0.115,
      "step": 797
    },
    {
      "epoch": 6.622406639004149,
      "grad_norm": 0.8181651830673218,
      "learning_rate": 4.060301507537689e-05,
      "loss": 0.102,
      "step": 798
    },
    {
      "epoch": 6.630705394190871,
      "grad_norm": 0.6446406245231628,
      "learning_rate": 4.040201005025126e-05,
      "loss": 0.0797,
      "step": 799
    },
    {
      "epoch": 6.639004149377593,
      "grad_norm": 0.7916334867477417,
      "learning_rate": 4.020100502512563e-05,
      "loss": 0.0799,
      "step": 800
    },
    {
      "epoch": 6.647302904564316,
      "grad_norm": 0.6265787482261658,
      "learning_rate": 4e-05,
      "loss": 0.0807,
      "step": 801
    },
    {
      "epoch": 6.655601659751038,
      "grad_norm": 0.6313364505767822,
      "learning_rate": 3.9798994974874376e-05,
      "loss": 0.0898,
      "step": 802
    },
    {
      "epoch": 6.66390041493776,
      "grad_norm": 0.7187129855155945,
      "learning_rate": 3.959798994974875e-05,
      "loss": 0.0892,
      "step": 803
    },
    {
      "epoch": 6.672199170124482,
      "grad_norm": 0.7834749221801758,
      "learning_rate": 3.9396984924623115e-05,
      "loss": 0.0977,
      "step": 804
    },
    {
      "epoch": 6.680497925311204,
      "grad_norm": 0.8661743402481079,
      "learning_rate": 3.919597989949749e-05,
      "loss": 0.0984,
      "step": 805
    },
    {
      "epoch": 6.6887966804979255,
      "grad_norm": 0.7072321772575378,
      "learning_rate": 3.899497487437186e-05,
      "loss": 0.0935,
      "step": 806
    },
    {
      "epoch": 6.6970954356846475,
      "grad_norm": 0.8210338950157166,
      "learning_rate": 3.8793969849246234e-05,
      "loss": 0.111,
      "step": 807
    },
    {
      "epoch": 6.7053941908713695,
      "grad_norm": 0.782817542552948,
      "learning_rate": 3.85929648241206e-05,
      "loss": 0.0787,
      "step": 808
    },
    {
      "epoch": 6.713692946058091,
      "grad_norm": 0.8115402460098267,
      "learning_rate": 3.8391959798994973e-05,
      "loss": 0.0922,
      "step": 809
    },
    {
      "epoch": 6.721991701244813,
      "grad_norm": 0.7008811235427856,
      "learning_rate": 3.8190954773869346e-05,
      "loss": 0.0919,
      "step": 810
    },
    {
      "epoch": 6.730290456431535,
      "grad_norm": 0.8950849175453186,
      "learning_rate": 3.798994974874372e-05,
      "loss": 0.0979,
      "step": 811
    },
    {
      "epoch": 6.738589211618257,
      "grad_norm": 0.8214327692985535,
      "learning_rate": 3.778894472361809e-05,
      "loss": 0.079,
      "step": 812
    },
    {
      "epoch": 6.746887966804979,
      "grad_norm": 0.6486192345619202,
      "learning_rate": 3.7587939698492465e-05,
      "loss": 0.0851,
      "step": 813
    },
    {
      "epoch": 6.755186721991701,
      "grad_norm": 1.1533232927322388,
      "learning_rate": 3.738693467336684e-05,
      "loss": 0.1026,
      "step": 814
    },
    {
      "epoch": 6.763485477178423,
      "grad_norm": 0.7687472701072693,
      "learning_rate": 3.7185929648241204e-05,
      "loss": 0.0885,
      "step": 815
    },
    {
      "epoch": 6.771784232365145,
      "grad_norm": 0.970294713973999,
      "learning_rate": 3.698492462311558e-05,
      "loss": 0.0945,
      "step": 816
    },
    {
      "epoch": 6.780082987551867,
      "grad_norm": 0.9467993974685669,
      "learning_rate": 3.678391959798995e-05,
      "loss": 0.0924,
      "step": 817
    },
    {
      "epoch": 6.788381742738589,
      "grad_norm": 0.7263206243515015,
      "learning_rate": 3.658291457286432e-05,
      "loss": 0.0884,
      "step": 818
    },
    {
      "epoch": 6.796680497925311,
      "grad_norm": 0.6175129413604736,
      "learning_rate": 3.6381909547738696e-05,
      "loss": 0.0806,
      "step": 819
    },
    {
      "epoch": 6.804979253112033,
      "grad_norm": 0.6947745680809021,
      "learning_rate": 3.618090452261307e-05,
      "loss": 0.1008,
      "step": 820
    },
    {
      "epoch": 6.813278008298755,
      "grad_norm": 0.723952054977417,
      "learning_rate": 3.597989949748744e-05,
      "loss": 0.0797,
      "step": 821
    },
    {
      "epoch": 6.821576763485477,
      "grad_norm": 0.6979265809059143,
      "learning_rate": 3.5778894472361815e-05,
      "loss": 0.0999,
      "step": 822
    },
    {
      "epoch": 6.829875518672199,
      "grad_norm": 0.6841933727264404,
      "learning_rate": 3.557788944723618e-05,
      "loss": 0.0999,
      "step": 823
    },
    {
      "epoch": 6.838174273858921,
      "grad_norm": 0.7790443897247314,
      "learning_rate": 3.5376884422110554e-05,
      "loss": 0.0903,
      "step": 824
    },
    {
      "epoch": 6.846473029045643,
      "grad_norm": 1.2935776710510254,
      "learning_rate": 3.517587939698493e-05,
      "loss": 0.135,
      "step": 825
    },
    {
      "epoch": 6.854771784232365,
      "grad_norm": 0.6473920941352844,
      "learning_rate": 3.49748743718593e-05,
      "loss": 0.0871,
      "step": 826
    },
    {
      "epoch": 6.863070539419088,
      "grad_norm": 0.7762858271598816,
      "learning_rate": 3.4773869346733667e-05,
      "loss": 0.0943,
      "step": 827
    },
    {
      "epoch": 6.87136929460581,
      "grad_norm": 0.8208593130111694,
      "learning_rate": 3.457286432160804e-05,
      "loss": 0.0835,
      "step": 828
    },
    {
      "epoch": 6.8796680497925315,
      "grad_norm": 0.7901422381401062,
      "learning_rate": 3.437185929648241e-05,
      "loss": 0.1039,
      "step": 829
    },
    {
      "epoch": 6.8879668049792535,
      "grad_norm": 0.5722119808197021,
      "learning_rate": 3.4170854271356785e-05,
      "loss": 0.0962,
      "step": 830
    },
    {
      "epoch": 6.8962655601659755,
      "grad_norm": 0.565933346748352,
      "learning_rate": 3.396984924623116e-05,
      "loss": 0.0748,
      "step": 831
    },
    {
      "epoch": 6.904564315352697,
      "grad_norm": 0.6291952729225159,
      "learning_rate": 3.3768844221105525e-05,
      "loss": 0.0646,
      "step": 832
    },
    {
      "epoch": 6.912863070539419,
      "grad_norm": 0.7148945331573486,
      "learning_rate": 3.35678391959799e-05,
      "loss": 0.1079,
      "step": 833
    },
    {
      "epoch": 6.921161825726141,
      "grad_norm": 0.7136537432670593,
      "learning_rate": 3.336683417085427e-05,
      "loss": 0.0789,
      "step": 834
    },
    {
      "epoch": 6.929460580912863,
      "grad_norm": 0.7700856328010559,
      "learning_rate": 3.3165829145728643e-05,
      "loss": 0.0925,
      "step": 835
    },
    {
      "epoch": 6.937759336099585,
      "grad_norm": 0.7640576362609863,
      "learning_rate": 3.2964824120603016e-05,
      "loss": 0.0855,
      "step": 836
    },
    {
      "epoch": 6.946058091286307,
      "grad_norm": 0.7509792447090149,
      "learning_rate": 3.276381909547739e-05,
      "loss": 0.1012,
      "step": 837
    },
    {
      "epoch": 6.954356846473029,
      "grad_norm": 0.7045333385467529,
      "learning_rate": 3.256281407035176e-05,
      "loss": 0.0885,
      "step": 838
    },
    {
      "epoch": 6.962655601659751,
      "grad_norm": 0.7132675051689148,
      "learning_rate": 3.2361809045226135e-05,
      "loss": 0.0905,
      "step": 839
    },
    {
      "epoch": 6.970954356846473,
      "grad_norm": 0.5881735682487488,
      "learning_rate": 3.21608040201005e-05,
      "loss": 0.0875,
      "step": 840
    },
    {
      "epoch": 6.979253112033195,
      "grad_norm": 0.6941697597503662,
      "learning_rate": 3.1959798994974875e-05,
      "loss": 0.0763,
      "step": 841
    },
    {
      "epoch": 6.987551867219917,
      "grad_norm": 0.5769615769386292,
      "learning_rate": 3.175879396984925e-05,
      "loss": 0.0703,
      "step": 842
    },
    {
      "epoch": 6.995850622406639,
      "grad_norm": 0.8091167211532593,
      "learning_rate": 3.155778894472362e-05,
      "loss": 0.0878,
      "step": 843
    },
    {
      "epoch": 7.004149377593361,
      "grad_norm": 0.5410608053207397,
      "learning_rate": 3.1356783919597993e-05,
      "loss": 0.0827,
      "step": 844
    },
    {
      "epoch": 7.012448132780083,
      "grad_norm": 0.47853708267211914,
      "learning_rate": 3.1155778894472366e-05,
      "loss": 0.0761,
      "step": 845
    },
    {
      "epoch": 7.020746887966805,
      "grad_norm": 0.5576235055923462,
      "learning_rate": 3.095477386934674e-05,
      "loss": 0.0923,
      "step": 846
    },
    {
      "epoch": 7.029045643153527,
      "grad_norm": 0.45836713910102844,
      "learning_rate": 3.075376884422111e-05,
      "loss": 0.0667,
      "step": 847
    },
    {
      "epoch": 7.037344398340249,
      "grad_norm": 0.4546763598918915,
      "learning_rate": 3.055276381909548e-05,
      "loss": 0.0736,
      "step": 848
    },
    {
      "epoch": 7.045643153526971,
      "grad_norm": 0.4241175949573517,
      "learning_rate": 3.0351758793969855e-05,
      "loss": 0.0604,
      "step": 849
    },
    {
      "epoch": 7.053941908713693,
      "grad_norm": 0.476601779460907,
      "learning_rate": 3.015075376884422e-05,
      "loss": 0.0754,
      "step": 850
    },
    {
      "epoch": 7.062240663900415,
      "grad_norm": 0.5420868396759033,
      "learning_rate": 2.994974874371859e-05,
      "loss": 0.0733,
      "step": 851
    },
    {
      "epoch": 7.070539419087137,
      "grad_norm": 0.5436594486236572,
      "learning_rate": 2.9748743718592964e-05,
      "loss": 0.075,
      "step": 852
    },
    {
      "epoch": 7.078838174273859,
      "grad_norm": 0.6780748963356018,
      "learning_rate": 2.9547738693467337e-05,
      "loss": 0.078,
      "step": 853
    },
    {
      "epoch": 7.087136929460581,
      "grad_norm": 0.8375440239906311,
      "learning_rate": 2.934673366834171e-05,
      "loss": 0.0936,
      "step": 854
    },
    {
      "epoch": 7.095435684647303,
      "grad_norm": 0.4651930034160614,
      "learning_rate": 2.914572864321608e-05,
      "loss": 0.0714,
      "step": 855
    },
    {
      "epoch": 7.1037344398340245,
      "grad_norm": 0.48011189699172974,
      "learning_rate": 2.8944723618090452e-05,
      "loss": 0.0678,
      "step": 856
    },
    {
      "epoch": 7.1120331950207465,
      "grad_norm": 0.4077259600162506,
      "learning_rate": 2.8743718592964825e-05,
      "loss": 0.062,
      "step": 857
    },
    {
      "epoch": 7.1203319502074685,
      "grad_norm": 0.46544888615608215,
      "learning_rate": 2.8542713567839198e-05,
      "loss": 0.0768,
      "step": 858
    },
    {
      "epoch": 7.12863070539419,
      "grad_norm": 0.600245475769043,
      "learning_rate": 2.8341708542713568e-05,
      "loss": 0.0807,
      "step": 859
    },
    {
      "epoch": 7.136929460580913,
      "grad_norm": 0.672461211681366,
      "learning_rate": 2.814070351758794e-05,
      "loss": 0.0777,
      "step": 860
    },
    {
      "epoch": 7.145228215767635,
      "grad_norm": 0.6946679949760437,
      "learning_rate": 2.7939698492462314e-05,
      "loss": 0.083,
      "step": 861
    },
    {
      "epoch": 7.153526970954357,
      "grad_norm": 0.5246666073799133,
      "learning_rate": 2.7738693467336686e-05,
      "loss": 0.0688,
      "step": 862
    },
    {
      "epoch": 7.161825726141079,
      "grad_norm": 0.6336027979850769,
      "learning_rate": 2.7537688442211056e-05,
      "loss": 0.0767,
      "step": 863
    },
    {
      "epoch": 7.170124481327801,
      "grad_norm": 0.44784456491470337,
      "learning_rate": 2.733668341708543e-05,
      "loss": 0.0637,
      "step": 864
    },
    {
      "epoch": 7.178423236514523,
      "grad_norm": 0.8395450711250305,
      "learning_rate": 2.7135678391959802e-05,
      "loss": 0.0947,
      "step": 865
    },
    {
      "epoch": 7.186721991701245,
      "grad_norm": 0.5171066522598267,
      "learning_rate": 2.6934673366834175e-05,
      "loss": 0.0695,
      "step": 866
    },
    {
      "epoch": 7.195020746887967,
      "grad_norm": 0.560386061668396,
      "learning_rate": 2.6733668341708545e-05,
      "loss": 0.0749,
      "step": 867
    },
    {
      "epoch": 7.203319502074689,
      "grad_norm": 0.5403150320053101,
      "learning_rate": 2.6532663316582917e-05,
      "loss": 0.0719,
      "step": 868
    },
    {
      "epoch": 7.211618257261411,
      "grad_norm": 0.7530381083488464,
      "learning_rate": 2.633165829145729e-05,
      "loss": 0.0869,
      "step": 869
    },
    {
      "epoch": 7.219917012448133,
      "grad_norm": 0.5552644729614258,
      "learning_rate": 2.613065326633166e-05,
      "loss": 0.0736,
      "step": 870
    },
    {
      "epoch": 7.228215767634855,
      "grad_norm": 0.45127713680267334,
      "learning_rate": 2.5929648241206033e-05,
      "loss": 0.0656,
      "step": 871
    },
    {
      "epoch": 7.236514522821577,
      "grad_norm": 0.649815022945404,
      "learning_rate": 2.5728643216080406e-05,
      "loss": 0.0705,
      "step": 872
    },
    {
      "epoch": 7.244813278008299,
      "grad_norm": 0.5871221423149109,
      "learning_rate": 2.5527638190954772e-05,
      "loss": 0.0741,
      "step": 873
    },
    {
      "epoch": 7.253112033195021,
      "grad_norm": 0.7311861515045166,
      "learning_rate": 2.5326633165829145e-05,
      "loss": 0.0818,
      "step": 874
    },
    {
      "epoch": 7.261410788381743,
      "grad_norm": 0.63481205701828,
      "learning_rate": 2.5125628140703518e-05,
      "loss": 0.0691,
      "step": 875
    },
    {
      "epoch": 7.269709543568465,
      "grad_norm": 0.5866575241088867,
      "learning_rate": 2.492462311557789e-05,
      "loss": 0.0765,
      "step": 876
    },
    {
      "epoch": 7.278008298755187,
      "grad_norm": 0.5032310485839844,
      "learning_rate": 2.4723618090452264e-05,
      "loss": 0.0613,
      "step": 877
    },
    {
      "epoch": 7.286307053941909,
      "grad_norm": 0.5864711403846741,
      "learning_rate": 2.4522613065326637e-05,
      "loss": 0.0688,
      "step": 878
    },
    {
      "epoch": 7.2946058091286305,
      "grad_norm": 0.5438239574432373,
      "learning_rate": 2.4321608040201007e-05,
      "loss": 0.0772,
      "step": 879
    },
    {
      "epoch": 7.3029045643153525,
      "grad_norm": 0.6457107067108154,
      "learning_rate": 2.4120603015075376e-05,
      "loss": 0.0646,
      "step": 880
    },
    {
      "epoch": 7.3112033195020745,
      "grad_norm": 0.40439456701278687,
      "learning_rate": 2.391959798994975e-05,
      "loss": 0.0606,
      "step": 881
    },
    {
      "epoch": 7.319502074688796,
      "grad_norm": 0.6005444526672363,
      "learning_rate": 2.3718592964824122e-05,
      "loss": 0.0827,
      "step": 882
    },
    {
      "epoch": 7.327800829875518,
      "grad_norm": 0.6400075554847717,
      "learning_rate": 2.351758793969849e-05,
      "loss": 0.0699,
      "step": 883
    },
    {
      "epoch": 7.33609958506224,
      "grad_norm": 0.6126580834388733,
      "learning_rate": 2.3316582914572865e-05,
      "loss": 0.0757,
      "step": 884
    },
    {
      "epoch": 7.344398340248962,
      "grad_norm": 0.5239130854606628,
      "learning_rate": 2.3115577889447238e-05,
      "loss": 0.0694,
      "step": 885
    },
    {
      "epoch": 7.352697095435684,
      "grad_norm": 0.5178470015525818,
      "learning_rate": 2.291457286432161e-05,
      "loss": 0.0659,
      "step": 886
    },
    {
      "epoch": 7.360995850622406,
      "grad_norm": 0.4848046600818634,
      "learning_rate": 2.271356783919598e-05,
      "loss": 0.0655,
      "step": 887
    },
    {
      "epoch": 7.369294605809129,
      "grad_norm": 0.34291940927505493,
      "learning_rate": 2.2512562814070353e-05,
      "loss": 0.056,
      "step": 888
    },
    {
      "epoch": 7.377593360995851,
      "grad_norm": 0.5391823053359985,
      "learning_rate": 2.2311557788944726e-05,
      "loss": 0.0891,
      "step": 889
    },
    {
      "epoch": 7.385892116182573,
      "grad_norm": 0.5576613545417786,
      "learning_rate": 2.21105527638191e-05,
      "loss": 0.078,
      "step": 890
    },
    {
      "epoch": 7.394190871369295,
      "grad_norm": 0.6881992220878601,
      "learning_rate": 2.190954773869347e-05,
      "loss": 0.0769,
      "step": 891
    },
    {
      "epoch": 7.402489626556017,
      "grad_norm": 0.5726189017295837,
      "learning_rate": 2.1708542713567838e-05,
      "loss": 0.0806,
      "step": 892
    },
    {
      "epoch": 7.410788381742739,
      "grad_norm": 0.3710097074508667,
      "learning_rate": 2.150753768844221e-05,
      "loss": 0.0592,
      "step": 893
    },
    {
      "epoch": 7.419087136929461,
      "grad_norm": 0.5538682341575623,
      "learning_rate": 2.1306532663316584e-05,
      "loss": 0.0698,
      "step": 894
    },
    {
      "epoch": 7.427385892116183,
      "grad_norm": 0.4645507335662842,
      "learning_rate": 2.1105527638190957e-05,
      "loss": 0.0622,
      "step": 895
    },
    {
      "epoch": 7.435684647302905,
      "grad_norm": 0.4964620769023895,
      "learning_rate": 2.0904522613065327e-05,
      "loss": 0.0662,
      "step": 896
    },
    {
      "epoch": 7.443983402489627,
      "grad_norm": 0.5505067110061646,
      "learning_rate": 2.07035175879397e-05,
      "loss": 0.064,
      "step": 897
    },
    {
      "epoch": 7.452282157676349,
      "grad_norm": 0.6311980485916138,
      "learning_rate": 2.0502512562814073e-05,
      "loss": 0.0782,
      "step": 898
    },
    {
      "epoch": 7.460580912863071,
      "grad_norm": 0.46925538778305054,
      "learning_rate": 2.0301507537688446e-05,
      "loss": 0.0616,
      "step": 899
    },
    {
      "epoch": 7.468879668049793,
      "grad_norm": 0.6708559989929199,
      "learning_rate": 2.0100502512562815e-05,
      "loss": 0.0798,
      "step": 900
    },
    {
      "epoch": 7.477178423236515,
      "grad_norm": 0.5328362584114075,
      "learning_rate": 1.9899497487437188e-05,
      "loss": 0.0659,
      "step": 901
    },
    {
      "epoch": 7.485477178423237,
      "grad_norm": 0.5193760395050049,
      "learning_rate": 1.9698492462311558e-05,
      "loss": 0.0679,
      "step": 902
    },
    {
      "epoch": 7.4937759336099585,
      "grad_norm": 0.586810290813446,
      "learning_rate": 1.949748743718593e-05,
      "loss": 0.0721,
      "step": 903
    },
    {
      "epoch": 7.5020746887966805,
      "grad_norm": 0.3457413911819458,
      "learning_rate": 1.92964824120603e-05,
      "loss": 0.0539,
      "step": 904
    },
    {
      "epoch": 7.5103734439834025,
      "grad_norm": 0.557894229888916,
      "learning_rate": 1.9095477386934673e-05,
      "loss": 0.0667,
      "step": 905
    },
    {
      "epoch": 7.518672199170124,
      "grad_norm": 0.6983307600021362,
      "learning_rate": 1.8894472361809046e-05,
      "loss": 0.0684,
      "step": 906
    },
    {
      "epoch": 7.526970954356846,
      "grad_norm": 0.5809400677680969,
      "learning_rate": 1.869346733668342e-05,
      "loss": 0.0803,
      "step": 907
    },
    {
      "epoch": 7.535269709543568,
      "grad_norm": 0.6888753771781921,
      "learning_rate": 1.849246231155779e-05,
      "loss": 0.0672,
      "step": 908
    },
    {
      "epoch": 7.54356846473029,
      "grad_norm": 0.550929844379425,
      "learning_rate": 1.829145728643216e-05,
      "loss": 0.0671,
      "step": 909
    },
    {
      "epoch": 7.551867219917012,
      "grad_norm": 0.5111249089241028,
      "learning_rate": 1.8090452261306535e-05,
      "loss": 0.0662,
      "step": 910
    },
    {
      "epoch": 7.560165975103734,
      "grad_norm": 0.46653032302856445,
      "learning_rate": 1.7889447236180908e-05,
      "loss": 0.0685,
      "step": 911
    },
    {
      "epoch": 7.568464730290456,
      "grad_norm": 0.5615831613540649,
      "learning_rate": 1.7688442211055277e-05,
      "loss": 0.0779,
      "step": 912
    },
    {
      "epoch": 7.576763485477178,
      "grad_norm": 0.4699278175830841,
      "learning_rate": 1.748743718592965e-05,
      "loss": 0.0727,
      "step": 913
    },
    {
      "epoch": 7.5850622406639,
      "grad_norm": 0.4296228885650635,
      "learning_rate": 1.728643216080402e-05,
      "loss": 0.0615,
      "step": 914
    },
    {
      "epoch": 7.593360995850622,
      "grad_norm": 0.5667852759361267,
      "learning_rate": 1.7085427135678393e-05,
      "loss": 0.0763,
      "step": 915
    },
    {
      "epoch": 7.601659751037344,
      "grad_norm": 1.5754680633544922,
      "learning_rate": 1.6884422110552762e-05,
      "loss": 0.0953,
      "step": 916
    },
    {
      "epoch": 7.609958506224066,
      "grad_norm": 0.5400224328041077,
      "learning_rate": 1.6683417085427135e-05,
      "loss": 0.066,
      "step": 917
    },
    {
      "epoch": 7.618257261410788,
      "grad_norm": 0.3979361355304718,
      "learning_rate": 1.6482412060301508e-05,
      "loss": 0.0614,
      "step": 918
    },
    {
      "epoch": 7.62655601659751,
      "grad_norm": 0.42722395062446594,
      "learning_rate": 1.628140703517588e-05,
      "loss": 0.0548,
      "step": 919
    },
    {
      "epoch": 7.634854771784232,
      "grad_norm": 0.5607731342315674,
      "learning_rate": 1.608040201005025e-05,
      "loss": 0.0671,
      "step": 920
    },
    {
      "epoch": 7.643153526970955,
      "grad_norm": 0.6461376547813416,
      "learning_rate": 1.5879396984924624e-05,
      "loss": 0.0833,
      "step": 921
    },
    {
      "epoch": 7.651452282157677,
      "grad_norm": 0.5389935374259949,
      "learning_rate": 1.5678391959798997e-05,
      "loss": 0.0777,
      "step": 922
    },
    {
      "epoch": 7.659751037344399,
      "grad_norm": 0.593097984790802,
      "learning_rate": 1.547738693467337e-05,
      "loss": 0.0857,
      "step": 923
    },
    {
      "epoch": 7.668049792531121,
      "grad_norm": 0.5157641172409058,
      "learning_rate": 1.527638190954774e-05,
      "loss": 0.0724,
      "step": 924
    },
    {
      "epoch": 7.676348547717843,
      "grad_norm": 0.5448418855667114,
      "learning_rate": 1.507537688442211e-05,
      "loss": 0.0708,
      "step": 925
    },
    {
      "epoch": 7.6846473029045645,
      "grad_norm": 0.5996012687683105,
      "learning_rate": 1.4874371859296482e-05,
      "loss": 0.0718,
      "step": 926
    },
    {
      "epoch": 7.6929460580912865,
      "grad_norm": 0.5735124945640564,
      "learning_rate": 1.4673366834170855e-05,
      "loss": 0.0697,
      "step": 927
    },
    {
      "epoch": 7.7012448132780085,
      "grad_norm": 0.7297540903091431,
      "learning_rate": 1.4472361809045226e-05,
      "loss": 0.091,
      "step": 928
    },
    {
      "epoch": 7.70954356846473,
      "grad_norm": 0.5380595326423645,
      "learning_rate": 1.4271356783919599e-05,
      "loss": 0.0655,
      "step": 929
    },
    {
      "epoch": 7.717842323651452,
      "grad_norm": 0.549006998538971,
      "learning_rate": 1.407035175879397e-05,
      "loss": 0.0723,
      "step": 930
    },
    {
      "epoch": 7.726141078838174,
      "grad_norm": 0.8147135972976685,
      "learning_rate": 1.3869346733668343e-05,
      "loss": 0.08,
      "step": 931
    },
    {
      "epoch": 7.734439834024896,
      "grad_norm": 0.5473212599754333,
      "learning_rate": 1.3668341708542715e-05,
      "loss": 0.0716,
      "step": 932
    },
    {
      "epoch": 7.742738589211618,
      "grad_norm": 0.6689213514328003,
      "learning_rate": 1.3467336683417087e-05,
      "loss": 0.0865,
      "step": 933
    },
    {
      "epoch": 7.75103734439834,
      "grad_norm": 0.5425443649291992,
      "learning_rate": 1.3266331658291459e-05,
      "loss": 0.0628,
      "step": 934
    },
    {
      "epoch": 7.759336099585062,
      "grad_norm": 0.43777182698249817,
      "learning_rate": 1.306532663316583e-05,
      "loss": 0.0703,
      "step": 935
    },
    {
      "epoch": 7.767634854771784,
      "grad_norm": 0.4458577334880829,
      "learning_rate": 1.2864321608040203e-05,
      "loss": 0.0602,
      "step": 936
    },
    {
      "epoch": 7.775933609958506,
      "grad_norm": 0.47705742716789246,
      "learning_rate": 1.2663316582914573e-05,
      "loss": 0.067,
      "step": 937
    },
    {
      "epoch": 7.784232365145228,
      "grad_norm": 0.53620845079422,
      "learning_rate": 1.2462311557788946e-05,
      "loss": 0.0727,
      "step": 938
    },
    {
      "epoch": 7.79253112033195,
      "grad_norm": 0.4756992757320404,
      "learning_rate": 1.2261306532663318e-05,
      "loss": 0.0641,
      "step": 939
    },
    {
      "epoch": 7.800829875518672,
      "grad_norm": 0.43085986375808716,
      "learning_rate": 1.2060301507537688e-05,
      "loss": 0.0618,
      "step": 940
    },
    {
      "epoch": 7.809128630705394,
      "grad_norm": 0.43521931767463684,
      "learning_rate": 1.1859296482412061e-05,
      "loss": 0.0648,
      "step": 941
    },
    {
      "epoch": 7.817427385892116,
      "grad_norm": 0.4934470057487488,
      "learning_rate": 1.1658291457286432e-05,
      "loss": 0.0664,
      "step": 942
    },
    {
      "epoch": 7.825726141078838,
      "grad_norm": 0.6725133657455444,
      "learning_rate": 1.1457286432160805e-05,
      "loss": 0.0825,
      "step": 943
    },
    {
      "epoch": 7.83402489626556,
      "grad_norm": 0.5663688778877258,
      "learning_rate": 1.1256281407035177e-05,
      "loss": 0.0734,
      "step": 944
    },
    {
      "epoch": 7.842323651452282,
      "grad_norm": 0.4717971384525299,
      "learning_rate": 1.105527638190955e-05,
      "loss": 0.0683,
      "step": 945
    },
    {
      "epoch": 7.850622406639004,
      "grad_norm": 0.5216928124427795,
      "learning_rate": 1.0854271356783919e-05,
      "loss": 0.0655,
      "step": 946
    },
    {
      "epoch": 7.858921161825726,
      "grad_norm": 0.847352921962738,
      "learning_rate": 1.0653266331658292e-05,
      "loss": 0.0836,
      "step": 947
    },
    {
      "epoch": 7.867219917012449,
      "grad_norm": 0.8572785258293152,
      "learning_rate": 1.0452261306532663e-05,
      "loss": 0.0923,
      "step": 948
    },
    {
      "epoch": 7.875518672199171,
      "grad_norm": 0.4681932032108307,
      "learning_rate": 1.0251256281407036e-05,
      "loss": 0.0622,
      "step": 949
    },
    {
      "epoch": 7.8838174273858925,
      "grad_norm": 0.38679009675979614,
      "learning_rate": 1.0050251256281408e-05,
      "loss": 0.0572,
      "step": 950
    },
    {
      "epoch": 7.8921161825726145,
      "grad_norm": 0.7565784454345703,
      "learning_rate": 9.849246231155779e-06,
      "loss": 0.0725,
      "step": 951
    },
    {
      "epoch": 7.9004149377593365,
      "grad_norm": 0.526689350605011,
      "learning_rate": 9.64824120603015e-06,
      "loss": 0.0772,
      "step": 952
    },
    {
      "epoch": 7.908713692946058,
      "grad_norm": 0.6407788395881653,
      "learning_rate": 9.447236180904523e-06,
      "loss": 0.0699,
      "step": 953
    },
    {
      "epoch": 7.91701244813278,
      "grad_norm": 0.6181537508964539,
      "learning_rate": 9.246231155778894e-06,
      "loss": 0.0803,
      "step": 954
    },
    {
      "epoch": 7.925311203319502,
      "grad_norm": 0.7875078320503235,
      "learning_rate": 9.045226130653267e-06,
      "loss": 0.0922,
      "step": 955
    },
    {
      "epoch": 7.933609958506224,
      "grad_norm": 0.44739100337028503,
      "learning_rate": 8.844221105527639e-06,
      "loss": 0.0628,
      "step": 956
    },
    {
      "epoch": 7.941908713692946,
      "grad_norm": 0.4995870888233185,
      "learning_rate": 8.64321608040201e-06,
      "loss": 0.0716,
      "step": 957
    },
    {
      "epoch": 7.950207468879668,
      "grad_norm": 0.5081132650375366,
      "learning_rate": 8.442211055276381e-06,
      "loss": 0.0703,
      "step": 958
    },
    {
      "epoch": 7.95850622406639,
      "grad_norm": 0.40601882338523865,
      "learning_rate": 8.241206030150754e-06,
      "loss": 0.0705,
      "step": 959
    },
    {
      "epoch": 7.966804979253112,
      "grad_norm": 0.8020235896110535,
      "learning_rate": 8.040201005025125e-06,
      "loss": 0.071,
      "step": 960
    },
    {
      "epoch": 7.975103734439834,
      "grad_norm": 0.5364794135093689,
      "learning_rate": 7.839195979899498e-06,
      "loss": 0.0592,
      "step": 961
    },
    {
      "epoch": 7.983402489626556,
      "grad_norm": 0.5275890231132507,
      "learning_rate": 7.63819095477387e-06,
      "loss": 0.0721,
      "step": 962
    },
    {
      "epoch": 7.991701244813278,
      "grad_norm": 0.5697305798530579,
      "learning_rate": 7.437185929648241e-06,
      "loss": 0.0751,
      "step": 963
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.6311787366867065,
      "learning_rate": 7.236180904522613e-06,
      "loss": 0.0677,
      "step": 964
    },
    {
      "epoch": 8.008298755186722,
      "grad_norm": 0.45691391825675964,
      "learning_rate": 7.035175879396985e-06,
      "loss": 0.0649,
      "step": 965
    },
    {
      "epoch": 8.016597510373444,
      "grad_norm": 0.4236806333065033,
      "learning_rate": 6.834170854271357e-06,
      "loss": 0.066,
      "step": 966
    },
    {
      "epoch": 8.024896265560166,
      "grad_norm": 0.4765545129776001,
      "learning_rate": 6.633165829145729e-06,
      "loss": 0.0661,
      "step": 967
    },
    {
      "epoch": 8.033195020746888,
      "grad_norm": 0.3851591646671295,
      "learning_rate": 6.4321608040201015e-06,
      "loss": 0.057,
      "step": 968
    },
    {
      "epoch": 8.04149377593361,
      "grad_norm": 0.41427183151245117,
      "learning_rate": 6.231155778894473e-06,
      "loss": 0.0658,
      "step": 969
    },
    {
      "epoch": 8.049792531120332,
      "grad_norm": 0.5848067998886108,
      "learning_rate": 6.030150753768844e-06,
      "loss": 0.0705,
      "step": 970
    },
    {
      "epoch": 8.058091286307054,
      "grad_norm": 0.3359411060810089,
      "learning_rate": 5.829145728643216e-06,
      "loss": 0.0588,
      "step": 971
    },
    {
      "epoch": 8.066390041493776,
      "grad_norm": 0.3493482768535614,
      "learning_rate": 5.628140703517588e-06,
      "loss": 0.0565,
      "step": 972
    },
    {
      "epoch": 8.074688796680498,
      "grad_norm": 0.4275664687156677,
      "learning_rate": 5.4271356783919595e-06,
      "loss": 0.0633,
      "step": 973
    },
    {
      "epoch": 8.08298755186722,
      "grad_norm": 0.3594025671482086,
      "learning_rate": 5.226130653266332e-06,
      "loss": 0.0542,
      "step": 974
    },
    {
      "epoch": 8.091286307053942,
      "grad_norm": 0.3199184238910675,
      "learning_rate": 5.025125628140704e-06,
      "loss": 0.0488,
      "step": 975
    },
    {
      "epoch": 8.099585062240664,
      "grad_norm": 0.4421146810054779,
      "learning_rate": 4.824120603015075e-06,
      "loss": 0.0751,
      "step": 976
    },
    {
      "epoch": 8.107883817427386,
      "grad_norm": 0.41225141286849976,
      "learning_rate": 4.623115577889447e-06,
      "loss": 0.0566,
      "step": 977
    },
    {
      "epoch": 8.116182572614107,
      "grad_norm": 0.44608601927757263,
      "learning_rate": 4.422110552763819e-06,
      "loss": 0.0731,
      "step": 978
    },
    {
      "epoch": 8.12448132780083,
      "grad_norm": 0.3318224549293518,
      "learning_rate": 4.2211055276381906e-06,
      "loss": 0.0566,
      "step": 979
    },
    {
      "epoch": 8.132780082987551,
      "grad_norm": 0.4134286046028137,
      "learning_rate": 4.020100502512563e-06,
      "loss": 0.0615,
      "step": 980
    },
    {
      "epoch": 8.141078838174273,
      "grad_norm": 0.4410179555416107,
      "learning_rate": 3.819095477386935e-06,
      "loss": 0.0651,
      "step": 981
    },
    {
      "epoch": 8.149377593360995,
      "grad_norm": 0.4014989733695984,
      "learning_rate": 3.6180904522613065e-06,
      "loss": 0.0577,
      "step": 982
    },
    {
      "epoch": 8.157676348547717,
      "grad_norm": 0.46714890003204346,
      "learning_rate": 3.4170854271356786e-06,
      "loss": 0.0766,
      "step": 983
    },
    {
      "epoch": 8.16597510373444,
      "grad_norm": 0.35950058698654175,
      "learning_rate": 3.2160804020100507e-06,
      "loss": 0.0544,
      "step": 984
    },
    {
      "epoch": 8.174273858921161,
      "grad_norm": 0.3269845247268677,
      "learning_rate": 3.015075376884422e-06,
      "loss": 0.0577,
      "step": 985
    },
    {
      "epoch": 8.182572614107883,
      "grad_norm": 0.4085640013217926,
      "learning_rate": 2.814070351758794e-06,
      "loss": 0.0584,
      "step": 986
    },
    {
      "epoch": 8.190871369294605,
      "grad_norm": 0.404848575592041,
      "learning_rate": 2.613065326633166e-06,
      "loss": 0.0592,
      "step": 987
    },
    {
      "epoch": 8.199170124481327,
      "grad_norm": 0.4414076805114746,
      "learning_rate": 2.4120603015075375e-06,
      "loss": 0.0673,
      "step": 988
    },
    {
      "epoch": 8.207468879668049,
      "grad_norm": 0.3407441973686218,
      "learning_rate": 2.2110552763819096e-06,
      "loss": 0.0538,
      "step": 989
    },
    {
      "epoch": 8.215767634854771,
      "grad_norm": 0.4204394519329071,
      "learning_rate": 2.0100502512562813e-06,
      "loss": 0.0617,
      "step": 990
    },
    {
      "epoch": 8.224066390041493,
      "grad_norm": 0.42020073533058167,
      "learning_rate": 1.8090452261306533e-06,
      "loss": 0.0639,
      "step": 991
    },
    {
      "epoch": 8.232365145228215,
      "grad_norm": 0.35948267579078674,
      "learning_rate": 1.6080402010050254e-06,
      "loss": 0.0529,
      "step": 992
    },
    {
      "epoch": 8.240663900414937,
      "grad_norm": 0.46226340532302856,
      "learning_rate": 1.407035175879397e-06,
      "loss": 0.0606,
      "step": 993
    },
    {
      "epoch": 8.248962655601659,
      "grad_norm": 0.4195599853992462,
      "learning_rate": 1.2060301507537688e-06,
      "loss": 0.064,
      "step": 994
    },
    {
      "epoch": 8.25726141078838,
      "grad_norm": 0.5524742007255554,
      "learning_rate": 1.0050251256281407e-06,
      "loss": 0.0697,
      "step": 995
    },
    {
      "epoch": 8.265560165975105,
      "grad_norm": 0.4629044532775879,
      "learning_rate": 8.040201005025127e-07,
      "loss": 0.064,
      "step": 996
    },
    {
      "epoch": 8.273858921161827,
      "grad_norm": 0.5302028656005859,
      "learning_rate": 6.030150753768844e-07,
      "loss": 0.071,
      "step": 997
    },
    {
      "epoch": 8.282157676348548,
      "grad_norm": 0.4662427306175232,
      "learning_rate": 4.0201005025125634e-07,
      "loss": 0.0641,
      "step": 998
    },
    {
      "epoch": 8.29045643153527,
      "grad_norm": 0.4106457233428955,
      "learning_rate": 2.0100502512562817e-07,
      "loss": 0.0643,
      "step": 999
    },
    {
      "epoch": 8.298755186721992,
      "grad_norm": 0.6027162075042725,
      "learning_rate": 0.0,
      "loss": 0.0815,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.114818534849997e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
