{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.149377593360996,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008298755186721992,
      "grad_norm": 9.834770202636719,
      "learning_rate": 4e-05,
      "loss": 3.1989,
      "step": 1
    },
    {
      "epoch": 0.016597510373443983,
      "grad_norm": 8.876495361328125,
      "learning_rate": 8e-05,
      "loss": 2.7387,
      "step": 2
    },
    {
      "epoch": 0.024896265560165973,
      "grad_norm": 7.920222759246826,
      "learning_rate": 0.00012,
      "loss": 2.5127,
      "step": 3
    },
    {
      "epoch": 0.03319502074688797,
      "grad_norm": 8.231279373168945,
      "learning_rate": 0.00016,
      "loss": 2.8868,
      "step": 4
    },
    {
      "epoch": 0.04149377593360996,
      "grad_norm": 5.689220428466797,
      "learning_rate": 0.0002,
      "loss": 1.9865,
      "step": 5
    },
    {
      "epoch": 0.04979253112033195,
      "grad_norm": 5.014395713806152,
      "learning_rate": 0.00019979899497487438,
      "loss": 1.914,
      "step": 6
    },
    {
      "epoch": 0.058091286307053944,
      "grad_norm": 3.4225754737854004,
      "learning_rate": 0.00019959798994974876,
      "loss": 1.6518,
      "step": 7
    },
    {
      "epoch": 0.06639004149377593,
      "grad_norm": 2.520620584487915,
      "learning_rate": 0.00019939698492462313,
      "loss": 1.5366,
      "step": 8
    },
    {
      "epoch": 0.07468879668049792,
      "grad_norm": 1.9455488920211792,
      "learning_rate": 0.0001991959798994975,
      "loss": 1.495,
      "step": 9
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 1.434770107269287,
      "learning_rate": 0.00019899497487437187,
      "loss": 1.3669,
      "step": 10
    },
    {
      "epoch": 0.0912863070539419,
      "grad_norm": 1.3720701932907104,
      "learning_rate": 0.00019879396984924622,
      "loss": 1.4437,
      "step": 11
    },
    {
      "epoch": 0.0995850622406639,
      "grad_norm": 1.2090380191802979,
      "learning_rate": 0.00019859296482412062,
      "loss": 1.2371,
      "step": 12
    },
    {
      "epoch": 0.1078838174273859,
      "grad_norm": 1.174675703048706,
      "learning_rate": 0.000198391959798995,
      "loss": 1.3927,
      "step": 13
    },
    {
      "epoch": 0.11618257261410789,
      "grad_norm": 1.1215078830718994,
      "learning_rate": 0.00019819095477386937,
      "loss": 1.3365,
      "step": 14
    },
    {
      "epoch": 0.12448132780082988,
      "grad_norm": 1.0751546621322632,
      "learning_rate": 0.0001979899497487437,
      "loss": 1.3854,
      "step": 15
    },
    {
      "epoch": 0.13278008298755187,
      "grad_norm": 1.140397548675537,
      "learning_rate": 0.0001977889447236181,
      "loss": 1.3481,
      "step": 16
    },
    {
      "epoch": 0.14107883817427386,
      "grad_norm": 1.0643315315246582,
      "learning_rate": 0.00019758793969849249,
      "loss": 1.2802,
      "step": 17
    },
    {
      "epoch": 0.14937759336099585,
      "grad_norm": 1.1096618175506592,
      "learning_rate": 0.00019738693467336683,
      "loss": 1.3606,
      "step": 18
    },
    {
      "epoch": 0.15767634854771784,
      "grad_norm": 1.0394700765609741,
      "learning_rate": 0.0001971859296482412,
      "loss": 1.1443,
      "step": 19
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 1.058250904083252,
      "learning_rate": 0.0001969849246231156,
      "loss": 1.2893,
      "step": 20
    },
    {
      "epoch": 0.17427385892116182,
      "grad_norm": 1.0969181060791016,
      "learning_rate": 0.00019678391959798995,
      "loss": 1.3521,
      "step": 21
    },
    {
      "epoch": 0.1825726141078838,
      "grad_norm": 1.095960021018982,
      "learning_rate": 0.00019658291457286432,
      "loss": 1.3592,
      "step": 22
    },
    {
      "epoch": 0.1908713692946058,
      "grad_norm": 1.1666111946105957,
      "learning_rate": 0.0001963819095477387,
      "loss": 1.34,
      "step": 23
    },
    {
      "epoch": 0.1991701244813278,
      "grad_norm": 1.1203254461288452,
      "learning_rate": 0.0001961809045226131,
      "loss": 1.3133,
      "step": 24
    },
    {
      "epoch": 0.2074688796680498,
      "grad_norm": 1.1151355504989624,
      "learning_rate": 0.00019597989949748744,
      "loss": 1.2779,
      "step": 25
    },
    {
      "epoch": 0.2157676348547718,
      "grad_norm": 1.046364426612854,
      "learning_rate": 0.00019577889447236181,
      "loss": 1.2051,
      "step": 26
    },
    {
      "epoch": 0.22406639004149378,
      "grad_norm": 0.9693098068237305,
      "learning_rate": 0.0001955778894472362,
      "loss": 0.9997,
      "step": 27
    },
    {
      "epoch": 0.23236514522821577,
      "grad_norm": 1.026024341583252,
      "learning_rate": 0.00019537688442211056,
      "loss": 1.2023,
      "step": 28
    },
    {
      "epoch": 0.24066390041493776,
      "grad_norm": 1.1691055297851562,
      "learning_rate": 0.00019517587939698493,
      "loss": 1.4548,
      "step": 29
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 1.0435459613800049,
      "learning_rate": 0.0001949748743718593,
      "loss": 1.1723,
      "step": 30
    },
    {
      "epoch": 0.2572614107883817,
      "grad_norm": 0.906079888343811,
      "learning_rate": 0.00019477386934673368,
      "loss": 1.0041,
      "step": 31
    },
    {
      "epoch": 0.26556016597510373,
      "grad_norm": 0.9865516424179077,
      "learning_rate": 0.00019457286432160805,
      "loss": 1.1942,
      "step": 32
    },
    {
      "epoch": 0.27385892116182575,
      "grad_norm": 1.021187424659729,
      "learning_rate": 0.00019437185929648243,
      "loss": 1.2537,
      "step": 33
    },
    {
      "epoch": 0.2821576763485477,
      "grad_norm": 0.9775299429893494,
      "learning_rate": 0.0001941708542713568,
      "loss": 1.1173,
      "step": 34
    },
    {
      "epoch": 0.29045643153526973,
      "grad_norm": 0.9696130156517029,
      "learning_rate": 0.00019396984924623117,
      "loss": 1.2971,
      "step": 35
    },
    {
      "epoch": 0.2987551867219917,
      "grad_norm": 0.9941537976264954,
      "learning_rate": 0.00019376884422110552,
      "loss": 1.1919,
      "step": 36
    },
    {
      "epoch": 0.3070539419087137,
      "grad_norm": 0.999888002872467,
      "learning_rate": 0.00019356783919597992,
      "loss": 1.1837,
      "step": 37
    },
    {
      "epoch": 0.3153526970954357,
      "grad_norm": 0.9754721522331238,
      "learning_rate": 0.0001933668341708543,
      "loss": 1.1716,
      "step": 38
    },
    {
      "epoch": 0.3236514522821577,
      "grad_norm": 1.0635788440704346,
      "learning_rate": 0.00019316582914572864,
      "loss": 1.1674,
      "step": 39
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 0.9573739767074585,
      "learning_rate": 0.000192964824120603,
      "loss": 1.2759,
      "step": 40
    },
    {
      "epoch": 0.34024896265560167,
      "grad_norm": 0.9173368811607361,
      "learning_rate": 0.0001927638190954774,
      "loss": 1.0878,
      "step": 41
    },
    {
      "epoch": 0.34854771784232363,
      "grad_norm": 1.0573523044586182,
      "learning_rate": 0.00019256281407035178,
      "loss": 1.2958,
      "step": 42
    },
    {
      "epoch": 0.35684647302904565,
      "grad_norm": 0.9285069704055786,
      "learning_rate": 0.00019236180904522613,
      "loss": 0.9416,
      "step": 43
    },
    {
      "epoch": 0.3651452282157676,
      "grad_norm": 1.0217926502227783,
      "learning_rate": 0.0001921608040201005,
      "loss": 1.2428,
      "step": 44
    },
    {
      "epoch": 0.37344398340248963,
      "grad_norm": 0.9715312123298645,
      "learning_rate": 0.0001919597989949749,
      "loss": 1.0571,
      "step": 45
    },
    {
      "epoch": 0.3817427385892116,
      "grad_norm": 0.9465537071228027,
      "learning_rate": 0.00019175879396984925,
      "loss": 1.2466,
      "step": 46
    },
    {
      "epoch": 0.3900414937759336,
      "grad_norm": 0.9062249660491943,
      "learning_rate": 0.00019155778894472362,
      "loss": 1.0952,
      "step": 47
    },
    {
      "epoch": 0.3983402489626556,
      "grad_norm": 0.8801530599594116,
      "learning_rate": 0.000191356783919598,
      "loss": 1.1594,
      "step": 48
    },
    {
      "epoch": 0.4066390041493776,
      "grad_norm": 0.8837567567825317,
      "learning_rate": 0.0001911557788944724,
      "loss": 1.2214,
      "step": 49
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 0.9074248671531677,
      "learning_rate": 0.00019095477386934674,
      "loss": 1.1842,
      "step": 50
    },
    {
      "epoch": 0.42323651452282157,
      "grad_norm": 0.9016375541687012,
      "learning_rate": 0.0001907537688442211,
      "loss": 1.0276,
      "step": 51
    },
    {
      "epoch": 0.4315352697095436,
      "grad_norm": 0.9039750099182129,
      "learning_rate": 0.00019055276381909548,
      "loss": 1.143,
      "step": 52
    },
    {
      "epoch": 0.43983402489626555,
      "grad_norm": 0.9559658169746399,
      "learning_rate": 0.00019035175879396986,
      "loss": 1.2638,
      "step": 53
    },
    {
      "epoch": 0.44813278008298757,
      "grad_norm": 0.9471536874771118,
      "learning_rate": 0.00019015075376884423,
      "loss": 1.4242,
      "step": 54
    },
    {
      "epoch": 0.45643153526970953,
      "grad_norm": 0.9232811331748962,
      "learning_rate": 0.0001899497487437186,
      "loss": 1.1486,
      "step": 55
    },
    {
      "epoch": 0.46473029045643155,
      "grad_norm": 0.8779085874557495,
      "learning_rate": 0.00018974874371859298,
      "loss": 1.0212,
      "step": 56
    },
    {
      "epoch": 0.4730290456431535,
      "grad_norm": 0.8986270427703857,
      "learning_rate": 0.00018954773869346732,
      "loss": 1.0882,
      "step": 57
    },
    {
      "epoch": 0.48132780082987553,
      "grad_norm": 0.9575268030166626,
      "learning_rate": 0.00018934673366834172,
      "loss": 1.0609,
      "step": 58
    },
    {
      "epoch": 0.4896265560165975,
      "grad_norm": 0.8881059885025024,
      "learning_rate": 0.0001891457286432161,
      "loss": 0.984,
      "step": 59
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 0.9802026152610779,
      "learning_rate": 0.00018894472361809047,
      "loss": 1.0065,
      "step": 60
    },
    {
      "epoch": 0.5062240663900415,
      "grad_norm": 0.8956257104873657,
      "learning_rate": 0.00018874371859296481,
      "loss": 1.0255,
      "step": 61
    },
    {
      "epoch": 0.5145228215767634,
      "grad_norm": 0.9210503697395325,
      "learning_rate": 0.00018854271356783921,
      "loss": 1.0526,
      "step": 62
    },
    {
      "epoch": 0.5228215767634855,
      "grad_norm": 1.0349193811416626,
      "learning_rate": 0.0001883417085427136,
      "loss": 1.2933,
      "step": 63
    },
    {
      "epoch": 0.5311203319502075,
      "grad_norm": 0.9830848574638367,
      "learning_rate": 0.00018814070351758793,
      "loss": 1.2576,
      "step": 64
    },
    {
      "epoch": 0.5394190871369294,
      "grad_norm": 1.0153281688690186,
      "learning_rate": 0.0001879396984924623,
      "loss": 1.2889,
      "step": 65
    },
    {
      "epoch": 0.5477178423236515,
      "grad_norm": 0.9067654013633728,
      "learning_rate": 0.0001877386934673367,
      "loss": 1.004,
      "step": 66
    },
    {
      "epoch": 0.5560165975103735,
      "grad_norm": 0.9022826552391052,
      "learning_rate": 0.00018753768844221108,
      "loss": 1.0246,
      "step": 67
    },
    {
      "epoch": 0.5643153526970954,
      "grad_norm": 0.8329315781593323,
      "learning_rate": 0.00018733668341708543,
      "loss": 1.0887,
      "step": 68
    },
    {
      "epoch": 0.5726141078838174,
      "grad_norm": 0.9449093341827393,
      "learning_rate": 0.0001871356783919598,
      "loss": 0.9657,
      "step": 69
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 0.8289721608161926,
      "learning_rate": 0.0001869346733668342,
      "loss": 1.0229,
      "step": 70
    },
    {
      "epoch": 0.5892116182572614,
      "grad_norm": 0.8489035367965698,
      "learning_rate": 0.00018673366834170854,
      "loss": 1.1857,
      "step": 71
    },
    {
      "epoch": 0.5975103734439834,
      "grad_norm": 0.9677807688713074,
      "learning_rate": 0.00018653266331658292,
      "loss": 1.1606,
      "step": 72
    },
    {
      "epoch": 0.6058091286307054,
      "grad_norm": 0.8593841791152954,
      "learning_rate": 0.0001863316582914573,
      "loss": 1.1037,
      "step": 73
    },
    {
      "epoch": 0.6141078838174274,
      "grad_norm": 1.0242351293563843,
      "learning_rate": 0.0001861306532663317,
      "loss": 1.3459,
      "step": 74
    },
    {
      "epoch": 0.6224066390041494,
      "grad_norm": 1.0419251918792725,
      "learning_rate": 0.00018592964824120604,
      "loss": 1.1912,
      "step": 75
    },
    {
      "epoch": 0.6307053941908713,
      "grad_norm": 0.8729200959205627,
      "learning_rate": 0.0001857286432160804,
      "loss": 1.0862,
      "step": 76
    },
    {
      "epoch": 0.6390041493775933,
      "grad_norm": 0.7877036333084106,
      "learning_rate": 0.00018552763819095478,
      "loss": 0.8849,
      "step": 77
    },
    {
      "epoch": 0.6473029045643154,
      "grad_norm": 0.9751052260398865,
      "learning_rate": 0.00018532663316582915,
      "loss": 1.303,
      "step": 78
    },
    {
      "epoch": 0.6556016597510373,
      "grad_norm": 0.9295453429222107,
      "learning_rate": 0.00018512562814070353,
      "loss": 1.0281,
      "step": 79
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 0.928661584854126,
      "learning_rate": 0.0001849246231155779,
      "loss": 1.2213,
      "step": 80
    },
    {
      "epoch": 0.6721991701244814,
      "grad_norm": 1.009596824645996,
      "learning_rate": 0.00018472361809045227,
      "loss": 1.0756,
      "step": 81
    },
    {
      "epoch": 0.6804979253112033,
      "grad_norm": 0.907459557056427,
      "learning_rate": 0.00018452261306532662,
      "loss": 1.094,
      "step": 82
    },
    {
      "epoch": 0.6887966804979253,
      "grad_norm": 0.8837058544158936,
      "learning_rate": 0.00018432160804020102,
      "loss": 1.1637,
      "step": 83
    },
    {
      "epoch": 0.6970954356846473,
      "grad_norm": 0.917782723903656,
      "learning_rate": 0.0001841206030150754,
      "loss": 1.1039,
      "step": 84
    },
    {
      "epoch": 0.7053941908713693,
      "grad_norm": 0.8537139892578125,
      "learning_rate": 0.00018391959798994977,
      "loss": 1.1867,
      "step": 85
    },
    {
      "epoch": 0.7136929460580913,
      "grad_norm": 0.8984200358390808,
      "learning_rate": 0.0001837185929648241,
      "loss": 1.0321,
      "step": 86
    },
    {
      "epoch": 0.7219917012448133,
      "grad_norm": 0.8227758407592773,
      "learning_rate": 0.0001835175879396985,
      "loss": 1.1252,
      "step": 87
    },
    {
      "epoch": 0.7302904564315352,
      "grad_norm": 0.8680870532989502,
      "learning_rate": 0.00018331658291457288,
      "loss": 1.1791,
      "step": 88
    },
    {
      "epoch": 0.7385892116182573,
      "grad_norm": 0.8905494213104248,
      "learning_rate": 0.00018311557788944723,
      "loss": 1.1316,
      "step": 89
    },
    {
      "epoch": 0.7468879668049793,
      "grad_norm": 0.8255711793899536,
      "learning_rate": 0.0001829145728643216,
      "loss": 0.8688,
      "step": 90
    },
    {
      "epoch": 0.7551867219917012,
      "grad_norm": 0.8046554327011108,
      "learning_rate": 0.000182713567839196,
      "loss": 1.0993,
      "step": 91
    },
    {
      "epoch": 0.7634854771784232,
      "grad_norm": 1.0828288793563843,
      "learning_rate": 0.00018251256281407038,
      "loss": 1.0765,
      "step": 92
    },
    {
      "epoch": 0.7717842323651453,
      "grad_norm": 0.7832619547843933,
      "learning_rate": 0.00018231155778894472,
      "loss": 1.0401,
      "step": 93
    },
    {
      "epoch": 0.7800829875518672,
      "grad_norm": 0.9080089330673218,
      "learning_rate": 0.0001821105527638191,
      "loss": 1.0679,
      "step": 94
    },
    {
      "epoch": 0.7883817427385892,
      "grad_norm": 0.8765209913253784,
      "learning_rate": 0.0001819095477386935,
      "loss": 0.9614,
      "step": 95
    },
    {
      "epoch": 0.7966804979253111,
      "grad_norm": 0.927598774433136,
      "learning_rate": 0.00018170854271356784,
      "loss": 1.021,
      "step": 96
    },
    {
      "epoch": 0.8049792531120332,
      "grad_norm": 0.8881963491439819,
      "learning_rate": 0.00018150753768844221,
      "loss": 1.0176,
      "step": 97
    },
    {
      "epoch": 0.8132780082987552,
      "grad_norm": 0.9378779530525208,
      "learning_rate": 0.0001813065326633166,
      "loss": 1.1495,
      "step": 98
    },
    {
      "epoch": 0.8215767634854771,
      "grad_norm": 1.0039005279541016,
      "learning_rate": 0.00018110552763819096,
      "loss": 1.1909,
      "step": 99
    },
    {
      "epoch": 0.8298755186721992,
      "grad_norm": 0.8454744815826416,
      "learning_rate": 0.00018090452261306533,
      "loss": 0.8871,
      "step": 100
    },
    {
      "epoch": 0.8381742738589212,
      "grad_norm": 1.0230399370193481,
      "learning_rate": 0.0001807035175879397,
      "loss": 1.2223,
      "step": 101
    },
    {
      "epoch": 0.8464730290456431,
      "grad_norm": 1.0655370950698853,
      "learning_rate": 0.00018050251256281408,
      "loss": 1.1749,
      "step": 102
    },
    {
      "epoch": 0.8547717842323651,
      "grad_norm": 0.8916755318641663,
      "learning_rate": 0.00018030150753768845,
      "loss": 1.0435,
      "step": 103
    },
    {
      "epoch": 0.8630705394190872,
      "grad_norm": 0.935066282749176,
      "learning_rate": 0.00018010050251256282,
      "loss": 1.2371,
      "step": 104
    },
    {
      "epoch": 0.8713692946058091,
      "grad_norm": 0.8592977523803711,
      "learning_rate": 0.0001798994974874372,
      "loss": 1.0313,
      "step": 105
    },
    {
      "epoch": 0.8796680497925311,
      "grad_norm": 0.8850722908973694,
      "learning_rate": 0.00017969849246231157,
      "loss": 1.2017,
      "step": 106
    },
    {
      "epoch": 0.8879668049792531,
      "grad_norm": 0.8350237011909485,
      "learning_rate": 0.00017949748743718592,
      "loss": 0.9661,
      "step": 107
    },
    {
      "epoch": 0.8962655601659751,
      "grad_norm": 0.8727333545684814,
      "learning_rate": 0.00017929648241206032,
      "loss": 1.1939,
      "step": 108
    },
    {
      "epoch": 0.9045643153526971,
      "grad_norm": 0.8180919289588928,
      "learning_rate": 0.0001790954773869347,
      "loss": 0.9684,
      "step": 109
    },
    {
      "epoch": 0.9128630705394191,
      "grad_norm": 0.96207195520401,
      "learning_rate": 0.00017889447236180906,
      "loss": 1.033,
      "step": 110
    },
    {
      "epoch": 0.921161825726141,
      "grad_norm": 0.8653010129928589,
      "learning_rate": 0.0001786934673366834,
      "loss": 1.1068,
      "step": 111
    },
    {
      "epoch": 0.9294605809128631,
      "grad_norm": 0.8511499166488647,
      "learning_rate": 0.0001784924623115578,
      "loss": 0.9369,
      "step": 112
    },
    {
      "epoch": 0.9377593360995851,
      "grad_norm": 0.9456340670585632,
      "learning_rate": 0.00017829145728643218,
      "loss": 1.1872,
      "step": 113
    },
    {
      "epoch": 0.946058091286307,
      "grad_norm": 0.8849169015884399,
      "learning_rate": 0.00017809045226130653,
      "loss": 0.9248,
      "step": 114
    },
    {
      "epoch": 0.9543568464730291,
      "grad_norm": 0.8799336552619934,
      "learning_rate": 0.0001778894472361809,
      "loss": 1.0893,
      "step": 115
    },
    {
      "epoch": 0.9626556016597511,
      "grad_norm": 0.8502377867698669,
      "learning_rate": 0.0001776884422110553,
      "loss": 1.0415,
      "step": 116
    },
    {
      "epoch": 0.970954356846473,
      "grad_norm": 0.9667127728462219,
      "learning_rate": 0.00017748743718592967,
      "loss": 1.162,
      "step": 117
    },
    {
      "epoch": 0.979253112033195,
      "grad_norm": 0.8258129954338074,
      "learning_rate": 0.00017728643216080402,
      "loss": 1.1026,
      "step": 118
    },
    {
      "epoch": 0.9875518672199171,
      "grad_norm": 0.866804301738739,
      "learning_rate": 0.0001770854271356784,
      "loss": 0.934,
      "step": 119
    },
    {
      "epoch": 0.995850622406639,
      "grad_norm": 0.9576835632324219,
      "learning_rate": 0.0001768844221105528,
      "loss": 1.1231,
      "step": 120
    },
    {
      "epoch": 1.004149377593361,
      "grad_norm": 0.9966908693313599,
      "learning_rate": 0.00017668341708542714,
      "loss": 0.8512,
      "step": 121
    },
    {
      "epoch": 1.012448132780083,
      "grad_norm": 0.766650915145874,
      "learning_rate": 0.0001764824120603015,
      "loss": 0.8274,
      "step": 122
    },
    {
      "epoch": 1.020746887966805,
      "grad_norm": 0.8157761096954346,
      "learning_rate": 0.00017628140703517588,
      "loss": 0.8751,
      "step": 123
    },
    {
      "epoch": 1.0290456431535269,
      "grad_norm": 0.8358221054077148,
      "learning_rate": 0.00017608040201005026,
      "loss": 0.7612,
      "step": 124
    },
    {
      "epoch": 1.037344398340249,
      "grad_norm": 0.8332679271697998,
      "learning_rate": 0.00017587939698492463,
      "loss": 0.9263,
      "step": 125
    },
    {
      "epoch": 1.045643153526971,
      "grad_norm": 0.8390955924987793,
      "learning_rate": 0.000175678391959799,
      "loss": 0.8524,
      "step": 126
    },
    {
      "epoch": 1.053941908713693,
      "grad_norm": 0.8618723750114441,
      "learning_rate": 0.00017547738693467338,
      "loss": 0.841,
      "step": 127
    },
    {
      "epoch": 1.062240663900415,
      "grad_norm": 0.8663930892944336,
      "learning_rate": 0.00017527638190954775,
      "loss": 0.7894,
      "step": 128
    },
    {
      "epoch": 1.070539419087137,
      "grad_norm": 0.9730732440948486,
      "learning_rate": 0.00017507537688442212,
      "loss": 0.8326,
      "step": 129
    },
    {
      "epoch": 1.0788381742738589,
      "grad_norm": 0.9107650518417358,
      "learning_rate": 0.0001748743718592965,
      "loss": 0.8185,
      "step": 130
    },
    {
      "epoch": 1.0871369294605808,
      "grad_norm": 0.9191886782646179,
      "learning_rate": 0.00017467336683417087,
      "loss": 0.7931,
      "step": 131
    },
    {
      "epoch": 1.095435684647303,
      "grad_norm": 0.9611661434173584,
      "learning_rate": 0.00017447236180904521,
      "loss": 0.7425,
      "step": 132
    },
    {
      "epoch": 1.103734439834025,
      "grad_norm": 1.0332282781600952,
      "learning_rate": 0.00017427135678391961,
      "loss": 0.902,
      "step": 133
    },
    {
      "epoch": 1.112033195020747,
      "grad_norm": 1.001977562904358,
      "learning_rate": 0.000174070351758794,
      "loss": 0.7224,
      "step": 134
    },
    {
      "epoch": 1.120331950207469,
      "grad_norm": 1.2880430221557617,
      "learning_rate": 0.00017386934673366836,
      "loss": 0.8492,
      "step": 135
    },
    {
      "epoch": 1.1286307053941909,
      "grad_norm": 1.0940419435501099,
      "learning_rate": 0.0001736683417085427,
      "loss": 0.8821,
      "step": 136
    },
    {
      "epoch": 1.1369294605809128,
      "grad_norm": 1.0259041786193848,
      "learning_rate": 0.0001734673366834171,
      "loss": 0.773,
      "step": 137
    },
    {
      "epoch": 1.1452282157676348,
      "grad_norm": 1.1189830303192139,
      "learning_rate": 0.00017326633165829148,
      "loss": 0.9387,
      "step": 138
    },
    {
      "epoch": 1.1535269709543567,
      "grad_norm": 0.9735915064811707,
      "learning_rate": 0.00017306532663316582,
      "loss": 0.7305,
      "step": 139
    },
    {
      "epoch": 1.161825726141079,
      "grad_norm": 1.1672183275222778,
      "learning_rate": 0.0001728643216080402,
      "loss": 0.93,
      "step": 140
    },
    {
      "epoch": 1.170124481327801,
      "grad_norm": 0.991202712059021,
      "learning_rate": 0.0001726633165829146,
      "loss": 0.7077,
      "step": 141
    },
    {
      "epoch": 1.1784232365145229,
      "grad_norm": 1.1231645345687866,
      "learning_rate": 0.00017246231155778897,
      "loss": 0.8509,
      "step": 142
    },
    {
      "epoch": 1.1867219917012448,
      "grad_norm": 1.166212558746338,
      "learning_rate": 0.00017226130653266332,
      "loss": 0.9317,
      "step": 143
    },
    {
      "epoch": 1.1950207468879668,
      "grad_norm": 0.9466010332107544,
      "learning_rate": 0.0001720603015075377,
      "loss": 0.6634,
      "step": 144
    },
    {
      "epoch": 1.2033195020746887,
      "grad_norm": 1.1704787015914917,
      "learning_rate": 0.00017185929648241206,
      "loss": 0.8615,
      "step": 145
    },
    {
      "epoch": 1.2116182572614107,
      "grad_norm": 1.0647733211517334,
      "learning_rate": 0.00017165829145728644,
      "loss": 0.7379,
      "step": 146
    },
    {
      "epoch": 1.2199170124481329,
      "grad_norm": 1.0223045349121094,
      "learning_rate": 0.0001714572864321608,
      "loss": 0.7575,
      "step": 147
    },
    {
      "epoch": 1.2282157676348548,
      "grad_norm": 1.0791923999786377,
      "learning_rate": 0.00017125628140703518,
      "loss": 0.8638,
      "step": 148
    },
    {
      "epoch": 1.2365145228215768,
      "grad_norm": 1.1411551237106323,
      "learning_rate": 0.00017105527638190955,
      "loss": 0.8712,
      "step": 149
    },
    {
      "epoch": 1.2448132780082988,
      "grad_norm": 1.2744802236557007,
      "learning_rate": 0.00017085427135678393,
      "loss": 1.077,
      "step": 150
    },
    {
      "epoch": 1.2531120331950207,
      "grad_norm": 1.0772117376327515,
      "learning_rate": 0.0001706532663316583,
      "loss": 0.8626,
      "step": 151
    },
    {
      "epoch": 1.2614107883817427,
      "grad_norm": 1.1927402019500732,
      "learning_rate": 0.00017045226130653267,
      "loss": 0.9746,
      "step": 152
    },
    {
      "epoch": 1.2697095435684647,
      "grad_norm": 1.0795263051986694,
      "learning_rate": 0.00017025125628140705,
      "loss": 0.7622,
      "step": 153
    },
    {
      "epoch": 1.2780082987551866,
      "grad_norm": 1.108771562576294,
      "learning_rate": 0.00017005025125628142,
      "loss": 0.8431,
      "step": 154
    },
    {
      "epoch": 1.2863070539419086,
      "grad_norm": 0.962293267250061,
      "learning_rate": 0.0001698492462311558,
      "loss": 0.744,
      "step": 155
    },
    {
      "epoch": 1.2946058091286308,
      "grad_norm": 1.1309700012207031,
      "learning_rate": 0.00016964824120603016,
      "loss": 0.7943,
      "step": 156
    },
    {
      "epoch": 1.3029045643153527,
      "grad_norm": 0.9431834816932678,
      "learning_rate": 0.0001694472361809045,
      "loss": 0.7365,
      "step": 157
    },
    {
      "epoch": 1.3112033195020747,
      "grad_norm": 1.0529766082763672,
      "learning_rate": 0.0001692462311557789,
      "loss": 0.7294,
      "step": 158
    },
    {
      "epoch": 1.3195020746887967,
      "grad_norm": 1.1238313913345337,
      "learning_rate": 0.00016904522613065328,
      "loss": 0.8127,
      "step": 159
    },
    {
      "epoch": 1.3278008298755186,
      "grad_norm": 1.1808702945709229,
      "learning_rate": 0.00016884422110552766,
      "loss": 0.7708,
      "step": 160
    },
    {
      "epoch": 1.3360995850622408,
      "grad_norm": 1.0923155546188354,
      "learning_rate": 0.000168643216080402,
      "loss": 0.6723,
      "step": 161
    },
    {
      "epoch": 1.3443983402489628,
      "grad_norm": 1.1982401609420776,
      "learning_rate": 0.0001684422110552764,
      "loss": 0.7671,
      "step": 162
    },
    {
      "epoch": 1.3526970954356847,
      "grad_norm": 1.076532006263733,
      "learning_rate": 0.00016824120603015078,
      "loss": 0.7768,
      "step": 163
    },
    {
      "epoch": 1.3609958506224067,
      "grad_norm": 1.0442031621932983,
      "learning_rate": 0.00016804020100502512,
      "loss": 0.6618,
      "step": 164
    },
    {
      "epoch": 1.3692946058091287,
      "grad_norm": 1.1625006198883057,
      "learning_rate": 0.0001678391959798995,
      "loss": 0.9103,
      "step": 165
    },
    {
      "epoch": 1.3775933609958506,
      "grad_norm": 1.1231801509857178,
      "learning_rate": 0.0001676381909547739,
      "loss": 0.8194,
      "step": 166
    },
    {
      "epoch": 1.3858921161825726,
      "grad_norm": 1.0368504524230957,
      "learning_rate": 0.00016743718592964827,
      "loss": 0.6221,
      "step": 167
    },
    {
      "epoch": 1.3941908713692945,
      "grad_norm": 1.316107153892517,
      "learning_rate": 0.0001672361809045226,
      "loss": 0.8604,
      "step": 168
    },
    {
      "epoch": 1.4024896265560165,
      "grad_norm": 1.205311894416809,
      "learning_rate": 0.00016703517587939699,
      "loss": 0.821,
      "step": 169
    },
    {
      "epoch": 1.4107883817427385,
      "grad_norm": 1.173353910446167,
      "learning_rate": 0.00016683417085427136,
      "loss": 0.8628,
      "step": 170
    },
    {
      "epoch": 1.4190871369294606,
      "grad_norm": 1.1120721101760864,
      "learning_rate": 0.00016663316582914573,
      "loss": 0.6534,
      "step": 171
    },
    {
      "epoch": 1.4273858921161826,
      "grad_norm": 1.1444933414459229,
      "learning_rate": 0.0001664321608040201,
      "loss": 0.8155,
      "step": 172
    },
    {
      "epoch": 1.4356846473029046,
      "grad_norm": 1.2159368991851807,
      "learning_rate": 0.00016623115577889448,
      "loss": 0.9404,
      "step": 173
    },
    {
      "epoch": 1.4439834024896265,
      "grad_norm": 1.1341862678527832,
      "learning_rate": 0.00016603015075376885,
      "loss": 0.7995,
      "step": 174
    },
    {
      "epoch": 1.4522821576763485,
      "grad_norm": 1.097231149673462,
      "learning_rate": 0.00016582914572864322,
      "loss": 0.8662,
      "step": 175
    },
    {
      "epoch": 1.4605809128630705,
      "grad_norm": 1.0552012920379639,
      "learning_rate": 0.0001656281407035176,
      "loss": 0.736,
      "step": 176
    },
    {
      "epoch": 1.4688796680497926,
      "grad_norm": 1.0139731168746948,
      "learning_rate": 0.00016542713567839197,
      "loss": 0.7086,
      "step": 177
    },
    {
      "epoch": 1.4771784232365146,
      "grad_norm": 1.1557316780090332,
      "learning_rate": 0.00016522613065326634,
      "loss": 0.7486,
      "step": 178
    },
    {
      "epoch": 1.4854771784232366,
      "grad_norm": 1.1118431091308594,
      "learning_rate": 0.00016502512562814072,
      "loss": 0.897,
      "step": 179
    },
    {
      "epoch": 1.4937759336099585,
      "grad_norm": 1.0895923376083374,
      "learning_rate": 0.0001648241206030151,
      "loss": 0.7315,
      "step": 180
    },
    {
      "epoch": 1.5020746887966805,
      "grad_norm": 1.0663847923278809,
      "learning_rate": 0.00016462311557788946,
      "loss": 0.7519,
      "step": 181
    },
    {
      "epoch": 1.5103734439834025,
      "grad_norm": 1.1324940919876099,
      "learning_rate": 0.0001644221105527638,
      "loss": 0.7563,
      "step": 182
    },
    {
      "epoch": 1.5186721991701244,
      "grad_norm": 1.2150053977966309,
      "learning_rate": 0.0001642211055276382,
      "loss": 0.8168,
      "step": 183
    },
    {
      "epoch": 1.5269709543568464,
      "grad_norm": 1.1770274639129639,
      "learning_rate": 0.00016402010050251258,
      "loss": 0.7233,
      "step": 184
    },
    {
      "epoch": 1.5352697095435683,
      "grad_norm": 1.0841350555419922,
      "learning_rate": 0.00016381909547738695,
      "loss": 0.68,
      "step": 185
    },
    {
      "epoch": 1.5435684647302903,
      "grad_norm": 1.1049998998641968,
      "learning_rate": 0.0001636180904522613,
      "loss": 0.7211,
      "step": 186
    },
    {
      "epoch": 1.5518672199170125,
      "grad_norm": 1.205085039138794,
      "learning_rate": 0.0001634170854271357,
      "loss": 0.8381,
      "step": 187
    },
    {
      "epoch": 1.5601659751037344,
      "grad_norm": 1.0672179460525513,
      "learning_rate": 0.00016321608040201007,
      "loss": 0.6602,
      "step": 188
    },
    {
      "epoch": 1.5684647302904564,
      "grad_norm": 1.139390468597412,
      "learning_rate": 0.00016301507537688442,
      "loss": 0.754,
      "step": 189
    },
    {
      "epoch": 1.5767634854771784,
      "grad_norm": 1.1813709735870361,
      "learning_rate": 0.0001628140703517588,
      "loss": 0.7663,
      "step": 190
    },
    {
      "epoch": 1.5850622406639006,
      "grad_norm": 1.23631751537323,
      "learning_rate": 0.00016261306532663316,
      "loss": 0.8166,
      "step": 191
    },
    {
      "epoch": 1.5933609958506225,
      "grad_norm": 1.1930744647979736,
      "learning_rate": 0.00016241206030150756,
      "loss": 0.9419,
      "step": 192
    },
    {
      "epoch": 1.6016597510373445,
      "grad_norm": 1.3094475269317627,
      "learning_rate": 0.0001622110552763819,
      "loss": 0.9255,
      "step": 193
    },
    {
      "epoch": 1.6099585062240664,
      "grad_norm": 1.2743759155273438,
      "learning_rate": 0.00016201005025125628,
      "loss": 1.0056,
      "step": 194
    },
    {
      "epoch": 1.6182572614107884,
      "grad_norm": 1.1439329385757446,
      "learning_rate": 0.00016180904522613066,
      "loss": 0.8781,
      "step": 195
    },
    {
      "epoch": 1.6265560165975104,
      "grad_norm": 1.110429286956787,
      "learning_rate": 0.00016160804020100503,
      "loss": 0.7872,
      "step": 196
    },
    {
      "epoch": 1.6348547717842323,
      "grad_norm": 1.1604655981063843,
      "learning_rate": 0.0001614070351758794,
      "loss": 0.8802,
      "step": 197
    },
    {
      "epoch": 1.6431535269709543,
      "grad_norm": 1.1259818077087402,
      "learning_rate": 0.00016120603015075378,
      "loss": 0.8,
      "step": 198
    },
    {
      "epoch": 1.6514522821576763,
      "grad_norm": 1.1615406274795532,
      "learning_rate": 0.00016100502512562815,
      "loss": 0.789,
      "step": 199
    },
    {
      "epoch": 1.6597510373443982,
      "grad_norm": 1.2093924283981323,
      "learning_rate": 0.00016080402010050252,
      "loss": 1.0213,
      "step": 200
    },
    {
      "epoch": 1.6680497925311202,
      "grad_norm": 1.0446913242340088,
      "learning_rate": 0.0001606030150753769,
      "loss": 0.7272,
      "step": 201
    },
    {
      "epoch": 1.6763485477178424,
      "grad_norm": 1.1881910562515259,
      "learning_rate": 0.00016040201005025127,
      "loss": 0.8217,
      "step": 202
    },
    {
      "epoch": 1.6846473029045643,
      "grad_norm": 1.139897346496582,
      "learning_rate": 0.00016020100502512564,
      "loss": 0.8207,
      "step": 203
    },
    {
      "epoch": 1.6929460580912863,
      "grad_norm": 1.0955438613891602,
      "learning_rate": 0.00016,
      "loss": 0.8115,
      "step": 204
    },
    {
      "epoch": 1.7012448132780082,
      "grad_norm": 1.1490460634231567,
      "learning_rate": 0.00015979899497487439,
      "loss": 0.9039,
      "step": 205
    },
    {
      "epoch": 1.7095435684647304,
      "grad_norm": 1.13961660861969,
      "learning_rate": 0.00015959798994974876,
      "loss": 0.7681,
      "step": 206
    },
    {
      "epoch": 1.7178423236514524,
      "grad_norm": 1.0851726531982422,
      "learning_rate": 0.0001593969849246231,
      "loss": 0.7862,
      "step": 207
    },
    {
      "epoch": 1.7261410788381744,
      "grad_norm": 1.1023656129837036,
      "learning_rate": 0.0001591959798994975,
      "loss": 0.6669,
      "step": 208
    },
    {
      "epoch": 1.7344398340248963,
      "grad_norm": 1.1414350271224976,
      "learning_rate": 0.00015899497487437188,
      "loss": 0.796,
      "step": 209
    },
    {
      "epoch": 1.7427385892116183,
      "grad_norm": 1.0385098457336426,
      "learning_rate": 0.00015879396984924625,
      "loss": 0.7034,
      "step": 210
    },
    {
      "epoch": 1.7510373443983402,
      "grad_norm": 1.0851752758026123,
      "learning_rate": 0.0001585929648241206,
      "loss": 0.7084,
      "step": 211
    },
    {
      "epoch": 1.7593360995850622,
      "grad_norm": 1.2279151678085327,
      "learning_rate": 0.000158391959798995,
      "loss": 0.8162,
      "step": 212
    },
    {
      "epoch": 1.7676348547717842,
      "grad_norm": 1.0834919214248657,
      "learning_rate": 0.00015819095477386937,
      "loss": 0.8092,
      "step": 213
    },
    {
      "epoch": 1.7759336099585061,
      "grad_norm": 1.202647089958191,
      "learning_rate": 0.00015798994974874372,
      "loss": 0.7616,
      "step": 214
    },
    {
      "epoch": 1.784232365145228,
      "grad_norm": 1.0954307317733765,
      "learning_rate": 0.0001577889447236181,
      "loss": 0.731,
      "step": 215
    },
    {
      "epoch": 1.79253112033195,
      "grad_norm": 1.210024356842041,
      "learning_rate": 0.00015758793969849246,
      "loss": 0.8165,
      "step": 216
    },
    {
      "epoch": 1.8008298755186722,
      "grad_norm": 1.2587181329727173,
      "learning_rate": 0.00015738693467336686,
      "loss": 0.7842,
      "step": 217
    },
    {
      "epoch": 1.8091286307053942,
      "grad_norm": 1.2736486196517944,
      "learning_rate": 0.0001571859296482412,
      "loss": 0.7744,
      "step": 218
    },
    {
      "epoch": 1.8174273858921162,
      "grad_norm": 1.0740529298782349,
      "learning_rate": 0.00015698492462311558,
      "loss": 0.7231,
      "step": 219
    },
    {
      "epoch": 1.8257261410788381,
      "grad_norm": 1.270075798034668,
      "learning_rate": 0.00015678391959798995,
      "loss": 0.8698,
      "step": 220
    },
    {
      "epoch": 1.8340248962655603,
      "grad_norm": 1.0416918992996216,
      "learning_rate": 0.00015658291457286433,
      "loss": 0.7478,
      "step": 221
    },
    {
      "epoch": 1.8423236514522823,
      "grad_norm": 1.2136261463165283,
      "learning_rate": 0.0001563819095477387,
      "loss": 0.8747,
      "step": 222
    },
    {
      "epoch": 1.8506224066390042,
      "grad_norm": 1.352434754371643,
      "learning_rate": 0.00015618090452261307,
      "loss": 0.8396,
      "step": 223
    },
    {
      "epoch": 1.8589211618257262,
      "grad_norm": 1.0699255466461182,
      "learning_rate": 0.00015597989949748745,
      "loss": 0.807,
      "step": 224
    },
    {
      "epoch": 1.8672199170124482,
      "grad_norm": 1.294288158416748,
      "learning_rate": 0.00015577889447236182,
      "loss": 0.8846,
      "step": 225
    },
    {
      "epoch": 1.8755186721991701,
      "grad_norm": 1.235807180404663,
      "learning_rate": 0.0001555778894472362,
      "loss": 0.7925,
      "step": 226
    },
    {
      "epoch": 1.883817427385892,
      "grad_norm": 1.2441710233688354,
      "learning_rate": 0.00015537688442211056,
      "loss": 0.8438,
      "step": 227
    },
    {
      "epoch": 1.892116182572614,
      "grad_norm": 1.0660206079483032,
      "learning_rate": 0.00015517587939698494,
      "loss": 0.642,
      "step": 228
    },
    {
      "epoch": 1.900414937759336,
      "grad_norm": 1.0658175945281982,
      "learning_rate": 0.0001549748743718593,
      "loss": 0.6239,
      "step": 229
    },
    {
      "epoch": 1.908713692946058,
      "grad_norm": 1.2978029251098633,
      "learning_rate": 0.00015477386934673368,
      "loss": 0.8851,
      "step": 230
    },
    {
      "epoch": 1.91701244813278,
      "grad_norm": 1.2388139963150024,
      "learning_rate": 0.00015457286432160806,
      "loss": 0.7939,
      "step": 231
    },
    {
      "epoch": 1.9253112033195021,
      "grad_norm": 1.0918916463851929,
      "learning_rate": 0.0001543718592964824,
      "loss": 0.7042,
      "step": 232
    },
    {
      "epoch": 1.933609958506224,
      "grad_norm": 1.1465984582901,
      "learning_rate": 0.0001541708542713568,
      "loss": 0.8131,
      "step": 233
    },
    {
      "epoch": 1.941908713692946,
      "grad_norm": 1.1936181783676147,
      "learning_rate": 0.00015396984924623117,
      "loss": 0.9302,
      "step": 234
    },
    {
      "epoch": 1.950207468879668,
      "grad_norm": 1.1323003768920898,
      "learning_rate": 0.00015376884422110555,
      "loss": 0.7874,
      "step": 235
    },
    {
      "epoch": 1.9585062240663902,
      "grad_norm": 1.1818000078201294,
      "learning_rate": 0.0001535678391959799,
      "loss": 0.8504,
      "step": 236
    },
    {
      "epoch": 1.9668049792531122,
      "grad_norm": 1.0579715967178345,
      "learning_rate": 0.00015336683417085427,
      "loss": 0.6439,
      "step": 237
    },
    {
      "epoch": 1.9751037344398341,
      "grad_norm": 1.0311403274536133,
      "learning_rate": 0.00015316582914572867,
      "loss": 0.7426,
      "step": 238
    },
    {
      "epoch": 1.983402489626556,
      "grad_norm": 1.125788927078247,
      "learning_rate": 0.000152964824120603,
      "loss": 0.88,
      "step": 239
    },
    {
      "epoch": 1.991701244813278,
      "grad_norm": 1.28829026222229,
      "learning_rate": 0.00015276381909547739,
      "loss": 0.771,
      "step": 240
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3153051137924194,
      "learning_rate": 0.00015256281407035176,
      "loss": 0.8033,
      "step": 241
    },
    {
      "epoch": 2.008298755186722,
      "grad_norm": 1.0306272506713867,
      "learning_rate": 0.00015236180904522613,
      "loss": 0.5551,
      "step": 242
    },
    {
      "epoch": 2.016597510373444,
      "grad_norm": 0.9033105969429016,
      "learning_rate": 0.0001521608040201005,
      "loss": 0.4459,
      "step": 243
    },
    {
      "epoch": 2.024896265560166,
      "grad_norm": 1.0479035377502441,
      "learning_rate": 0.00015195979899497488,
      "loss": 0.5417,
      "step": 244
    },
    {
      "epoch": 2.033195020746888,
      "grad_norm": 1.165921926498413,
      "learning_rate": 0.00015175879396984925,
      "loss": 0.6249,
      "step": 245
    },
    {
      "epoch": 2.04149377593361,
      "grad_norm": 1.1623784303665161,
      "learning_rate": 0.00015155778894472362,
      "loss": 0.6443,
      "step": 246
    },
    {
      "epoch": 2.0497925311203318,
      "grad_norm": 1.3475767374038696,
      "learning_rate": 0.000151356783919598,
      "loss": 0.6067,
      "step": 247
    },
    {
      "epoch": 2.0580912863070537,
      "grad_norm": 1.3195610046386719,
      "learning_rate": 0.00015115577889447237,
      "loss": 0.6229,
      "step": 248
    },
    {
      "epoch": 2.066390041493776,
      "grad_norm": 1.4343822002410889,
      "learning_rate": 0.00015095477386934674,
      "loss": 0.676,
      "step": 249
    },
    {
      "epoch": 2.074688796680498,
      "grad_norm": 1.41526460647583,
      "learning_rate": 0.00015075376884422112,
      "loss": 0.5436,
      "step": 250
    },
    {
      "epoch": 2.08298755186722,
      "grad_norm": 1.4628443717956543,
      "learning_rate": 0.0001505527638190955,
      "loss": 0.4251,
      "step": 251
    },
    {
      "epoch": 2.091286307053942,
      "grad_norm": 1.6013458967208862,
      "learning_rate": 0.00015035175879396986,
      "loss": 0.4609,
      "step": 252
    },
    {
      "epoch": 2.099585062240664,
      "grad_norm": 1.4982620477676392,
      "learning_rate": 0.00015015075376884423,
      "loss": 0.4682,
      "step": 253
    },
    {
      "epoch": 2.107883817427386,
      "grad_norm": 1.5559749603271484,
      "learning_rate": 0.0001499497487437186,
      "loss": 0.5458,
      "step": 254
    },
    {
      "epoch": 2.116182572614108,
      "grad_norm": 1.370057225227356,
      "learning_rate": 0.00014974874371859298,
      "loss": 0.3259,
      "step": 255
    },
    {
      "epoch": 2.12448132780083,
      "grad_norm": 1.626512885093689,
      "learning_rate": 0.00014954773869346735,
      "loss": 0.5596,
      "step": 256
    },
    {
      "epoch": 2.132780082987552,
      "grad_norm": 1.5761382579803467,
      "learning_rate": 0.0001493467336683417,
      "loss": 0.4569,
      "step": 257
    },
    {
      "epoch": 2.141078838174274,
      "grad_norm": 1.3972676992416382,
      "learning_rate": 0.0001491457286432161,
      "loss": 0.3561,
      "step": 258
    },
    {
      "epoch": 2.1493775933609958,
      "grad_norm": 1.452401041984558,
      "learning_rate": 0.00014894472361809047,
      "loss": 0.5342,
      "step": 259
    },
    {
      "epoch": 2.1576763485477177,
      "grad_norm": 1.2507885694503784,
      "learning_rate": 0.00014874371859296482,
      "loss": 0.4624,
      "step": 260
    },
    {
      "epoch": 2.1659751037344397,
      "grad_norm": 1.254610300064087,
      "learning_rate": 0.0001485427135678392,
      "loss": 0.3703,
      "step": 261
    },
    {
      "epoch": 2.1742738589211617,
      "grad_norm": 1.3569316864013672,
      "learning_rate": 0.00014834170854271356,
      "loss": 0.409,
      "step": 262
    },
    {
      "epoch": 2.1825726141078836,
      "grad_norm": 1.4537683725357056,
      "learning_rate": 0.00014814070351758796,
      "loss": 0.562,
      "step": 263
    },
    {
      "epoch": 2.190871369294606,
      "grad_norm": 1.4368224143981934,
      "learning_rate": 0.0001479396984924623,
      "loss": 0.558,
      "step": 264
    },
    {
      "epoch": 2.199170124481328,
      "grad_norm": 1.3906646966934204,
      "learning_rate": 0.00014773869346733668,
      "loss": 0.61,
      "step": 265
    },
    {
      "epoch": 2.20746887966805,
      "grad_norm": 1.345909833908081,
      "learning_rate": 0.00014753768844221106,
      "loss": 0.476,
      "step": 266
    },
    {
      "epoch": 2.215767634854772,
      "grad_norm": 1.6240447759628296,
      "learning_rate": 0.00014733668341708543,
      "loss": 0.5487,
      "step": 267
    },
    {
      "epoch": 2.224066390041494,
      "grad_norm": 1.5580388307571411,
      "learning_rate": 0.0001471356783919598,
      "loss": 0.4983,
      "step": 268
    },
    {
      "epoch": 2.232365145228216,
      "grad_norm": 1.4720885753631592,
      "learning_rate": 0.00014693467336683417,
      "loss": 0.5069,
      "step": 269
    },
    {
      "epoch": 2.240663900414938,
      "grad_norm": 1.503630518913269,
      "learning_rate": 0.00014673366834170855,
      "loss": 0.4464,
      "step": 270
    },
    {
      "epoch": 2.2489626556016598,
      "grad_norm": 1.5194319486618042,
      "learning_rate": 0.00014653266331658292,
      "loss": 0.533,
      "step": 271
    },
    {
      "epoch": 2.2572614107883817,
      "grad_norm": 1.4581201076507568,
      "learning_rate": 0.0001463316582914573,
      "loss": 0.476,
      "step": 272
    },
    {
      "epoch": 2.2655601659751037,
      "grad_norm": 1.3086541891098022,
      "learning_rate": 0.00014613065326633167,
      "loss": 0.5336,
      "step": 273
    },
    {
      "epoch": 2.2738589211618256,
      "grad_norm": 1.6226232051849365,
      "learning_rate": 0.00014592964824120604,
      "loss": 0.5103,
      "step": 274
    },
    {
      "epoch": 2.2821576763485476,
      "grad_norm": 1.4995474815368652,
      "learning_rate": 0.0001457286432160804,
      "loss": 0.5015,
      "step": 275
    },
    {
      "epoch": 2.2904564315352696,
      "grad_norm": 1.633615255355835,
      "learning_rate": 0.00014552763819095479,
      "loss": 0.5829,
      "step": 276
    },
    {
      "epoch": 2.2987551867219915,
      "grad_norm": 1.5225706100463867,
      "learning_rate": 0.00014532663316582916,
      "loss": 0.4864,
      "step": 277
    },
    {
      "epoch": 2.3070539419087135,
      "grad_norm": 1.5390501022338867,
      "learning_rate": 0.00014512562814070353,
      "loss": 0.5627,
      "step": 278
    },
    {
      "epoch": 2.3153526970954355,
      "grad_norm": 1.5574965476989746,
      "learning_rate": 0.0001449246231155779,
      "loss": 0.5721,
      "step": 279
    },
    {
      "epoch": 2.323651452282158,
      "grad_norm": 1.4507496356964111,
      "learning_rate": 0.00014472361809045228,
      "loss": 0.417,
      "step": 280
    },
    {
      "epoch": 2.33195020746888,
      "grad_norm": 1.6556905508041382,
      "learning_rate": 0.00014452261306532665,
      "loss": 0.6319,
      "step": 281
    },
    {
      "epoch": 2.340248962655602,
      "grad_norm": 1.269890546798706,
      "learning_rate": 0.000144321608040201,
      "loss": 0.5073,
      "step": 282
    },
    {
      "epoch": 2.3485477178423237,
      "grad_norm": 1.3045636415481567,
      "learning_rate": 0.00014412060301507537,
      "loss": 0.4349,
      "step": 283
    },
    {
      "epoch": 2.3568464730290457,
      "grad_norm": 1.5845695734024048,
      "learning_rate": 0.00014391959798994977,
      "loss": 0.4871,
      "step": 284
    },
    {
      "epoch": 2.3651452282157677,
      "grad_norm": 1.4645051956176758,
      "learning_rate": 0.00014371859296482411,
      "loss": 0.496,
      "step": 285
    },
    {
      "epoch": 2.3734439834024896,
      "grad_norm": 1.293988823890686,
      "learning_rate": 0.0001435175879396985,
      "loss": 0.4848,
      "step": 286
    },
    {
      "epoch": 2.3817427385892116,
      "grad_norm": 1.4349843263626099,
      "learning_rate": 0.00014331658291457286,
      "loss": 0.515,
      "step": 287
    },
    {
      "epoch": 2.3900414937759336,
      "grad_norm": 1.411733627319336,
      "learning_rate": 0.00014311557788944726,
      "loss": 0.5961,
      "step": 288
    },
    {
      "epoch": 2.3983402489626555,
      "grad_norm": 1.5085761547088623,
      "learning_rate": 0.0001429145728643216,
      "loss": 0.6686,
      "step": 289
    },
    {
      "epoch": 2.4066390041493775,
      "grad_norm": 1.5481432676315308,
      "learning_rate": 0.00014271356783919598,
      "loss": 0.5847,
      "step": 290
    },
    {
      "epoch": 2.4149377593360994,
      "grad_norm": 1.488020896911621,
      "learning_rate": 0.00014251256281407035,
      "loss": 0.559,
      "step": 291
    },
    {
      "epoch": 2.4232365145228214,
      "grad_norm": 1.1918635368347168,
      "learning_rate": 0.00014231155778894473,
      "loss": 0.4551,
      "step": 292
    },
    {
      "epoch": 2.431535269709544,
      "grad_norm": 1.418962836265564,
      "learning_rate": 0.0001421105527638191,
      "loss": 0.6567,
      "step": 293
    },
    {
      "epoch": 2.4398340248962658,
      "grad_norm": 1.4779499769210815,
      "learning_rate": 0.00014190954773869347,
      "loss": 0.5272,
      "step": 294
    },
    {
      "epoch": 2.4481327800829877,
      "grad_norm": 1.2152917385101318,
      "learning_rate": 0.00014170854271356784,
      "loss": 0.4129,
      "step": 295
    },
    {
      "epoch": 2.4564315352697097,
      "grad_norm": 1.3103164434432983,
      "learning_rate": 0.00014150753768844222,
      "loss": 0.434,
      "step": 296
    },
    {
      "epoch": 2.4647302904564317,
      "grad_norm": 1.5716071128845215,
      "learning_rate": 0.0001413065326633166,
      "loss": 0.6958,
      "step": 297
    },
    {
      "epoch": 2.4730290456431536,
      "grad_norm": 1.5472100973129272,
      "learning_rate": 0.00014110552763819096,
      "loss": 0.5077,
      "step": 298
    },
    {
      "epoch": 2.4813278008298756,
      "grad_norm": 1.6116987466812134,
      "learning_rate": 0.00014090452261306534,
      "loss": 0.561,
      "step": 299
    },
    {
      "epoch": 2.4896265560165975,
      "grad_norm": 1.7108062505722046,
      "learning_rate": 0.0001407035175879397,
      "loss": 0.5897,
      "step": 300
    },
    {
      "epoch": 2.4979253112033195,
      "grad_norm": 1.3344438076019287,
      "learning_rate": 0.00014050251256281408,
      "loss": 0.3637,
      "step": 301
    },
    {
      "epoch": 2.5062240663900415,
      "grad_norm": 1.6170378923416138,
      "learning_rate": 0.00014030150753768846,
      "loss": 0.6306,
      "step": 302
    },
    {
      "epoch": 2.5145228215767634,
      "grad_norm": 1.3905774354934692,
      "learning_rate": 0.0001401005025125628,
      "loss": 0.3971,
      "step": 303
    },
    {
      "epoch": 2.5228215767634854,
      "grad_norm": 1.2503491640090942,
      "learning_rate": 0.0001398994974874372,
      "loss": 0.3916,
      "step": 304
    },
    {
      "epoch": 2.5311203319502074,
      "grad_norm": 1.4203201532363892,
      "learning_rate": 0.00013969849246231157,
      "loss": 0.5421,
      "step": 305
    },
    {
      "epoch": 2.5394190871369293,
      "grad_norm": 1.5038259029388428,
      "learning_rate": 0.00013949748743718595,
      "loss": 0.4543,
      "step": 306
    },
    {
      "epoch": 2.5477178423236513,
      "grad_norm": 1.6125001907348633,
      "learning_rate": 0.0001392964824120603,
      "loss": 0.6194,
      "step": 307
    },
    {
      "epoch": 2.5560165975103732,
      "grad_norm": 1.4636293649673462,
      "learning_rate": 0.00013909547738693467,
      "loss": 0.4445,
      "step": 308
    },
    {
      "epoch": 2.564315352697095,
      "grad_norm": 1.5559983253479004,
      "learning_rate": 0.00013889447236180907,
      "loss": 0.5097,
      "step": 309
    },
    {
      "epoch": 2.572614107883817,
      "grad_norm": 1.5436317920684814,
      "learning_rate": 0.0001386934673366834,
      "loss": 0.4196,
      "step": 310
    },
    {
      "epoch": 2.5809128630705396,
      "grad_norm": 1.5327664613723755,
      "learning_rate": 0.00013849246231155778,
      "loss": 0.4551,
      "step": 311
    },
    {
      "epoch": 2.5892116182572615,
      "grad_norm": 1.5487794876098633,
      "learning_rate": 0.00013829145728643216,
      "loss": 0.4719,
      "step": 312
    },
    {
      "epoch": 2.5975103734439835,
      "grad_norm": 1.386175513267517,
      "learning_rate": 0.00013809045226130656,
      "loss": 0.5179,
      "step": 313
    },
    {
      "epoch": 2.6058091286307055,
      "grad_norm": 1.5780686140060425,
      "learning_rate": 0.0001378894472361809,
      "loss": 0.5392,
      "step": 314
    },
    {
      "epoch": 2.6141078838174274,
      "grad_norm": 1.3493915796279907,
      "learning_rate": 0.00013768844221105528,
      "loss": 0.4789,
      "step": 315
    },
    {
      "epoch": 2.6224066390041494,
      "grad_norm": 1.3132704496383667,
      "learning_rate": 0.00013748743718592965,
      "loss": 0.475,
      "step": 316
    },
    {
      "epoch": 2.6307053941908713,
      "grad_norm": 1.408057451248169,
      "learning_rate": 0.00013728643216080402,
      "loss": 0.5086,
      "step": 317
    },
    {
      "epoch": 2.6390041493775933,
      "grad_norm": 1.4688434600830078,
      "learning_rate": 0.0001370854271356784,
      "loss": 0.5581,
      "step": 318
    },
    {
      "epoch": 2.6473029045643153,
      "grad_norm": 1.629462480545044,
      "learning_rate": 0.00013688442211055277,
      "loss": 0.5956,
      "step": 319
    },
    {
      "epoch": 2.6556016597510372,
      "grad_norm": 1.3723214864730835,
      "learning_rate": 0.00013668341708542714,
      "loss": 0.4978,
      "step": 320
    },
    {
      "epoch": 2.663900414937759,
      "grad_norm": 1.6624574661254883,
      "learning_rate": 0.00013648241206030151,
      "loss": 0.6127,
      "step": 321
    },
    {
      "epoch": 2.6721991701244816,
      "grad_norm": 1.589137077331543,
      "learning_rate": 0.0001362814070351759,
      "loss": 0.5504,
      "step": 322
    },
    {
      "epoch": 2.6804979253112036,
      "grad_norm": 1.6160728931427002,
      "learning_rate": 0.00013608040201005026,
      "loss": 0.4938,
      "step": 323
    },
    {
      "epoch": 2.6887966804979255,
      "grad_norm": 1.4274375438690186,
      "learning_rate": 0.00013587939698492463,
      "loss": 0.5306,
      "step": 324
    },
    {
      "epoch": 2.6970954356846475,
      "grad_norm": 1.4162631034851074,
      "learning_rate": 0.000135678391959799,
      "loss": 0.4896,
      "step": 325
    },
    {
      "epoch": 2.7053941908713695,
      "grad_norm": 1.3767859935760498,
      "learning_rate": 0.00013547738693467338,
      "loss": 0.5601,
      "step": 326
    },
    {
      "epoch": 2.7136929460580914,
      "grad_norm": 1.6843544244766235,
      "learning_rate": 0.00013527638190954775,
      "loss": 0.6061,
      "step": 327
    },
    {
      "epoch": 2.7219917012448134,
      "grad_norm": 1.3982195854187012,
      "learning_rate": 0.0001350753768844221,
      "loss": 0.5964,
      "step": 328
    },
    {
      "epoch": 2.7302904564315353,
      "grad_norm": 1.6013236045837402,
      "learning_rate": 0.00013487437185929647,
      "loss": 0.573,
      "step": 329
    },
    {
      "epoch": 2.7385892116182573,
      "grad_norm": 1.512844204902649,
      "learning_rate": 0.00013467336683417087,
      "loss": 0.6266,
      "step": 330
    },
    {
      "epoch": 2.7468879668049793,
      "grad_norm": 1.2383170127868652,
      "learning_rate": 0.00013447236180904524,
      "loss": 0.421,
      "step": 331
    },
    {
      "epoch": 2.7551867219917012,
      "grad_norm": 1.1637558937072754,
      "learning_rate": 0.0001342713567839196,
      "loss": 0.3855,
      "step": 332
    },
    {
      "epoch": 2.763485477178423,
      "grad_norm": 1.364800214767456,
      "learning_rate": 0.00013407035175879396,
      "loss": 0.5157,
      "step": 333
    },
    {
      "epoch": 2.771784232365145,
      "grad_norm": 1.525434970855713,
      "learning_rate": 0.00013386934673366836,
      "loss": 0.6112,
      "step": 334
    },
    {
      "epoch": 2.780082987551867,
      "grad_norm": 1.3496448993682861,
      "learning_rate": 0.0001336683417085427,
      "loss": 0.5108,
      "step": 335
    },
    {
      "epoch": 2.788381742738589,
      "grad_norm": 1.296197772026062,
      "learning_rate": 0.00013346733668341708,
      "loss": 0.5022,
      "step": 336
    },
    {
      "epoch": 2.796680497925311,
      "grad_norm": 1.3150502443313599,
      "learning_rate": 0.00013326633165829146,
      "loss": 0.4493,
      "step": 337
    },
    {
      "epoch": 2.804979253112033,
      "grad_norm": 1.3933988809585571,
      "learning_rate": 0.00013306532663316586,
      "loss": 0.5702,
      "step": 338
    },
    {
      "epoch": 2.813278008298755,
      "grad_norm": 1.4046320915222168,
      "learning_rate": 0.0001328643216080402,
      "loss": 0.5101,
      "step": 339
    },
    {
      "epoch": 2.821576763485477,
      "grad_norm": 1.4432249069213867,
      "learning_rate": 0.00013266331658291457,
      "loss": 0.6391,
      "step": 340
    },
    {
      "epoch": 2.8298755186721993,
      "grad_norm": 1.4560797214508057,
      "learning_rate": 0.00013246231155778895,
      "loss": 0.5309,
      "step": 341
    },
    {
      "epoch": 2.8381742738589213,
      "grad_norm": 1.3300548791885376,
      "learning_rate": 0.00013226130653266332,
      "loss": 0.466,
      "step": 342
    },
    {
      "epoch": 2.8464730290456433,
      "grad_norm": 1.3274461030960083,
      "learning_rate": 0.0001320603015075377,
      "loss": 0.5081,
      "step": 343
    },
    {
      "epoch": 2.854771784232365,
      "grad_norm": 1.2527449131011963,
      "learning_rate": 0.00013185929648241207,
      "loss": 0.4719,
      "step": 344
    },
    {
      "epoch": 2.863070539419087,
      "grad_norm": 1.378798484802246,
      "learning_rate": 0.00013165829145728644,
      "loss": 0.4598,
      "step": 345
    },
    {
      "epoch": 2.871369294605809,
      "grad_norm": 1.4690757989883423,
      "learning_rate": 0.0001314572864321608,
      "loss": 0.5713,
      "step": 346
    },
    {
      "epoch": 2.879668049792531,
      "grad_norm": 1.4666279554367065,
      "learning_rate": 0.00013125628140703518,
      "loss": 0.5453,
      "step": 347
    },
    {
      "epoch": 2.887966804979253,
      "grad_norm": 1.6002241373062134,
      "learning_rate": 0.00013105527638190956,
      "loss": 0.5778,
      "step": 348
    },
    {
      "epoch": 2.896265560165975,
      "grad_norm": 1.3132392168045044,
      "learning_rate": 0.00013085427135678393,
      "loss": 0.3643,
      "step": 349
    },
    {
      "epoch": 2.904564315352697,
      "grad_norm": 1.668445348739624,
      "learning_rate": 0.0001306532663316583,
      "loss": 0.6205,
      "step": 350
    },
    {
      "epoch": 2.912863070539419,
      "grad_norm": 1.3509478569030762,
      "learning_rate": 0.00013045226130653268,
      "loss": 0.4044,
      "step": 351
    },
    {
      "epoch": 2.921161825726141,
      "grad_norm": 1.5193318128585815,
      "learning_rate": 0.00013025125628140705,
      "loss": 0.4695,
      "step": 352
    },
    {
      "epoch": 2.9294605809128633,
      "grad_norm": 1.5286747217178345,
      "learning_rate": 0.0001300502512562814,
      "loss": 0.5294,
      "step": 353
    },
    {
      "epoch": 2.9377593360995853,
      "grad_norm": 1.4287731647491455,
      "learning_rate": 0.00012984924623115577,
      "loss": 0.4803,
      "step": 354
    },
    {
      "epoch": 2.9460580912863072,
      "grad_norm": 1.3258037567138672,
      "learning_rate": 0.00012964824120603017,
      "loss": 0.438,
      "step": 355
    },
    {
      "epoch": 2.954356846473029,
      "grad_norm": 1.4105286598205566,
      "learning_rate": 0.00012944723618090454,
      "loss": 0.4822,
      "step": 356
    },
    {
      "epoch": 2.962655601659751,
      "grad_norm": 1.3815860748291016,
      "learning_rate": 0.0001292462311557789,
      "loss": 0.5421,
      "step": 357
    },
    {
      "epoch": 2.970954356846473,
      "grad_norm": 1.7275184392929077,
      "learning_rate": 0.00012904522613065326,
      "loss": 0.5488,
      "step": 358
    },
    {
      "epoch": 2.979253112033195,
      "grad_norm": 1.4605810642242432,
      "learning_rate": 0.00012884422110552766,
      "loss": 0.4857,
      "step": 359
    },
    {
      "epoch": 2.987551867219917,
      "grad_norm": 1.6668367385864258,
      "learning_rate": 0.000128643216080402,
      "loss": 0.6744,
      "step": 360
    },
    {
      "epoch": 2.995850622406639,
      "grad_norm": 1.6439107656478882,
      "learning_rate": 0.00012844221105527638,
      "loss": 0.6162,
      "step": 361
    },
    {
      "epoch": 3.004149377593361,
      "grad_norm": 1.2980343103408813,
      "learning_rate": 0.00012824120603015075,
      "loss": 0.3288,
      "step": 362
    },
    {
      "epoch": 3.012448132780083,
      "grad_norm": 0.9949775338172913,
      "learning_rate": 0.00012804020100502515,
      "loss": 0.2816,
      "step": 363
    },
    {
      "epoch": 3.020746887966805,
      "grad_norm": 1.3273799419403076,
      "learning_rate": 0.0001278391959798995,
      "loss": 0.3171,
      "step": 364
    },
    {
      "epoch": 3.029045643153527,
      "grad_norm": 1.3138936758041382,
      "learning_rate": 0.00012763819095477387,
      "loss": 0.343,
      "step": 365
    },
    {
      "epoch": 3.037344398340249,
      "grad_norm": 1.2748262882232666,
      "learning_rate": 0.00012743718592964824,
      "loss": 0.3094,
      "step": 366
    },
    {
      "epoch": 3.045643153526971,
      "grad_norm": 1.1729071140289307,
      "learning_rate": 0.00012723618090452262,
      "loss": 0.25,
      "step": 367
    },
    {
      "epoch": 3.0539419087136928,
      "grad_norm": 1.5705856084823608,
      "learning_rate": 0.000127035175879397,
      "loss": 0.3983,
      "step": 368
    },
    {
      "epoch": 3.0622406639004147,
      "grad_norm": 1.2656395435333252,
      "learning_rate": 0.00012683417085427136,
      "loss": 0.2342,
      "step": 369
    },
    {
      "epoch": 3.070539419087137,
      "grad_norm": 1.5403857231140137,
      "learning_rate": 0.00012663316582914574,
      "loss": 0.2225,
      "step": 370
    },
    {
      "epoch": 3.078838174273859,
      "grad_norm": 1.498780608177185,
      "learning_rate": 0.0001264321608040201,
      "loss": 0.2395,
      "step": 371
    },
    {
      "epoch": 3.087136929460581,
      "grad_norm": 2.152519941329956,
      "learning_rate": 0.00012623115577889448,
      "loss": 0.2919,
      "step": 372
    },
    {
      "epoch": 3.095435684647303,
      "grad_norm": 2.0457377433776855,
      "learning_rate": 0.00012603015075376885,
      "loss": 0.3276,
      "step": 373
    },
    {
      "epoch": 3.103734439834025,
      "grad_norm": 1.6457867622375488,
      "learning_rate": 0.00012582914572864323,
      "loss": 0.2551,
      "step": 374
    },
    {
      "epoch": 3.112033195020747,
      "grad_norm": 1.990527629852295,
      "learning_rate": 0.0001256281407035176,
      "loss": 0.3764,
      "step": 375
    },
    {
      "epoch": 3.120331950207469,
      "grad_norm": 2.1637842655181885,
      "learning_rate": 0.00012542713567839197,
      "loss": 0.2699,
      "step": 376
    },
    {
      "epoch": 3.128630705394191,
      "grad_norm": 1.9699734449386597,
      "learning_rate": 0.00012522613065326635,
      "loss": 0.2693,
      "step": 377
    },
    {
      "epoch": 3.136929460580913,
      "grad_norm": 2.0203089714050293,
      "learning_rate": 0.0001250251256281407,
      "loss": 0.3985,
      "step": 378
    },
    {
      "epoch": 3.145228215767635,
      "grad_norm": 1.7054259777069092,
      "learning_rate": 0.00012482412060301507,
      "loss": 0.3108,
      "step": 379
    },
    {
      "epoch": 3.1535269709543567,
      "grad_norm": 1.7709321975708008,
      "learning_rate": 0.00012462311557788947,
      "loss": 0.3441,
      "step": 380
    },
    {
      "epoch": 3.1618257261410787,
      "grad_norm": 1.743093490600586,
      "learning_rate": 0.00012442211055276384,
      "loss": 0.3682,
      "step": 381
    },
    {
      "epoch": 3.1701244813278007,
      "grad_norm": 1.4276049137115479,
      "learning_rate": 0.00012422110552763818,
      "loss": 0.261,
      "step": 382
    },
    {
      "epoch": 3.1784232365145226,
      "grad_norm": 1.4463350772857666,
      "learning_rate": 0.00012402010050251256,
      "loss": 0.2906,
      "step": 383
    },
    {
      "epoch": 3.186721991701245,
      "grad_norm": 1.373047113418579,
      "learning_rate": 0.00012381909547738696,
      "loss": 0.2677,
      "step": 384
    },
    {
      "epoch": 3.195020746887967,
      "grad_norm": 1.5100605487823486,
      "learning_rate": 0.0001236180904522613,
      "loss": 0.3099,
      "step": 385
    },
    {
      "epoch": 3.203319502074689,
      "grad_norm": 1.569165825843811,
      "learning_rate": 0.00012341708542713568,
      "loss": 0.3448,
      "step": 386
    },
    {
      "epoch": 3.211618257261411,
      "grad_norm": 1.6049097776412964,
      "learning_rate": 0.00012321608040201005,
      "loss": 0.2713,
      "step": 387
    },
    {
      "epoch": 3.219917012448133,
      "grad_norm": 1.851670265197754,
      "learning_rate": 0.00012301507537688445,
      "loss": 0.3652,
      "step": 388
    },
    {
      "epoch": 3.228215767634855,
      "grad_norm": 1.4777473211288452,
      "learning_rate": 0.0001228140703517588,
      "loss": 0.333,
      "step": 389
    },
    {
      "epoch": 3.236514522821577,
      "grad_norm": 1.3974393606185913,
      "learning_rate": 0.00012261306532663317,
      "loss": 0.2542,
      "step": 390
    },
    {
      "epoch": 3.2448132780082988,
      "grad_norm": 1.483158826828003,
      "learning_rate": 0.00012241206030150754,
      "loss": 0.2806,
      "step": 391
    },
    {
      "epoch": 3.2531120331950207,
      "grad_norm": 1.5087217092514038,
      "learning_rate": 0.00012221105527638191,
      "loss": 0.2946,
      "step": 392
    },
    {
      "epoch": 3.2614107883817427,
      "grad_norm": 1.6400272846221924,
      "learning_rate": 0.00012201005025125629,
      "loss": 0.3357,
      "step": 393
    },
    {
      "epoch": 3.2697095435684647,
      "grad_norm": 1.8245055675506592,
      "learning_rate": 0.00012180904522613066,
      "loss": 0.3752,
      "step": 394
    },
    {
      "epoch": 3.2780082987551866,
      "grad_norm": 1.3522634506225586,
      "learning_rate": 0.00012160804020100502,
      "loss": 0.2813,
      "step": 395
    },
    {
      "epoch": 3.2863070539419086,
      "grad_norm": 2.292672634124756,
      "learning_rate": 0.00012140703517587942,
      "loss": 0.3503,
      "step": 396
    },
    {
      "epoch": 3.2946058091286305,
      "grad_norm": 1.5787632465362549,
      "learning_rate": 0.00012120603015075378,
      "loss": 0.3053,
      "step": 397
    },
    {
      "epoch": 3.3029045643153525,
      "grad_norm": 1.5995595455169678,
      "learning_rate": 0.00012100502512562815,
      "loss": 0.3786,
      "step": 398
    },
    {
      "epoch": 3.3112033195020745,
      "grad_norm": 1.5775854587554932,
      "learning_rate": 0.00012080402010050251,
      "loss": 0.2661,
      "step": 399
    },
    {
      "epoch": 3.3195020746887964,
      "grad_norm": 1.4307591915130615,
      "learning_rate": 0.00012060301507537688,
      "loss": 0.2648,
      "step": 400
    },
    {
      "epoch": 3.327800829875519,
      "grad_norm": 1.75480318069458,
      "learning_rate": 0.00012040201005025127,
      "loss": 0.2852,
      "step": 401
    },
    {
      "epoch": 3.336099585062241,
      "grad_norm": 1.7646872997283936,
      "learning_rate": 0.00012020100502512563,
      "loss": 0.3003,
      "step": 402
    },
    {
      "epoch": 3.3443983402489628,
      "grad_norm": 1.785440444946289,
      "learning_rate": 0.00012,
      "loss": 0.2849,
      "step": 403
    },
    {
      "epoch": 3.3526970954356847,
      "grad_norm": 1.388603687286377,
      "learning_rate": 0.00011979899497487436,
      "loss": 0.2307,
      "step": 404
    },
    {
      "epoch": 3.3609958506224067,
      "grad_norm": 1.5026975870132446,
      "learning_rate": 0.00011959798994974876,
      "loss": 0.2813,
      "step": 405
    },
    {
      "epoch": 3.3692946058091287,
      "grad_norm": 1.7790018320083618,
      "learning_rate": 0.00011939698492462312,
      "loss": 0.3112,
      "step": 406
    },
    {
      "epoch": 3.3775933609958506,
      "grad_norm": 1.5257751941680908,
      "learning_rate": 0.0001191959798994975,
      "loss": 0.2586,
      "step": 407
    },
    {
      "epoch": 3.3858921161825726,
      "grad_norm": 1.251250982284546,
      "learning_rate": 0.00011899497487437185,
      "loss": 0.2715,
      "step": 408
    },
    {
      "epoch": 3.3941908713692945,
      "grad_norm": 1.684721827507019,
      "learning_rate": 0.00011879396984924624,
      "loss": 0.365,
      "step": 409
    },
    {
      "epoch": 3.4024896265560165,
      "grad_norm": 1.561068058013916,
      "learning_rate": 0.00011859296482412061,
      "loss": 0.2675,
      "step": 410
    },
    {
      "epoch": 3.4107883817427385,
      "grad_norm": 1.8195722103118896,
      "learning_rate": 0.00011839195979899497,
      "loss": 0.3503,
      "step": 411
    },
    {
      "epoch": 3.4190871369294604,
      "grad_norm": 1.6030350923538208,
      "learning_rate": 0.00011819095477386935,
      "loss": 0.3374,
      "step": 412
    },
    {
      "epoch": 3.4273858921161824,
      "grad_norm": 1.6927751302719116,
      "learning_rate": 0.00011798994974874373,
      "loss": 0.3239,
      "step": 413
    },
    {
      "epoch": 3.435684647302905,
      "grad_norm": 1.713119626045227,
      "learning_rate": 0.0001177889447236181,
      "loss": 0.3539,
      "step": 414
    },
    {
      "epoch": 3.4439834024896268,
      "grad_norm": 1.6695854663848877,
      "learning_rate": 0.00011758793969849247,
      "loss": 0.2646,
      "step": 415
    },
    {
      "epoch": 3.4522821576763487,
      "grad_norm": 1.5293790102005005,
      "learning_rate": 0.00011738693467336684,
      "loss": 0.2884,
      "step": 416
    },
    {
      "epoch": 3.4605809128630707,
      "grad_norm": 1.5337064266204834,
      "learning_rate": 0.00011718592964824122,
      "loss": 0.2703,
      "step": 417
    },
    {
      "epoch": 3.4688796680497926,
      "grad_norm": 1.7782806158065796,
      "learning_rate": 0.00011698492462311558,
      "loss": 0.3427,
      "step": 418
    },
    {
      "epoch": 3.4771784232365146,
      "grad_norm": 1.6614301204681396,
      "learning_rate": 0.00011678391959798996,
      "loss": 0.3049,
      "step": 419
    },
    {
      "epoch": 3.4854771784232366,
      "grad_norm": 1.931740164756775,
      "learning_rate": 0.00011658291457286432,
      "loss": 0.3682,
      "step": 420
    },
    {
      "epoch": 3.4937759336099585,
      "grad_norm": 1.9377493858337402,
      "learning_rate": 0.00011638190954773872,
      "loss": 0.3339,
      "step": 421
    },
    {
      "epoch": 3.5020746887966805,
      "grad_norm": 1.7052581310272217,
      "learning_rate": 0.00011618090452261308,
      "loss": 0.2986,
      "step": 422
    },
    {
      "epoch": 3.5103734439834025,
      "grad_norm": 1.4318687915802002,
      "learning_rate": 0.00011597989949748745,
      "loss": 0.2872,
      "step": 423
    },
    {
      "epoch": 3.5186721991701244,
      "grad_norm": 1.9974919557571411,
      "learning_rate": 0.00011577889447236181,
      "loss": 0.4528,
      "step": 424
    },
    {
      "epoch": 3.5269709543568464,
      "grad_norm": 1.4653384685516357,
      "learning_rate": 0.00011557788944723618,
      "loss": 0.2848,
      "step": 425
    },
    {
      "epoch": 3.5352697095435683,
      "grad_norm": 1.6809428930282593,
      "learning_rate": 0.00011537688442211057,
      "loss": 0.3389,
      "step": 426
    },
    {
      "epoch": 3.5435684647302903,
      "grad_norm": 1.6793209314346313,
      "learning_rate": 0.00011517587939698493,
      "loss": 0.3269,
      "step": 427
    },
    {
      "epoch": 3.5518672199170123,
      "grad_norm": 1.2731343507766724,
      "learning_rate": 0.0001149748743718593,
      "loss": 0.2155,
      "step": 428
    },
    {
      "epoch": 3.5601659751037342,
      "grad_norm": 1.6625101566314697,
      "learning_rate": 0.00011477386934673366,
      "loss": 0.374,
      "step": 429
    },
    {
      "epoch": 3.568464730290456,
      "grad_norm": 1.384021520614624,
      "learning_rate": 0.00011457286432160806,
      "loss": 0.2368,
      "step": 430
    },
    {
      "epoch": 3.576763485477178,
      "grad_norm": 1.7394168376922607,
      "learning_rate": 0.00011437185929648242,
      "loss": 0.3383,
      "step": 431
    },
    {
      "epoch": 3.5850622406639006,
      "grad_norm": 1.7575079202651978,
      "learning_rate": 0.00011417085427135679,
      "loss": 0.323,
      "step": 432
    },
    {
      "epoch": 3.5933609958506225,
      "grad_norm": 1.8756413459777832,
      "learning_rate": 0.00011396984924623115,
      "loss": 0.3938,
      "step": 433
    },
    {
      "epoch": 3.6016597510373445,
      "grad_norm": 1.6502516269683838,
      "learning_rate": 0.00011376884422110554,
      "loss": 0.3438,
      "step": 434
    },
    {
      "epoch": 3.6099585062240664,
      "grad_norm": 1.3170372247695923,
      "learning_rate": 0.00011356783919597991,
      "loss": 0.2741,
      "step": 435
    },
    {
      "epoch": 3.6182572614107884,
      "grad_norm": 1.383571743965149,
      "learning_rate": 0.00011336683417085427,
      "loss": 0.236,
      "step": 436
    },
    {
      "epoch": 3.6265560165975104,
      "grad_norm": 1.7583584785461426,
      "learning_rate": 0.00011316582914572864,
      "loss": 0.2911,
      "step": 437
    },
    {
      "epoch": 3.6348547717842323,
      "grad_norm": 1.4136300086975098,
      "learning_rate": 0.00011296482412060303,
      "loss": 0.2718,
      "step": 438
    },
    {
      "epoch": 3.6431535269709543,
      "grad_norm": 1.435177206993103,
      "learning_rate": 0.0001127638190954774,
      "loss": 0.2737,
      "step": 439
    },
    {
      "epoch": 3.6514522821576763,
      "grad_norm": 1.9127167463302612,
      "learning_rate": 0.00011256281407035176,
      "loss": 0.3877,
      "step": 440
    },
    {
      "epoch": 3.659751037344398,
      "grad_norm": 1.628758430480957,
      "learning_rate": 0.00011236180904522614,
      "loss": 0.2323,
      "step": 441
    },
    {
      "epoch": 3.66804979253112,
      "grad_norm": 1.8547006845474243,
      "learning_rate": 0.00011216080402010052,
      "loss": 0.3143,
      "step": 442
    },
    {
      "epoch": 3.6763485477178426,
      "grad_norm": 1.7076060771942139,
      "learning_rate": 0.00011195979899497488,
      "loss": 0.2936,
      "step": 443
    },
    {
      "epoch": 3.6846473029045645,
      "grad_norm": 1.410319209098816,
      "learning_rate": 0.00011175879396984925,
      "loss": 0.2542,
      "step": 444
    },
    {
      "epoch": 3.6929460580912865,
      "grad_norm": 1.997183918952942,
      "learning_rate": 0.00011155778894472361,
      "loss": 0.3589,
      "step": 445
    },
    {
      "epoch": 3.7012448132780085,
      "grad_norm": 1.7927271127700806,
      "learning_rate": 0.00011135678391959799,
      "loss": 0.3609,
      "step": 446
    },
    {
      "epoch": 3.7095435684647304,
      "grad_norm": 1.8161580562591553,
      "learning_rate": 0.00011115577889447237,
      "loss": 0.3863,
      "step": 447
    },
    {
      "epoch": 3.7178423236514524,
      "grad_norm": 1.2800289392471313,
      "learning_rate": 0.00011095477386934675,
      "loss": 0.2233,
      "step": 448
    },
    {
      "epoch": 3.7261410788381744,
      "grad_norm": 1.5680533647537231,
      "learning_rate": 0.0001107537688442211,
      "loss": 0.2681,
      "step": 449
    },
    {
      "epoch": 3.7344398340248963,
      "grad_norm": 1.7572847604751587,
      "learning_rate": 0.00011055276381909548,
      "loss": 0.2943,
      "step": 450
    },
    {
      "epoch": 3.7427385892116183,
      "grad_norm": 1.3184082508087158,
      "learning_rate": 0.00011035175879396986,
      "loss": 0.2227,
      "step": 451
    },
    {
      "epoch": 3.7510373443983402,
      "grad_norm": 1.3905905485153198,
      "learning_rate": 0.00011015075376884422,
      "loss": 0.2267,
      "step": 452
    },
    {
      "epoch": 3.759336099585062,
      "grad_norm": 1.8163409233093262,
      "learning_rate": 0.0001099497487437186,
      "loss": 0.2645,
      "step": 453
    },
    {
      "epoch": 3.767634854771784,
      "grad_norm": 1.5182335376739502,
      "learning_rate": 0.00010974874371859296,
      "loss": 0.3135,
      "step": 454
    },
    {
      "epoch": 3.775933609958506,
      "grad_norm": 1.8685775995254517,
      "learning_rate": 0.00010954773869346736,
      "loss": 0.2897,
      "step": 455
    },
    {
      "epoch": 3.784232365145228,
      "grad_norm": 1.936037302017212,
      "learning_rate": 0.00010934673366834172,
      "loss": 0.3515,
      "step": 456
    },
    {
      "epoch": 3.79253112033195,
      "grad_norm": 1.59627366065979,
      "learning_rate": 0.00010914572864321609,
      "loss": 0.3386,
      "step": 457
    },
    {
      "epoch": 3.800829875518672,
      "grad_norm": 1.7852853536605835,
      "learning_rate": 0.00010894472361809045,
      "loss": 0.3541,
      "step": 458
    },
    {
      "epoch": 3.809128630705394,
      "grad_norm": 1.6673699617385864,
      "learning_rate": 0.00010874371859296483,
      "loss": 0.3551,
      "step": 459
    },
    {
      "epoch": 3.817427385892116,
      "grad_norm": 1.6056805849075317,
      "learning_rate": 0.00010854271356783921,
      "loss": 0.3121,
      "step": 460
    },
    {
      "epoch": 3.825726141078838,
      "grad_norm": 1.9683942794799805,
      "learning_rate": 0.00010834170854271357,
      "loss": 0.357,
      "step": 461
    },
    {
      "epoch": 3.8340248962655603,
      "grad_norm": 1.584997534751892,
      "learning_rate": 0.00010814070351758794,
      "loss": 0.245,
      "step": 462
    },
    {
      "epoch": 3.8423236514522823,
      "grad_norm": 1.7992727756500244,
      "learning_rate": 0.00010793969849246233,
      "loss": 0.3382,
      "step": 463
    },
    {
      "epoch": 3.8506224066390042,
      "grad_norm": 1.4429272413253784,
      "learning_rate": 0.0001077386934673367,
      "loss": 0.2822,
      "step": 464
    },
    {
      "epoch": 3.858921161825726,
      "grad_norm": 1.1504849195480347,
      "learning_rate": 0.00010753768844221106,
      "loss": 0.2219,
      "step": 465
    },
    {
      "epoch": 3.867219917012448,
      "grad_norm": 1.7789454460144043,
      "learning_rate": 0.00010733668341708543,
      "loss": 0.4031,
      "step": 466
    },
    {
      "epoch": 3.87551867219917,
      "grad_norm": 1.60355544090271,
      "learning_rate": 0.00010713567839195982,
      "loss": 0.308,
      "step": 467
    },
    {
      "epoch": 3.883817427385892,
      "grad_norm": 1.4862003326416016,
      "learning_rate": 0.00010693467336683418,
      "loss": 0.2755,
      "step": 468
    },
    {
      "epoch": 3.892116182572614,
      "grad_norm": 1.7626913785934448,
      "learning_rate": 0.00010673366834170855,
      "loss": 0.3377,
      "step": 469
    },
    {
      "epoch": 3.900414937759336,
      "grad_norm": 2.076101541519165,
      "learning_rate": 0.00010653266331658291,
      "loss": 0.3786,
      "step": 470
    },
    {
      "epoch": 3.908713692946058,
      "grad_norm": 1.790191650390625,
      "learning_rate": 0.00010633165829145728,
      "loss": 0.3904,
      "step": 471
    },
    {
      "epoch": 3.91701244813278,
      "grad_norm": 1.5966328382492065,
      "learning_rate": 0.00010613065326633167,
      "loss": 0.3486,
      "step": 472
    },
    {
      "epoch": 3.9253112033195023,
      "grad_norm": 1.5402107238769531,
      "learning_rate": 0.00010592964824120604,
      "loss": 0.2909,
      "step": 473
    },
    {
      "epoch": 3.9336099585062243,
      "grad_norm": 1.6316250562667847,
      "learning_rate": 0.0001057286432160804,
      "loss": 0.3231,
      "step": 474
    },
    {
      "epoch": 3.9419087136929463,
      "grad_norm": 1.528947353363037,
      "learning_rate": 0.00010552763819095478,
      "loss": 0.2669,
      "step": 475
    },
    {
      "epoch": 3.9502074688796682,
      "grad_norm": 1.5234590768814087,
      "learning_rate": 0.00010532663316582916,
      "loss": 0.3236,
      "step": 476
    },
    {
      "epoch": 3.95850622406639,
      "grad_norm": 1.5661605596542358,
      "learning_rate": 0.00010512562814070352,
      "loss": 0.3433,
      "step": 477
    },
    {
      "epoch": 3.966804979253112,
      "grad_norm": 1.7142928838729858,
      "learning_rate": 0.0001049246231155779,
      "loss": 0.3548,
      "step": 478
    },
    {
      "epoch": 3.975103734439834,
      "grad_norm": 2.0498223304748535,
      "learning_rate": 0.00010472361809045225,
      "loss": 0.3014,
      "step": 479
    },
    {
      "epoch": 3.983402489626556,
      "grad_norm": 1.7480872869491577,
      "learning_rate": 0.00010452261306532664,
      "loss": 0.3366,
      "step": 480
    },
    {
      "epoch": 3.991701244813278,
      "grad_norm": 1.642552137374878,
      "learning_rate": 0.00010432160804020101,
      "loss": 0.34,
      "step": 481
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.0438427925109863,
      "learning_rate": 0.00010412060301507539,
      "loss": 0.3159,
      "step": 482
    },
    {
      "epoch": 4.008298755186722,
      "grad_norm": 0.9733163714408875,
      "learning_rate": 0.00010391959798994975,
      "loss": 0.1746,
      "step": 483
    },
    {
      "epoch": 4.016597510373444,
      "grad_norm": 1.1325403451919556,
      "learning_rate": 0.00010371859296482413,
      "loss": 0.2169,
      "step": 484
    },
    {
      "epoch": 4.024896265560166,
      "grad_norm": 0.912590742111206,
      "learning_rate": 0.0001035175879396985,
      "loss": 0.1461,
      "step": 485
    },
    {
      "epoch": 4.033195020746888,
      "grad_norm": 0.9720569252967834,
      "learning_rate": 0.00010331658291457286,
      "loss": 0.1631,
      "step": 486
    },
    {
      "epoch": 4.04149377593361,
      "grad_norm": 0.9019319415092468,
      "learning_rate": 0.00010311557788944724,
      "loss": 0.1161,
      "step": 487
    },
    {
      "epoch": 4.049792531120332,
      "grad_norm": 1.3256398439407349,
      "learning_rate": 0.00010291457286432162,
      "loss": 0.1509,
      "step": 488
    },
    {
      "epoch": 4.058091286307054,
      "grad_norm": 1.3138483762741089,
      "learning_rate": 0.00010271356783919598,
      "loss": 0.1614,
      "step": 489
    },
    {
      "epoch": 4.066390041493776,
      "grad_norm": 1.6167417764663696,
      "learning_rate": 0.00010251256281407036,
      "loss": 0.1747,
      "step": 490
    },
    {
      "epoch": 4.074688796680498,
      "grad_norm": 1.7400906085968018,
      "learning_rate": 0.00010231155778894473,
      "loss": 0.1579,
      "step": 491
    },
    {
      "epoch": 4.08298755186722,
      "grad_norm": 1.4375563859939575,
      "learning_rate": 0.00010211055276381909,
      "loss": 0.1499,
      "step": 492
    },
    {
      "epoch": 4.091286307053942,
      "grad_norm": 1.3594424724578857,
      "learning_rate": 0.00010190954773869348,
      "loss": 0.1823,
      "step": 493
    },
    {
      "epoch": 4.0995850622406635,
      "grad_norm": 1.796951174736023,
      "learning_rate": 0.00010170854271356785,
      "loss": 0.1899,
      "step": 494
    },
    {
      "epoch": 4.1078838174273855,
      "grad_norm": 2.16772723197937,
      "learning_rate": 0.00010150753768844221,
      "loss": 0.2319,
      "step": 495
    },
    {
      "epoch": 4.1161825726141075,
      "grad_norm": 1.6479599475860596,
      "learning_rate": 0.00010130653266331658,
      "loss": 0.1914,
      "step": 496
    },
    {
      "epoch": 4.124481327800829,
      "grad_norm": 1.6715614795684814,
      "learning_rate": 0.00010110552763819097,
      "loss": 0.207,
      "step": 497
    },
    {
      "epoch": 4.132780082987552,
      "grad_norm": 2.0872504711151123,
      "learning_rate": 0.00010090452261306533,
      "loss": 0.2126,
      "step": 498
    },
    {
      "epoch": 4.141078838174274,
      "grad_norm": 1.2104239463806152,
      "learning_rate": 0.0001007035175879397,
      "loss": 0.1714,
      "step": 499
    },
    {
      "epoch": 4.149377593360996,
      "grad_norm": 1.6436537504196167,
      "learning_rate": 0.00010050251256281407,
      "loss": 0.2109,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0580930781087744e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
